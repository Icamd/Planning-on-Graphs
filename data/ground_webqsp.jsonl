{"id": "WebQTest-0", "question": "what does jamaican people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does jamaican people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.007816641591489315, -0.024141890928149223, -0.05123569816350937], "norm_scores": [0.339983731508255, 0.3344784379005432, 0.32553786039352417]}}
{"id": "WebQTest-1", "question": "what did james k polk do before he was president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"]], "ground_paths": [["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.nationality", "government.government_office_or_title.jurisdiction"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did james k polk do before he was president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.nationality <SEP> government.government_office_or_title.jurisdiction </PATH>"], "scores": [-0.01965351775288582, -0.023195328190922737, -0.02756471559405327], "norm_scores": [0.3346065580844879, 0.33342355489730835, 0.33196988701820374]}}
{"id": "WebQTest-3", "question": "who plays ken barlow in coronation street", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays ken barlow in coronation street [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.01834176294505596, -0.025331422686576843, -0.0322323702275753], "norm_scores": [0.3356560468673706, 0.3333180844783783, 0.3310258388519287]}}
{"id": "WebQTest-6", "question": "where is jamarcus russell from", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jamarcus russell from [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.008091732859611511, -0.019853223115205765, -0.09176190197467804], "norm_scores": [0.3438740074634552, 0.3398532271385193, 0.3162727952003479]}}
{"id": "WebQTest-7", "question": "where was george washington carver from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was george washington carver from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.005168572533875704, -0.036151349544525146, -0.040255799889564514], "norm_scores": [0.3407139182090759, 0.33031952381134033, 0.32896652817726135]}}
{"id": "WebQTest-8", "question": "what else did ben franklin invent", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what else did ben franklin invent [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>"], "scores": [-0.007006578613072634, -0.02537095546722412, -0.0682467594742775], "norm_scores": [0.342184454202652, 0.335957795381546, 0.3218577802181244]}}
{"id": "WebQTest-9", "question": "who was richard nixon married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["people.person.profession", "common.topic.notable_types"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["people.person.nationality", "people.person.nationality"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_ownership.owner"], ["people.person.children", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was richard nixon married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.022270560264587402, -0.03332994133234024, -0.047492168843746185], "norm_scores": [0.3373710513114929, 0.3336605131626129, 0.32896843552589417]}}
{"id": "WebQTest-12", "question": "who is governor of ohio 2011", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "ground_paths": [["base.locations.states_and_provences.country", "people.person.nationality"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is governor of ohio 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.016278697177767754, -0.02163606323301792, -0.03294942155480385], "norm_scores": [0.3357818126678467, 0.3339877128601074, 0.3302304744720459]}}
{"id": "WebQTest-13", "question": "who was vice president after kennedy died", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president after kennedy died [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.011500820517539978, -0.01400854904204607, -0.0672946348786354], "norm_scores": [0.339762806892395, 0.33891186118125916, 0.32132530212402344]}}
{"id": "WebQTest-14", "question": "where is the fukushima daiichi nuclear plant located", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.street_address", "location.mailing_address.citytown"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the fukushima daiichi nuclear plant located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.street_address <SEP> location.mailing_address.citytown </PATH>"], "scores": [-0.009932905435562134, -0.02615327388048172, -0.03804823383688927], "norm_scores": [0.3382736146450043, 0.3328309655189514, 0.3288953900337219]}}
{"id": "WebQTest-16", "question": "what countries are part of the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_area_type"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are part of the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.021565955132246017, -0.02475064992904663, -0.03161768987774849], "norm_scores": [0.33480435609817505, 0.3337397873401642, 0.33145585656166077]}}
{"id": "WebQTest-19", "question": "what is my timezone in louisiana", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is my timezone in louisiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.268349660676904e-05, -0.15988115966320038, -0.18230435252189636], "norm_scores": [0.37235432863235474, 0.317341148853302, 0.31030452251434326]}}
{"id": "WebQTest-20", "question": "what did st augustine do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did st augustine do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.001937191584147513, -0.05305998772382736, -0.06827409565448761], "norm_scores": [0.34650304913520813, 0.3292340040206909, 0.32426291704177856]}}
{"id": "WebQTest-21", "question": "who is keyshia cole dad", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is keyshia cole dad [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.0061726998537778854, -0.02629780024290085, -0.1688051074743271], "norm_scores": [0.3533594310283661, 0.3463191092014313, 0.30032142996788025]}}
{"id": "WebQTest-22", "question": "what kind government does egypt have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind government does egypt have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005432487465441227, -0.023911278694868088, -0.13977976143360138], "norm_scores": [0.35014283657073975, 0.3437320291996002, 0.30612513422966003]}}
{"id": "WebQTest-23", "question": "what town was martin luther king assassinated in", "prediction": [["people.deceased_person.place_of_death"], ["base.biblioness.bibs_location.country"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what town was martin luther king assassinated in [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0016836128197610378, -0.06829150766134262, -0.0711134523153305], "norm_scores": [0.34861576557159424, 0.32615166902542114, 0.3252325654029846]}}
{"id": "WebQTest-24", "question": "where did edgar allan poe died", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did edgar allan poe died [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> deceased_person.place_of_death </PATH>"], "scores": [-0.00020127969037275761, -0.09454979002475739, -0.1122438833117485], "norm_scores": [0.35663697123527527, 0.3245273530483246, 0.31883567571640015]}}
{"id": "WebQTest-26", "question": "what to do today in atlanta with kids", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do today in atlanta with kids [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01581641286611557, -0.016067486256361008, -0.024060945957899094], "norm_scores": [0.3342761695384979, 0.33419227600097656, 0.3315315544605255]}}
{"id": "WebQTest-28", "question": "what electorate does anna bligh represent", "prediction": [["people.person.nationality", "base.biblioness.bibs_location.country"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"]], "ground_paths": [["government.politician.government_positions_held", "government.political_district.representatives"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what electorate does anna bligh represent [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>"], "scores": [-0.014704175293445587, -0.019412318244576454, -0.02605961263179779], "norm_scores": [0.3351193368434906, 0.3335452377796173, 0.3313354253768921]}}
{"id": "WebQTest-31", "question": "where did andy murray started playing tennis", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_facility.teams"], ["sports.sports_team_location.teams"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did andy murray started playing tennis [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.02283378876745701, -0.03582644835114479, -0.042842600494623184], "norm_scores": [0.33700883388519287, 0.3326584994792938, 0.3303326964378357]}}
{"id": "WebQTest-32", "question": "what is the australian dollar called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_used", "common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the australian dollar called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.00775887630879879, -0.02836534008383751, -0.06106749176979065], "norm_scores": [0.3415660262107849, 0.3345995545387268, 0.3238344192504883]}}
{"id": "WebQTest-33", "question": "what timezone is sweden", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is sweden [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.000543397618457675, -0.0629831999540329, -0.15343622863292694], "norm_scores": [0.3574375808238983, 0.33580175042152405, 0.3067606985569]}}
{"id": "WebQTest-34", "question": "who did cam newton sign with", "prediction": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cam newton sign with [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>"], "scores": [-0.022871147841215134, -0.024936825037002563, -0.026166832074522972], "norm_scores": [0.33392924070358276, 0.33324018120765686, 0.332830548286438]}}
{"id": "WebQTest-35", "question": "what county is frederick md in", "prediction": [["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["location.location.containedby"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is frederick md in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.020042750984430313, -0.020739398896694183, -0.029679862782359123], "norm_scores": [0.33448028564453125, 0.3342473804950714, 0.33127233386039734]}}
{"id": "WebQTest-36", "question": "what highschool did harper lee go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did harper lee go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.012550000101327896, -0.01966792531311512, -0.02677270770072937], "norm_scores": [0.33570730686187744, 0.3333262503147125, 0.33096644282341003]}}
{"id": "WebQTest-37", "question": "what timezone is utah in", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is utah in [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.3303244486451149e-05, -0.14629225432872772, -0.19486624002456665], "norm_scores": [0.37217995524406433, 0.3215324878692627, 0.30628761649131775]}}
{"id": "WebQTest-38", "question": "who did george w. bush run against for the second term", "prediction": [["time.event.locations", "people.person.nationality"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["government.election.campaigns", "government.election_campaign.candidate"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did george w. bush run against for the second term [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations <SEP> people.person.nationality </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> government.election.campaigns <SEP> government.election_campaign.candidate </PATH>"], "scores": [-0.013264689594507217, -0.02227376401424408, -0.023096133023500443], "norm_scores": [0.33542996644973755, 0.3324216306209564, 0.33214837312698364]}}
{"id": "WebQTest-39", "question": "who is niall ferguson 's wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["freebase.valuenotation.has_value", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.profession", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is niall ferguson 's wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.025526978075504303, -0.028349049389362335, -0.032459571957588196], "norm_scores": [0.3344176113605499, 0.33347517251968384, 0.33210721611976624]}}
{"id": "WebQTest-41", "question": "where is the galapagos islands located on a world map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.island.island_group"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the galapagos islands located on a world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.island.island_group </PATH>"], "scores": [-0.00023390870774164796, -0.08660922199487686, -0.10701236873865128], "norm_scores": [0.3551168143749237, 0.3257308900356293, 0.31915226578712463]}}
{"id": "WebQTest-42", "question": "what are the songs that justin bieber wrote", "prediction": [["music.composer.compositions"], ["music.recording.artist"], ["music.composition.composer"]], "ground_paths": [["music.featured_artist.recordings"], ["music.artist.album"], ["music.lyricist.lyrics_written"], ["music.composition.composer"], ["music.album.artist"], ["music.recording.featured_artists"], ["music.composition.lyricist"], ["music.composer.compositions"], ["music.featured_artist.albums"], ["music.recording.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the songs that justin bieber wrote [/INST]", "raw_output": {"paths": ["<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.018286677077412605, -0.02157817967236042, -0.03320588916540146], "norm_scores": [0.3353560268878937, 0.33425402641296387, 0.33038991689682007]}}
{"id": "WebQTest-43", "question": "what did george clemenceau do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did george clemenceau do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.003434112761169672, -0.04803027957677841, -0.05474602431058884], "norm_scores": [0.34407228231430054, 0.32906508445739746, 0.3268625736236572]}}
{"id": "WebQTest-44", "question": "what is the state flower of arizona", "prediction": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state flower of arizona [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>"], "scores": [-0.012824129313230515, -0.015916042029857635, -0.022522341459989548], "norm_scores": [0.3347547650337219, 0.3337213397026062, 0.33152392506599426]}}
{"id": "WebQTest-45", "question": "where was rihanna born and raised", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was rihanna born and raised [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.007055041845887899, -0.027305830270051956, -0.05498756840825081], "norm_scores": [0.3409302532672882, 0.3340955972671509, 0.32497408986091614]}}
{"id": "WebQTest-46", "question": "what year did president william henry harrison take office", "prediction": [["government.election_campaign.candidate", "government.election_campaign.election"], ["government.election_campaign.candidate", "time.event.instance_of_recurring_event"], ["people.person.nationality", "location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did president william henry harrison take office [/INST]", "raw_output": {"paths": ["<PATH> government.election_campaign.candidate <SEP> government.election_campaign.election </PATH>", "<PATH> government.election_campaign.candidate <SEP> time.event.instance_of_recurring_event </PATH>", "<PATH> people.person.nationality <SEP> location.location.events </PATH>"], "scores": [-0.0192714836448431, -0.03496143966913223, -0.03707445412874222], "norm_scores": [0.3370649814605713, 0.33181771636009216, 0.33111730217933655]}}
{"id": "WebQTest-47", "question": "where george lopez was born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where george lopez was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0083766495808959, -0.0199274905025959, -0.09516625106334686], "norm_scores": [0.344188392162323, 0.3402356207370758, 0.3155759871006012]}}
{"id": "WebQTest-48", "question": "where are samsung based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are samsung based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded </PATH>"], "scores": [-0.008803723379969597, -0.031694188714027405, -0.039648834615945816], "norm_scores": [0.33932867646217346, 0.331649512052536, 0.3290218412876129]}}
{"id": "WebQTest-49", "question": "what did the islamic people believe in", "prediction": [["religion.belief.belief_of"], ["religion.religion.beliefs"], ["religion.deity.deity_of"]], "ground_paths": [["religion.religion.beliefs"], ["religion.belief.belief_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the islamic people believe in [/INST]", "raw_output": {"paths": ["<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religion.beliefs </PATH>", "<PATH> religion.deity.deity_of </PATH>"], "scores": [-0.00848511140793562, -0.017309969291090965, -0.10944978892803192], "norm_scores": [0.345401793718338, 0.3423670828342438, 0.3122311234474182]}}
{"id": "WebQTest-51", "question": "who will play mr gray in the film", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who will play mr gray in the film [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.02224104292690754, -0.022584430873394012, -0.02883058600127697], "norm_scores": [0.3341030180454254, 0.3339883089065552, 0.3319086730480194]}}
{"id": "WebQTest-52", "question": "what did george orwell died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did george orwell died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.0015412220964208245, -0.04407164081931114, -0.12617436051368713], "norm_scores": [0.3519662022590637, 0.337310791015625, 0.31072303652763367]}}
{"id": "WebQTest-54", "question": "what country did adolf hitler control", "prediction": [["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["military.military_commander.military_commands", "military.military_command.military_combatant"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["government.political_appointer.appointees", "government.governmental_jurisdiction.governing_officials"], ["base.crime.crime.victim_s", "time.event.locations"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["organization.organization.founders", "organization.organization.place_founded"], ["organization.organization_founder.organizations_founded", "organization.organization.place_founded"], ["military.military_command.military_commander", "military.military_combatant.military_commanders"], ["people.deceased_person.place_of_death", "location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did adolf hitler control [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.0256710946559906, -0.02877965196967125, -0.0292341411113739], "norm_scores": [0.33407503366470337, 0.3330381512641907, 0.33288681507110596]}}
{"id": "WebQTest-55", "question": "who is michael buble", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael buble [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0003474945551715791, -0.0798686295747757, -0.13629060983657837], "norm_scores": [0.35759612917900085, 0.3302609622478485, 0.31214290857315063]}}
{"id": "WebQTest-56", "question": "what county is kansas city kansas", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"], ["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is kansas city kansas [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.01759529486298561, -0.021932024508714676, -0.03854646533727646], "norm_scores": [0.33614131808280945, 0.3346867263317108, 0.3291720151901245]}}
{"id": "WebQTest-58", "question": "who did jackie robinson first play for", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did jackie robinson first play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014399570412933826, -0.016801485791802406, -0.03225964680314064], "norm_scores": [0.33558180928230286, 0.33477672934532166, 0.3296414613723755]}}
{"id": "WebQTest-59", "question": "when did annie open", "prediction": [["theater.theater_character.plays_appears_in"], ["theater.play.characters"], ["film.film.directed_by"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did annie open [/INST]", "raw_output": {"paths": ["<PATH> theater.theater_character.plays_appears_in </PATH>", "<PATH> theater.play.characters </PATH>", "<PATH> film.film.directed_by </PATH>"], "scores": [-0.03218317776918411, -0.042526211589574814, -0.043695393949747086], "norm_scores": [0.3357660472393036, 0.33231112360954285, 0.33192282915115356]}}
{"id": "WebQTest-60", "question": "where did eleanor roosevelt die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did eleanor roosevelt die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.00149919418618083, -0.0671272948384285, -0.0790037289261818], "norm_scores": [0.3494179844856262, 0.327222615480423, 0.3233593702316284]}}
{"id": "WebQTest-61", "question": "what are the religions practiced in indonesia", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["royalty.kingdom.rulers", "people.person.religion"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["book.book_subject.works", "book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the religions practiced in indonesia [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> royalty.kingdom.rulers <SEP> people.person.religion </PATH>"], "scores": [-0.0060593257658183575, -0.017342524603009224, -0.07386159896850586], "norm_scores": [0.34208789467811584, 0.33824974298477173, 0.3196623921394348]}}
{"id": "WebQTest-62", "question": "what was jesse james killed with", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was jesse james killed with [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0013953042216598988, -0.061622265726327896, -0.07409875094890594], "norm_scores": [0.3482588231563568, 0.3279033899307251, 0.3238377273082733]}}
{"id": "WebQTest-63", "question": "who was vp for lincoln", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["government.political_party_tenure.politician", "government.political_party_tenure.politician"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vp for lincoln [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.politician </PATH>"], "scores": [-0.007219221442937851, -0.019683275371789932, -0.09820685535669327], "norm_scores": [0.344751238822937, 0.34048089385032654, 0.31476789712905884]}}
{"id": "WebQTest-64", "question": "what other books did charles dickens write", "prediction": [["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_universe_creator.fictional_universes_created"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["tv.tv_program_creator.programs_created"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other books did charles dickens write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_universe_creator.fictional_universes_created </PATH>"], "scores": [-0.005924134515225887, -0.02885272167623043, -0.04706967622041702], "norm_scores": [0.3404809534549713, 0.33276301622390747, 0.32675600051879883]}}
{"id": "WebQTest-65", "question": "who was the first dictator of the soviet union", "prediction": [["people.person.nationality"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the first dictator of the soviet union [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.015844956040382385, -0.01973629742860794, -0.03209543600678444], "norm_scores": [0.3355708122253418, 0.3342675268650055, 0.3301616907119751]}}
{"id": "WebQTest-66", "question": "what is the currency of puerto rico called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of puerto rico called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.0005740797496400774, -0.061944629997015, -0.13423994183540344], "norm_scores": [0.35519474744796753, 0.3340516686439514, 0.31075361371040344]}}
{"id": "WebQTest-67", "question": "what kind of cancer did carl wilson have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of cancer did carl wilson have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.013209127821028233, -0.014927004463970661, -0.03563284873962402], "norm_scores": [0.33600914478302, 0.33543241024017334, 0.32855838537216187]}}
{"id": "WebQTest-68", "question": "who plays the voice of kitt in knight rider", "prediction": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the voice of kitt in knight rider [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.015556004829704762, -0.02095664106309414, -0.02238716185092926], "norm_scores": [0.3346937298774719, 0.332891047000885, 0.3324151933193207]}}
{"id": "WebQTest-69", "question": "what county is brentwood tennessee in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.us_county.hud_county_place"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is brentwood tennessee in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>"], "scores": [-0.014810332097113132, -0.023712633177638054, -0.03306935355067253], "norm_scores": [0.336355596780777, 0.3333745300769806, 0.33026981353759766]}}
{"id": "WebQTest-71", "question": "who did michael j fox marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did michael j fox marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.0264248289167881, -0.026844752952456474, -0.03974064812064171], "norm_scores": [0.33485662937164307, 0.3347160518169403, 0.3304273188114166]}}
{"id": "WebQTest-72", "question": "what battles did stonewall jackson fight in", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_person.participated_in_conflicts"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["military.military_person.participated_in_conflicts"], ["base.culturalevent.event.entity_involved"], ["people.person.nationality", "book.written_work.subjects"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["military.military_conflict.military_personnel_involved"], ["common.topic.image"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what battles did stonewall jackson fight in [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.011244782246649265, -0.029017211869359016, -0.029933221638202667], "norm_scores": [0.3373967707157135, 0.3314533829689026, 0.3311498761177063]}}
{"id": "WebQTest-73", "question": "what language do the maasai tribe speak", "prediction": [["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the maasai tribe speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0006474616238847375, -0.07984872907400131, -0.08988326042890549], "norm_scores": [0.352300763130188, 0.32547447085380554, 0.32222479581832886]}}
{"id": "WebQTest-74", "question": "who did benjamin franklin get married to", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["people.marriage.spouse", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did benjamin franklin get married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.027806805446743965, -0.03044189140200615, -0.03841393440961838], "norm_scores": [0.3348045349121094, 0.33392342925071716, 0.3312719762325287]}}
{"id": "WebQTest-75", "question": "what disease did patrick swayze died from", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["medicine.notable_person_with_medical_condition.condition"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did patrick swayze died from [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>"], "scores": [-0.004981324542313814, -0.028636813163757324, -0.03825706988573074], "norm_scores": [0.33968615531921387, 0.3317449986934662, 0.32856884598731995]}}
{"id": "WebQTest-76", "question": "what works of art did leonardo da vinci produce", "prediction": [["visual_art.visual_artist.art_forms"], ["film.film.subjects"], ["visual_art.art_form.artists"]], "ground_paths": [["visual_art.artwork.artist"], ["visual_art.visual_artist.artworks"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what works of art did leonardo da vinci produce [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> film.film.subjects </PATH>", "<PATH> visual_art.art_form.artists </PATH>"], "scores": [-0.0008322703070007265, -0.07532346248626709, -0.08227670192718506], "norm_scores": [0.3508772552013397, 0.32568976283073425, 0.3234330117702484]}}
{"id": "WebQTest-77", "question": "what capital of austria", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what capital of austria [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.006874473299831152, -0.0385877788066864, -0.06384430080652237], "norm_scores": [0.34324076771736145, 0.3325262665748596, 0.32423296570777893]}}
{"id": "WebQTest-78", "question": "what type of cancer did eva peron have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did eva peron have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.00778994569554925, -0.026753393933176994, -0.03987651690840721], "norm_scores": [0.3390246629714966, 0.3326561450958252, 0.3283191919326782]}}
{"id": "WebQTest-79", "question": "what country did buddha come from", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did buddha come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.006785633973777294, -0.03373667225241661, -0.0475166030228138], "norm_scores": [0.34089016914367676, 0.33182549476623535, 0.3272843360900879]}}
{"id": "WebQTest-80", "question": "what county is greeley colorado in", "prediction": [["location.us_county.county_seat"], ["location.us_county.hud_county_place"], ["location.location.containedby"]], "ground_paths": [["location.us_county.hud_county_place"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is greeley colorado in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.018022937700152397, -0.02367282472550869, -0.02739180065691471], "norm_scores": [0.3350037634372711, 0.333116352558136, 0.3318798243999481]}}
{"id": "WebQTest-82", "question": "what did peter tchaikovsky do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did peter tchaikovsky do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0008998580160550773, -0.06886113435029984, -0.07372590154409409], "norm_scores": [0.3491548001766205, 0.32621416449546814, 0.3246310353279114]}}
{"id": "WebQTest-83", "question": "which countries speak german officially", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak german officially [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.017914077267050743, -0.020438671112060547, -0.024780336767435074], "norm_scores": [0.3343770503997803, 0.33353397250175476, 0.33208900690078735]}}
{"id": "WebQTest-84", "question": "what type of music did john lennon sing", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did john lennon sing [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0012811614433303475, -0.053736016154289246, -0.11318670958280563], "norm_scores": [0.351737916469574, 0.33376309275627136, 0.3144989609718323]}}
{"id": "WebQTest-85", "question": "who are the colorado representatives", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the colorado representatives [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.024853257462382317, -0.026616353541612625, -0.029339171946048737], "norm_scores": [0.33402782678604126, 0.333439439535141, 0.33253276348114014]}}
{"id": "WebQTest-86", "question": "which country does greenland belong to", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["location.country.administrative_divisions"]], "ground_paths": [["location.country.administrative_divisions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country does greenland belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.country.administrative_divisions </PATH>"], "scores": [-0.017568016424775124, -0.03353387489914894, -0.039118602871894836], "norm_scores": [0.33751392364501953, 0.33216801285743713, 0.3303180932998657]}}
{"id": "WebQTest-87", "question": "what the zip code for seattle washington", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the zip code for seattle washington [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0001512721792096272, -0.09125495702028275, -0.15485596656799316], "norm_scores": [0.3610641360282898, 0.3296237587928772, 0.3093121349811554]}}
{"id": "WebQTest-88", "question": "who is willow smith mom name", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is willow smith mom name [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.0077378093264997005, -0.02289438061416149, -0.1285146325826645], "norm_scores": [0.3482877314090729, 0.34304866194725037, 0.30866363644599915]}}
{"id": "WebQTest-89", "question": "what do you call the chinese writing system", "prediction": [["language.human_language.writing_system"], ["language.language_writing_system.writing_systems"], ["language.language_writing_system.writing_system"]], "ground_paths": [["language.human_language.writing_system"], ["language.language_writing_system.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do you call the chinese writing system [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.writing_system </PATH>", "<PATH> language.language_writing_system.writing_systems </PATH>", "<PATH> language.language_writing_system.writing_system </PATH>"], "scores": [-0.007990352809429169, -0.04540643095970154, -0.05166865140199661], "norm_scores": [0.34240278601646423, 0.329828143119812, 0.32776913046836853]}}
{"id": "WebQTest-90", "question": "who played on the jeffersons", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played on the jeffersons [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.024462200701236725, -0.02477189339697361, -0.0274969395250082], "norm_scores": [0.3337048292160034, 0.33360153436660767, 0.3326936662197113]}}
{"id": "WebQTest-91", "question": "what is the name of the san francisco newspaper", "prediction": [["book.newspaper.circulation_areas"], ["periodicals.newspaper_circulation_area.newspapers"], ["location.location.containedby"]], "ground_paths": [["book.newspaper.circulation_areas"], ["periodicals.newspaper_circulation_area.newspapers"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the san francisco newspaper [/INST]", "raw_output": {"paths": ["<PATH> book.newspaper.circulation_areas </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0031649337615817785, -0.029675612226128578, -0.09313197433948517], "norm_scores": [0.34628450870513916, 0.3372248709201813, 0.31649065017700195]}}
{"id": "WebQTest-93", "question": "what continent does armenia belong to", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["base.locations.continents.countries_within"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent does armenia belong to [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.014475495554506779, -0.024828096851706505, -0.02832271344959736], "norm_scores": [0.33602726459503174, 0.33256644010543823, 0.33140626549720764]}}
{"id": "WebQTest-94", "question": "what did randy savage died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did randy savage died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0008814969332888722, -0.054529789835214615, -0.13325072824954987], "norm_scores": [0.35413485765457153, 0.33563676476478577, 0.3102283179759979]}}
{"id": "WebQTest-95", "question": "what was the title of the book charles darwin wrote", "prediction": [["book.written_work.author"], ["common.topic.notable_types"], ["book.book_edition.author_editor"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], [], ["book.written_work.author"], ["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the title of the book charles darwin wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.001480892300605774, -0.07437261939048767, -0.07554466277360916], "norm_scores": [0.34985658526420593, 0.325262188911438, 0.3248811960220337]}}
{"id": "WebQTest-96", "question": "where did richard nixon die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did richard nixon die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people </PATH>"], "scores": [-0.0005234224372543395, -0.07889678329229355, -0.09943533688783646], "norm_scores": [0.3533017337322235, 0.3266695439815521, 0.3200286626815796]}}
{"id": "WebQTest-97", "question": "what songs does smokey robinson sing", "prediction": [["music.recording.artist"], ["music.artist.track"], ["music.composition.composer"]], "ground_paths": [["music.featured_artist.recordings"], ["music.producer.releases_produced"], ["music.artist.album"], ["music.album.artist"], ["music.recording.featured_artists"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["music.recording.producer"], ["music.recording.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs does smokey robinson sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.014626658521592617, -0.03778466582298279, -0.04597558453679085], "norm_scores": [0.3394150137901306, 0.3316451609134674, 0.3289397656917572]}}
{"id": "WebQTest-98", "question": "who was vice president when jfk was president", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president when jfk was president [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.01021979283541441, -0.014723507687449455, -0.08055878430604935], "norm_scores": [0.34157857298851013, 0.34004366397857666, 0.3183777928352356]}}
{"id": "WebQTest-99", "question": "what is serbian language called", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.language_family"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is serbian language called [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.language_family </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.017512956634163857, -0.026071058586239815, -0.03568669408559799], "norm_scores": [0.336307555437088, 0.33344167470932007, 0.3302507698535919]}}
{"id": "WebQTest-100", "question": "what language is spoken in haiti today", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in haiti today [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.011867677792906761, -0.016684208065271378, -0.05237698554992676], "norm_scores": [0.338352769613266, 0.33672699332237244, 0.3249202370643616]}}
{"id": "WebQTest-102", "question": "who played barbara gordon batgirl", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played barbara gordon batgirl [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.011492022313177586, -0.03231694549322128, -0.040227364748716354], "norm_scores": [0.33886080980300903, 0.3318770229816437, 0.3292621076107025]}}
{"id": "WebQTest-103", "question": "what basketball teams has shaq played for", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what basketball teams has shaq played for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.02143496833741665, -0.02170201577246189, -0.025722365826368332], "norm_scores": [0.3338391184806824, 0.33375000953674316, 0.33241090178489685]}}
{"id": "WebQTest-104", "question": "where is jay leno from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jay leno from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.004602814093232155, -0.034214869141578674, -0.05904407054185867], "norm_scores": [0.3427197337150574, 0.3327198624610901, 0.3245604336261749]}}
{"id": "WebQTest-105", "question": "what language do people from thailand speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from thailand speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.016446268185973167, -0.018521476536989212, -0.02923750691115856], "norm_scores": [0.3349840044975281, 0.3342895805835724, 0.3307264447212219]}}
{"id": "WebQTest-106", "question": "who did armie hammer play in the social network", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did armie hammer play in the social network [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.01915075071156025, -0.027061702683568, -0.027620982378721237], "norm_scores": [0.3351559340953827, 0.33251500129699707, 0.33232906460762024]}}
{"id": "WebQTest-107", "question": "what was thomas jefferson role in the declaration of independence", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["book.written_work.subjects"], ["book.author.book_editions_published"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was thomas jefferson role in the declaration of independence [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.author.book_editions_published </PATH>"], "scores": [-0.028861289843916893, -0.033359695225954056, -0.03338029980659485], "norm_scores": [0.3343360126018524, 0.3328354060649872, 0.332828551530838]}}
{"id": "WebQTest-108", "question": "what was the book written by charles darwin", "prediction": [["book.written_work.subjects"], ["book.written_work.author"], ["book.book_subject.works"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], [], ["book.written_work.author"], ["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the book written by charles darwin [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.013290833681821823, -0.01816498674452305, -0.05582587420940399], "norm_scores": [0.33858197927474976, 0.33693569898605347, 0.32448238134384155]}}
{"id": "WebQTest-109", "question": "who did tim tebow play college football for", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tim tebow play college football for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.016415497288107872, -0.02112521417438984, -0.0283987857401371], "norm_scores": [0.33518919348716736, 0.3336142897605896, 0.33119651675224304]}}
{"id": "WebQTest-110", "question": "who does peyton manning play football for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does peyton manning play football for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.009983782656490803, -0.02009660191833973, -0.031364407390356064], "norm_scores": [0.3368382155895233, 0.33344897627830505, 0.329712837934494]}}
{"id": "WebQTest-111", "question": "where is the carpathian mountain range located", "prediction": [["location.location.partially_containedby"], ["location.location.containedby"], ["location.location.partially_contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the carpathian mountain range located [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.01631147414445877, -0.016648536548018456, -0.03774900734424591], "norm_scores": [0.3357446789741516, 0.3356315791606903, 0.3286237418651581]}}
{"id": "WebQTest-112", "question": "what was robert burns", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was robert burns [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>"], "scores": [-0.00390968332067132, -0.043077535927295685, -0.05968815088272095], "norm_scores": [0.3439572751522064, 0.3307456076145172, 0.325297087430954]}}
{"id": "WebQTest-114", "question": "who did the voice of darth vader in episode 3", "prediction": [["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the voice of darth vader in episode 3 [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.027018388733267784, -0.031365301460027695, -0.03329380601644516], "norm_scores": [0.3345145583152771, 0.33306360244750977, 0.3324218988418579]}}
{"id": "WebQTest-115", "question": "what super bowl did peyton manning win", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship.events"]], "ground_paths": [["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what super bowl did peyton manning win [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.005557637196034193, -0.03794439136981964, -0.054975204169750214], "norm_scores": [0.34247565269470215, 0.3315616846084595, 0.32596269249916077]}}
{"id": "WebQTest-116", "question": "which country was justin bieber born in", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country was justin bieber born in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.00735628604888916, -0.026144471019506454, -0.05408024415373802], "norm_scores": [0.34062984585762024, 0.3342897593975067, 0.32508036494255066]}}
{"id": "WebQTest-118", "question": "who fought in the gulf war 1991", "prediction": [["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"]], "ground_paths": [["military.military_person.participated_in_conflicts", "people.person.nationality"], ["film.film.genre", "film.film.country"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["film.film.subjects", "film.film.country"], ["military.military_conflict.casualties", "military.military_combatant.casualties"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["film.film_subject.films", "film.film.country"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["time.event.locations"], ["book.book_subject.works", "media_common.netflix_title.netflix_genres"], ["base.culturalevent.event.entity_involved", "people.person.place_of_birth"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["film.film_genre.films_in_this_genre", "film.film.country"], ["film.film_subject.films", "tv.tv_program.country_of_origin"], ["base.culturalevent.event.entity_involved", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought in the gulf war 1991 [/INST]", "raw_output": {"paths": ["<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>", "<PATH> time.event.includes_event <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.020476078614592552, -0.02848062664270401, -0.03248853236436844], "norm_scores": [0.33556070923805237, 0.33288541436195374, 0.3315539062023163]}}
{"id": "WebQTest-119", "question": "who plays london tipton in suite life on deck", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["tv.tv_program.genre", "music.artist.genre"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays london tipton in suite life on deck [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.015547185204923153, -0.030701223760843277, -0.03094770759344101], "norm_scores": [0.33673688769340515, 0.33167240023612976, 0.3315906822681427]}}
{"id": "WebQTest-121", "question": "what team did david beckham play for in 2011", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["book.written_work.subjects", "book.written_work.subjects"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did david beckham play for in 2011 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.022614475339651108, -0.025221774354577065, -0.028328806161880493], "norm_scores": [0.3342583179473877, 0.33338794112205505, 0.3323536813259125]}}
{"id": "WebQTest-122", "question": "where did francisco coronado come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did francisco coronado come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0076348865404725075, -0.022236280143260956, -0.0666145533323288], "norm_scores": [0.34150317311286926, 0.3365529775619507, 0.32194390892982483]}}
{"id": "WebQTest-123", "question": "what all does google now do", "prediction": [["organization.organization.sectors"], ["organization.organization.founders"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.sectors"]], "ground_paths": [["business.company_product_relationship.company", "business.company_product_relationship.consumer_product"], ["computer.software.developer"], ["computer.software_developer.software"], ["business.company_product_relationship.company", "business.consumer_product.company"], ["computer.computer_manufacturer_brand.computer_models"], ["business.consumer_company.products", "business.company_product_relationship.consumer_product"], ["business.consumer_company.products"], ["business.consumer_company.products", "business.consumer_product.company"], ["internet.website.owner"], ["projects.project_participation.participant", "projects.project_participation.project"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all does google now do [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.sectors </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.sectors </PATH>"], "scores": [-0.04234471544623375, -0.0475841723382473, -0.052459824830293655], "norm_scores": [0.3350408971309662, 0.33329007029533386, 0.33166903257369995]}}
{"id": "WebQTest-124", "question": "what language does egyptian people speak", "prediction": [["location.country.languages_spoken"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does egyptian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken </PATH>"], "scores": [-0.037123408168554306, -0.039831481873989105, -0.040442995727062225], "norm_scores": [0.33400341868400574, 0.3331001102924347, 0.33289647102355957]}}
{"id": "WebQTest-125", "question": "what are abraham sons names", "prediction": [["people.person.children"], ["people.person.parents"], ["fictional_universe.fictional_character.parents"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are abraham sons names [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>"], "scores": [-0.005634212866425514, -0.031036781147122383, -0.09219235181808472], "norm_scores": [0.34578150510787964, 0.3371083736419678, 0.3171100914478302]}}
{"id": "WebQTest-126", "question": "who wrote the jana gana mana", "prediction": [["music.composition.composer"], ["music.composition.lyricist"], ["music.composer.compositions"]], "ground_paths": [["music.lyricist.lyrics_written"], ["music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote the jana gana mana [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.composition.lyricist </PATH>", "<PATH> music.composer.compositions </PATH>"], "scores": [-0.008251539431512356, -0.028936753049492836, -0.04295556619763374], "norm_scores": [0.33951032161712646, 0.33255961537361145, 0.3279300630092621]}}
{"id": "WebQTest-127", "question": "what type of cancer did gilda radner die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did gilda radner die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.008316096849739552, -0.02603895403444767, -0.028522761538624763], "norm_scores": [0.337560772895813, 0.3316309452056885, 0.33080825209617615]}}
{"id": "WebQTest-128", "question": "who plays juni cortez", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays juni cortez [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.02030152827501297, -0.0277458056807518, -0.0340556874871254], "norm_scores": [0.33569175004959106, 0.333202064037323, 0.3311062157154083]}}
{"id": "WebQTest-129", "question": "what was lucille ball", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was lucille ball [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0014717316953465343, -0.06416364014148712, -0.0740283876657486], "norm_scores": [0.34852364659309387, 0.3273448646068573, 0.3241315186023712]}}
{"id": "WebQTest-131", "question": "who inspired obama", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who inspired obama [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.009899956174194813, -0.016250744462013245, -0.11585191637277603], "norm_scores": [0.345645546913147, 0.3434573709964752, 0.3108970820903778]}}
{"id": "WebQTest-132", "question": "who is michael j fox wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.parents"]], "ground_paths": [["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>"], "scores": [-0.02216545306146145, -0.036671943962574005, -0.045063566416502], "norm_scores": [0.33750030398368835, 0.3326396942138672, 0.32986000180244446]}}
{"id": "WebQTest-133", "question": "where did dolly parton grow up", "prediction": [["music.artist.origin"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did dolly parton grow up [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.008741450496017933, -0.03073936328291893, -0.03353814408183098], "norm_scores": [0.3385526239871979, 0.3311865031719208, 0.33026087284088135]}}
{"id": "WebQTest-134", "question": "what type of books did agatha christie wrote", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of books did agatha christie wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.011178984306752682, -0.020819323137402534, -0.04187333211302757], "norm_scores": [0.337817519903183, 0.3345765173435211, 0.3276059925556183]}}
{"id": "WebQTest-138", "question": "what are the four main languages spoken in spain", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the four main languages spoken in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.01745467260479927, -0.017771834507584572, -0.02584383264183998], "norm_scores": [0.33429959416389465, 0.3341935873031616, 0.3315068185329437]}}
{"id": "WebQTest-139", "question": "where is the nra headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the nra headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.003701948095113039, -0.03757764399051666, -0.05089103803038597], "norm_scores": [0.342395544052124, 0.33099091053009033, 0.32661351561546326]}}
{"id": "WebQTest-141", "question": "who is gimli 's father in the hobbit", "prediction": [["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.children"]], "ground_paths": [["fictional_universe.fictional_character.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is gimli 's father in the hobbit [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.children </PATH>"], "scores": [-0.028256826102733612, -0.03144197538495064, -0.03959263861179352], "norm_scores": [0.334946870803833, 0.3338817358016968, 0.3311714231967926]}}
{"id": "WebQTest-142", "question": "who developed the tcp ip reference model", "prediction": [["law.invention.inventor"], ["common.topic.notable_types"], ["internet.website.owner", "organization.organization_founder.organizations_founded"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], ["base.argumentmaps.original_idea.innovator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who developed the tcp ip reference model [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> internet.website.owner <SEP> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.010605469346046448, -0.06072111427783966, -0.06785684078931808], "norm_scores": [0.34536635875701904, 0.3284846246242523, 0.32614901661872864]}}
{"id": "WebQTest-145", "question": "what college did albert einstein go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did albert einstein go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.020303936675190926, -0.022728994488716125, -0.023807503283023834], "norm_scores": [0.3339923620223999, 0.3331833779811859, 0.3328242301940918]}}
{"id": "WebQTest-146", "question": "what team does jordan own", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team_owner.teams_owned"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does jordan own [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team_owner.teams_owned </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.0032804931979626417, -0.04626293480396271, -0.0497734472155571], "norm_scores": [0.3433476984500885, 0.3289024531841278, 0.3277498781681061]}}
{"id": "WebQTest-147", "question": "who is the state governor of tennessee", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["base.locations.countries.states_provinces_within", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the state governor of tennessee [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.020130345597863197, -0.02603032998740673, -0.02853246219456196], "norm_scores": [0.3349241614341736, 0.33295392990112305, 0.332121878862381]}}
{"id": "WebQTest-149", "question": "what do portuguese people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do portuguese people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012756158597767353, -0.023303912952542305, -0.032614436000585556], "norm_scores": [0.3367178738117218, 0.33318495750427246, 0.3300971984863281]}}
{"id": "WebQTest-150", "question": "what language does cuba speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does cuba speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.008098614402115345, -0.021420592442154884, -0.07443386316299438], "norm_scores": [0.3421630263328552, 0.33763498067855835, 0.3202020227909088]}}
{"id": "WebQTest-153", "question": "what are the sights to see in madrid", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["base.arthist.helynevek.intezmeny"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["common.topic.image"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the sights to see in madrid [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.017129788175225258, -0.019422119483351707, -0.019846023991703987], "norm_scores": [0.33389008045196533, 0.3331255614757538, 0.33298438787460327]}}
{"id": "WebQTest-154", "question": "what instruments did louis armstrong play", "prediction": [["music.group_member.instruments_played"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.instrument.family"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instruments did louis armstrong play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.0002594094257801771, -0.08471303433179855, -0.12073163688182831], "norm_scores": [0.3564406633377075, 0.32757407426834106, 0.3159852623939514]}}
{"id": "WebQTest-155", "question": "what year was george w bush elected", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.event.locations", "location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was george w bush elected [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.event.locations <SEP> location.location.events </PATH>"], "scores": [-0.014050493016839027, -0.03439744934439659, -0.04730776324868202], "norm_scores": [0.33931106328964233, 0.3324768543243408, 0.32821208238601685]}}
{"id": "WebQTest-156", "question": "what time zone am i in california", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.countries.states_provinces_within"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone am i in california [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.states_provinces_within </PATH>"], "scores": [-1.721387161524035e-05, -0.13648411631584167, -0.15895161032676697], "norm_scores": [0.3669068515300751, 0.3201024830341339, 0.3129906952381134]}}
{"id": "WebQTest-157", "question": "where is mission san buenaventura located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mission san buenaventura located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.0008450504974462092, -0.06337609142065048, -0.08917234092950821], "norm_scores": [0.3502817153930664, 0.3290489912033081, 0.3206693232059479]}}
{"id": "WebQTest-159", "question": "what time in hilo hawaii", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["time.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time in hilo hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> time.location.time_zones </PATH>"], "scores": [-7.559364894405007e-05, -0.09457994252443314, -0.15257036685943604], "norm_scores": [0.3612211346626282, 0.32864758372306824, 0.3101312220096588]}}
{"id": "WebQTest-160", "question": "where did rudolf virchow conduct his research", "prediction": [["people.deceased_person.place_of_death", "location.location.containedby"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["people.person.employment_history", "business.employment_tenure.company"], ["business.employment_tenure.person", "business.employer.employees"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rudolf virchow conduct his research [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employer.employees </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.012482562102377415, -0.02433484047651291, -0.03360883519053459], "norm_scores": [0.33700525760650635, 0.33303454518318176, 0.32996025681495667]}}
{"id": "WebQTest-161", "question": "what university did gordon brown attend", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did gordon brown attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.015405092388391495, -0.01826312020421028, -0.02244339883327484], "norm_scores": [0.33443334698677063, 0.3334788978099823, 0.33208778500556946]}}
{"id": "WebQTest-162", "question": "what countries does greece share borders with", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains", "location.location.partially_contains"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.location.containedby", "location.location.partially_containedby"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["time.event.locations", "time.event.locations"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.containedby", "base.locations.continents.countries_within"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["media_common.netflix_title.netflix_genres", "film.film.country"], ["location.location.events", "location.location.events"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.containedby", "base.locations.countries.continent"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["location.location.containedby", "location.location.containedby"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.combatants"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization.founders", "organization.organization.founders"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.location.events", "time.event.locations"], ["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.official_language", "location.country.languages_spoken"], ["media_common.netflix_genre.titles", "film.film.country"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does greece share borders with [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.028610583394765854, -0.0419466570019722, -0.04780074954032898], "norm_scores": [0.3369561433792114, 0.33249229192733765, 0.33055153489112854]}}
{"id": "WebQTest-163", "question": "what year did the seahawks win the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the seahawks win the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.010888704098761082, -0.014895319938659668, -0.04514091834425926], "norm_scores": [0.33757221698760986, 0.33622241020202637, 0.32620537281036377]}}
{"id": "WebQTest-164", "question": "what did stephen hawking become famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did stephen hawking become famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.002147282939404249, -0.049892790615558624, -0.07307805120944977], "norm_scores": [0.34663212299346924, 0.33047088980674744, 0.32289695739746094]}}
{"id": "WebQTest-165", "question": "who plays donna noble", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["fictional_universe.fictional_character.ethnicity", "people.person.ethnicity"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays donna noble [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.021617701277136803, -0.02861979976296425, -0.039249811321496964], "norm_scores": [0.3360729217529297, 0.3337279260158539, 0.33019915223121643]}}
{"id": "WebQTest-166", "question": "what was dr seuss education", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss education [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.012930599972605705, -0.013307820074260235, -0.04303450509905815], "norm_scores": [0.3367040157318115, 0.33657702803611755, 0.32671892642974854]}}
{"id": "WebQTest-167", "question": "where did queensland get its name from", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["symbols.armorial_grant.coat_of_arms", "symbols.armorial_grant.armiger"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did queensland get its name from [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.armorial_grant.coat_of_arms <SEP> symbols.armorial_grant.armiger </PATH>"], "scores": [-0.007187320850789547, -0.02568582259118557, -0.07734690606594086], "norm_scores": [0.34318071603775024, 0.33689072728157043, 0.31992852687835693]}}
{"id": "WebQTest-168", "question": "what document did james madison write", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "ground_paths": [["book.book_subject.works"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what document did james madison write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.012764081358909607, -0.017223570495843887, -0.05672689154744148], "norm_scores": [0.33869144320487976, 0.33718442916870117, 0.32412418723106384]}}
{"id": "WebQTest-169", "question": "what are the three official languages of belgium", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the three official languages of belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.009468349628150463, -0.02106442116200924, -0.03918682038784027], "norm_scores": [0.3379303216934204, 0.3340342938899994, 0.3280353248119354]}}
{"id": "WebQTest-170", "question": "who played bilbo in lord of the rings", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["people.person.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played bilbo in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.031942859292030334, -0.03435920923948288, -0.03641331568360329], "norm_scores": [0.3340988755226135, 0.33329257369041443, 0.3326086103916168]}}
{"id": "WebQTest-171", "question": "what airport do you fly into to get to destin fl", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport do you fly into to get to destin fl [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01038646325469017, -0.018851300701498985, -0.04673657566308975], "norm_scores": [0.33830946683883667, 0.33545783162117004, 0.3262327015399933]}}
{"id": "WebQTest-172", "question": "what do you call members of the senate", "prediction": [["government.government_office_or_title.governmental_body_if_any", "common.topic.notable_types", "common.topic.notable_types"], ["government.governmental_body.offices_positions", "common.topic.notable_types", "common.topic.notable_types"], ["government.government_position_held.governmental_body", "government.government_position_held.office_position_or_title"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do you call members of the senate [/INST]", "raw_output": {"paths": ["<PATH> government.government_office_or_title.governmental_body_if_any <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> government.governmental_body.offices_positions <SEP> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.012608437798917294, -0.01980530098080635, -0.020404959097504616], "norm_scores": [0.3350013494491577, 0.33259904384613037, 0.3323996663093567]}}
{"id": "WebQTest-173", "question": "what countries are part of the baltic", "prediction": [["location.location.containedby"], ["location.location.contains"], ["common.topic.notable_types"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are part of the baltic [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.01134810596704483, -0.01914668269455433, -0.05229880288243294], "norm_scores": [0.3387409448623657, 0.33610954880714417, 0.3251494765281677]}}
{"id": "WebQTest-174", "question": "what team does colin kaepernick play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does colin kaepernick play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.013673203065991402, -0.024817179888486862, -0.02544637955725193], "norm_scores": [0.33588454127311707, 0.3321622312068939, 0.3319532871246338]}}
{"id": "WebQTest-175", "question": "where did joe flacco attend college", "prediction": [["people.person.nationality", "location.location.containedby"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["education.education.student", "education.education.institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did joe flacco attend college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.016691705211997032, -0.020037958398461342, -0.02209068089723587], "norm_scores": [0.33430561423301697, 0.33318883180618286, 0.33250558376312256]}}
{"id": "WebQTest-176", "question": "who is emma stone father", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is emma stone father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.008066806942224503, -0.023860080167651176, -0.1281692385673523], "norm_scores": [0.34829121828079224, 0.3428337574005127, 0.3088749647140503]}}
{"id": "WebQTest-177", "question": "where was theodore roosevelt buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.place_of_interment.interred_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was theodore roosevelt buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.009832965210080147, -0.024845654144883156, -0.06118597835302353], "norm_scores": [0.34071052074432373, 0.3356337547302246, 0.3236556649208069]}}
{"id": "WebQTest-178", "question": "who was the leader of the us during wwii", "prediction": [["event.speech_or_presentation.speech_topic", "event.speech_or_presentation.speaker_s"], ["military.military_conflict.commanders", "military.military_command.military_commander"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"]], "ground_paths": [["event.speech_topic.speeches_or_presentations_on_this_topic", "event.speech_or_presentation.speaker_s"], ["military.military_command.military_conflict", "military.military_commander.military_commands"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the leader of the us during wwii [/INST]", "raw_output": {"paths": ["<PATH> event.speech_or_presentation.speech_topic <SEP> event.speech_or_presentation.speaker_s </PATH>", "<PATH> military.military_conflict.commanders <SEP> military.military_command.military_commander </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.01635332591831684, -0.027608206495642662, -0.027806200087070465], "norm_scores": [0.3358611762523651, 0.33210229873657227, 0.332036554813385]}}
{"id": "WebQTest-179", "question": "what artistic movement did henri matisse belong to", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what artistic movement did henri matisse belong to [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.0013850554823875427, -0.04071277379989624, -0.10184282809495926], "norm_scores": [0.3489355444908142, 0.3354790508747101, 0.3155854344367981]}}
{"id": "WebQTest-180", "question": "where the missouri river ends", "prediction": [["geography.river.mouth"], ["location.location.partially_containedby"], ["location.location.containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where the missouri river ends [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.00373945408500731, -0.056460246443748474, -0.05892617627978325], "norm_scores": [0.3454287350177765, 0.32768917083740234, 0.32688212394714355]}}
{"id": "WebQTest-181", "question": "what type of currency do they use in thailand", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["royalty.monarch.kingdom", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in thailand [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> royalty.monarch.kingdom <SEP> location.country.currency_used </PATH>"], "scores": [-0.0103959571570158, -0.015427124686539173, -0.10660795122385025], "norm_scores": [0.3444412052631378, 0.3427126407623291, 0.3128461241722107]}}
{"id": "WebQTest-182", "question": "who is khloe kardashian 's husband", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is khloe kardashian 's husband [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.019221749156713486, -0.02005011960864067, -0.03577209636569023], "norm_scores": [0.33526021242141724, 0.33498263359069824, 0.3297571837902069]}}
{"id": "WebQTest-184", "question": "what state does romney live in", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.country"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["people.person.nationality", "location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state does romney live in [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.administrative_division.country </PATH>"], "scores": [-0.02960190922021866, -0.031506866216659546, -0.03629077598452568], "norm_scores": [0.33428823947906494, 0.3336520493030548, 0.33205971121788025]}}
{"id": "WebQTest-185", "question": "what type of government does france use", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does france use [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010320221073925495, -0.014032515697181225, -0.13528645038604736], "norm_scores": [0.3473643660545349, 0.3460772633552551, 0.3065583407878876]}}
{"id": "WebQTest-186", "question": "who rules denmark right now", "prediction": [["royalty.monarch.kingdom"], ["people.person.nationality"], ["royalty.kingdom.rulers"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who rules denmark right now [/INST]", "raw_output": {"paths": ["<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> royalty.kingdom.rulers </PATH>"], "scores": [-0.01042196061462164, -0.019825750961899757, -0.06675149500370026], "norm_scores": [0.34061479568481445, 0.33742672204971313, 0.3219584822654724]}}
{"id": "WebQTest-187", "question": "what did stephen hawking study", "prediction": [["people.person.profession"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.education", "education.education.major_field_of_study"]], "ground_paths": [["book.author.works_written", "book.written_work.subjects"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["education.education.student", "education.education.major_field_of_study"], ["book.written_work.author", "book.written_work.subjects"], ["people.person.profession", "people.professional_field.professions_in_this_field"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did stephen hawking study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>"], "scores": [-0.018143679946660995, -0.020743461325764656, -0.02602408081293106], "norm_scores": [0.3344980478286743, 0.3336295485496521, 0.33187243342399597]}}
{"id": "WebQTest-188", "question": "where did bristol palin go to school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did bristol palin go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.008543427102267742, -0.023298967629671097, -0.029003845527768135], "norm_scores": [0.3372567296028137, 0.3323168456554413, 0.3304264545440674]}}
{"id": "WebQTest-189", "question": "what region of the world is egypt associated with", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what region of the world is egypt associated with [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.018405470997095108, -0.01915271393954754, -0.035573892295360565], "norm_scores": [0.3353193998336792, 0.335068941116333, 0.3296116292476654]}}
{"id": "WebQTest-190", "question": "what did dr. seuss do", "prediction": [["people.person.profession"], ["book.book.interior_illustrations_by"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr. seuss do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.008816271089017391, -0.028607835993170738, -0.05792728811502457], "norm_scores": [0.3410084843635559, 0.33432573080062866, 0.32466578483581543]}}
{"id": "WebQTest-191", "question": "where did drew brees go to college wikianswers", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did drew brees go to college wikianswers [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.017272116616368294, -0.018440986052155495, -0.02676074020564556], "norm_scores": [0.33451664447784424, 0.3341258764266968, 0.33135756850242615]}}
{"id": "WebQTest-193", "question": "what school did ben roethlisberger go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did ben roethlisberger go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.01782427728176117, -0.022867048159241676, -0.023129435256123543], "norm_scores": [0.3344841003417969, 0.33280161023139954, 0.3327142894268036]}}
{"id": "WebQTest-194", "question": "where did the iroquois indians come from", "prediction": [["people.ethnicity.geographic_distribution"], ["people.ethnicity.geographic_distribution", "base.locations.states_and_provences.country"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the iroquois indians come from [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> base.locations.states_and_provences.country </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.00019435769354458898, -0.0996260717511177, -0.10084415227174759], "norm_scores": [0.3559223711490631, 0.32223495841026306, 0.32184267044067383]}}
{"id": "WebQTest-195", "question": "what kind of guitar did george harrison use", "prediction": [["music.group_member.instruments_played"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"]], "ground_paths": [["music.guitarist.guitars_played"], ["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar did george harrison use [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.00959369819611311, -0.03392409160733223, -0.036677803844213486], "norm_scores": [0.3390701115131378, 0.33091995120048523, 0.33000993728637695]}}
{"id": "WebQTest-196", "question": "what language do navajo people speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do navajo people speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0006137252203188837, -0.08083659410476685, -0.08682306855916977], "norm_scores": [0.35207387804985046, 0.324932724237442, 0.32299336791038513]}}
{"id": "WebQTest-197", "question": "what places make up new england", "prediction": [["location.location.contains"], ["location.location.containedby"], ["book.book_subject.works"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what places make up new england [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.009824449196457863, -0.019242893904447556, -0.12628689408302307], "norm_scores": [0.3471391201019287, 0.34388497471809387, 0.3089759051799774]}}
{"id": "WebQTest-199", "question": "what type of government does iraq have now", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does iraq have now [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0051605477929115295, -0.022674350067973137, -0.13012415170669556], "norm_scores": [0.34901970624923706, 0.34296026825904846, 0.3080200254917145]}}
{"id": "WebQTest-200", "question": "where was the vietnam war location", "prediction": [["time.event.locations"], ["location.location.events"], ["book.written_work.subjects"]], "ground_paths": [["book.book_subject.works"], ["time.event.locations"], ["book.written_work.subjects"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the vietnam war location [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.008589089848101139, -0.029172439128160477, -0.048257265239953995], "norm_scores": [0.34005099534988403, 0.33312317728996277, 0.3268258273601532]}}
{"id": "WebQTest-201", "question": "who did gerald ford select as his vice president when he became president", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did gerald ford select as his vice president when he became president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.005632491782307625, -0.02061326988041401, -0.06705635040998459], "norm_scores": [0.34181544184684753, 0.3367329239845276, 0.3214516341686249]}}
{"id": "WebQTest-202", "question": "what is the currency used in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency used in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.004870399832725525, -0.027851535007357597, -0.14480721950531006], "norm_scores": [0.3512846827507019, 0.3433038294315338, 0.30541154742240906]}}
{"id": "WebQTest-204", "question": "what is the capital of modern egypt", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of modern egypt [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.009843912906944752, -0.02466980740427971, -0.046801768243312836], "norm_scores": [0.3390979468822479, 0.3341076076030731, 0.3267943859100342]}}
{"id": "WebQTest-205", "question": "what school did sir ernest rutherford go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did sir ernest rutherford go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.contains </PATH>"], "scores": [-0.014280036091804504, -0.01851314678788185, -0.028287474066019058], "norm_scores": [0.33536046743392944, 0.33394384384155273, 0.3306956887245178]}}
{"id": "WebQTest-206", "question": "what position does brandon roy play", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["basketball.basketball_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position does brandon roy play [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.009720327332615852, -0.03069642372429371, -0.03959909826517105], "norm_scores": [0.33900541067123413, 0.3319684565067291, 0.32902616262435913]}}
{"id": "WebQTest-207", "question": "where is harvard university", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is harvard university [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution </PATH>"], "scores": [-0.0033821791876107454, -0.04875459894537926, -0.07790394872426987], "norm_scores": [0.34676122665405273, 0.3313794434070587, 0.32185932993888855]}}
{"id": "WebQTest-209", "question": "where is laos in world map", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"]], "ground_paths": [["book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is laos in world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.015227245166897774, -0.027818525210022926, -0.03372514992952347], "norm_scores": [0.3367956280708313, 0.3325815498828888, 0.3306228816509247]}}
{"id": "WebQTest-210", "question": "where is tyrese gibson from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tyrese gibson from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.006184323690831661, -0.024001065641641617, -0.08220328390598297], "norm_scores": [0.3437442481517792, 0.337674081325531, 0.3185817003250122]}}
{"id": "WebQTest-211", "question": "what countries are near italy", "prediction": [["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"], ["location.location.partially_containedby", "location.location.partially_containedby"]], "ground_paths": [["base.locations.countries.continent", "location.location.containedby"], ["location.location.partially_contains", "location.location.partially_contains"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["government.form_of_government.countries", "location.country.form_of_government"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["military.military_combatant.includes_allies", "military.military_combatant.includes_allies"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["location.location.containedby", "geography.river.basin_countries"], ["location.country.official_language", "location.country.official_language"], ["location.location.containedby", "base.locations.countries.continent"], ["location.location.containedby", "location.location.containedby"], ["location.country.form_of_government", "government.form_of_government.countries"], ["location.location.partially_contains", "geography.river.origin"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["geography.lake.basin_countries", "geography.lake.basin_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.combatants"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.official_language", "language.human_language.main_country"], ["travel.guidebook.travel_destinations_covered", "travel.travel_destination.guidebooks"], ["location.country.currency_used", "finance.currency.countries_used"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["location.location.partially_contains", "location.location.partially_containedby"], ["fictional_universe.fictional_setting.works_set_here", "fictional_universe.work_of_fiction.setting"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization.founders", "base.nations.supranational_body.member"], ["geography.river.basin_countries", "location.location.partially_contains"], ["organization.organization.founders", "organization.organization.founders"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.locations.countries.continent", "base.locations.countries.continent"], ["base.locations.countries.continent", "location.location.contains_major_portion_of"], ["location.country.currency_used", "location.country.currency_used"], ["location.location.time_zones", "location.location.time_zones"], ["fictional_universe.fictional_setting.universe", "fictional_universe.fictional_setting.universe"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are near italy [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.03523645177483559, -0.03701307624578476, -0.03999432921409607], "norm_scores": [0.33405953645706177, 0.3334665894508362, 0.33247390389442444]}}
{"id": "WebQTest-212", "question": "which states does the colorado river run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which states does the colorado river run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01203418243676424, -0.01942620426416397, -0.040600307285785675], "norm_scores": [0.3373280465602875, 0.3348437249660492, 0.32782822847366333]}}
{"id": "WebQTest-213", "question": "who did scarlett johansson date", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["film.film.executive_produced_by", "film.producer.films_executive_produced"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.popstra.celebrity.breakup", "base.popstra.breakup.participant"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.nationality", "people.person.nationality"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.person.spouse_s"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["people.person.languages", "people.person.languages"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did scarlett johansson date [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.016648445278406143, -0.022307977080345154, -0.03370941802859306], "norm_scores": [0.3358589708805084, 0.333963543176651, 0.3301774859428406]}}
{"id": "WebQTest-214", "question": "which continents were part of the roman empire", "prediction": [["fictional_universe.fictional_setting.languages", "language.human_language.region"], ["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contained_by"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.country.official_language", "language.human_language.region"], ["location.location.partially_contained_by", "location.location.partiallycontains"], ["location.location.partially_contained_by", "location.partial_containment_relationship.partially_contained_by"], ["book.book_subject.works", "book.written_work.subjects"], ["fictional_universe.fictional_setting.languages", "language.human_language.region"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which continents were part of the roman empire [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.region </PATH>", "<PATH> location.location.partially_contained_by <SEP> location.partial_containment_relationship.partially_contained_by </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.017594795674085617, -0.0313749685883522, -0.0365925207734108], "norm_scores": [0.33698445558547974, 0.3323725759983063, 0.3306429386138916]}}
{"id": "WebQTest-215", "question": "where was elvis costello born", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was elvis costello born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.01670074090361595, -0.018067702651023865, -0.032740406692028046], "norm_scores": [0.3352642059326172, 0.3348062336444855, 0.32992956042289734]}}
{"id": "WebQTest-216", "question": "where are the gobi desert located on a map", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the gobi desert located on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0014614415122196078, -0.06061809882521629, -0.07605338841676712], "norm_scores": [0.34834936261177063, 0.3283398747444153, 0.3233107626438141]}}
{"id": "WebQTest-217", "question": "what type of government does the us follow", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does the us follow [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>"], "scores": [-0.006594700738787651, -0.020341798663139343, -0.16831158101558685], "norm_scores": [0.3524814248085022, 0.34766900539398193, 0.29984959959983826]}}
{"id": "WebQTest-218", "question": "in which continent is germany", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in which continent is germany [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.007553510833531618, -0.03126223012804985, -0.054861247539520264], "norm_scores": [0.3412545323371887, 0.3332589864730835, 0.3254864811897278]}}
{"id": "WebQTest-220", "question": "what language turkey people speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language turkey people speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.007480115629732609, -0.029698044061660767, -0.05132455378770828], "norm_scores": [0.3407004177570343, 0.33321425318717957, 0.32608532905578613]}}
{"id": "WebQTest-221", "question": "who does jeremy lin play for", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.place_of_birth", "sports.sports_team.location"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does jeremy lin play for [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014053580351173878, -0.025633692741394043, -0.03242623805999756], "norm_scores": [0.3366684019565582, 0.3327922224998474, 0.33053937554359436]}}
{"id": "WebQTest-222", "question": "where is the seychelles on world map", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the seychelles on world map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.003989701624959707, -0.04702306166291237, -0.0531674362719059], "norm_scores": [0.34365543723106384, 0.3291804790496826, 0.32716408371925354]}}
{"id": "WebQTest-224", "question": "where did william morris go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did william morris go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015466672368347645, -0.015602579340338707, -0.026582781225442886], "norm_scores": [0.3345813751220703, 0.3345358967781067, 0.3308826982975006]}}
{"id": "WebQTest-225", "question": "what state is mount st. helens in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.mountain.mountain_type"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is mount st. helens in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.mountain.mountain_type </PATH>"], "scores": [-0.004336947575211525, -0.03146907687187195, -0.14406725764274597], "norm_scores": [0.35176271200180054, 0.3423469662666321, 0.305890291929245]}}
{"id": "WebQTest-226", "question": "who is jimmy savile", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is jimmy savile [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0007910385029390454, -0.07205456495285034, -0.07747513800859451], "norm_scores": [0.3499686121940613, 0.32589656114578247, 0.3241347670555115]}}
{"id": "WebQTest-227", "question": "where were the seneca indians located", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were the seneca indians located [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.000739141134545207, -0.08404464274644852, -0.0993155986070633], "norm_scores": [0.3538324534893036, 0.3255506455898285, 0.3206169009208679]}}
{"id": "WebQTest-231", "question": "when did florida marlins join mlb", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.championships"], ["sports.sports_league_participation.league", "sports.sports_league_participation.season"]], "ground_paths": [["baseball.baseball_team.team_stats", "baseball.baseball_team_stats.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did florida marlins join mlb [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_league_participation.league <SEP> sports.sports_league_participation.season </PATH>"], "scores": [-0.016260908916592598, -0.018909646198153496, -0.03856363892555237], "norm_scores": [0.336100697517395, 0.3352116048336029, 0.32868772745132446]}}
{"id": "WebQTest-232", "question": "what countries have spanish as the national language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have spanish as the national language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.01134275272488594, -0.022499671205878258, -0.030858943238854408], "norm_scores": [0.3367481529712677, 0.3330119550228119, 0.33023983240127563]}}
{"id": "WebQTest-233", "question": "what to see in phoenix arizona", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to see in phoenix arizona [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013828491792082787, -0.014054439030587673, -0.03613625466823578], "norm_scores": [0.33582815527915955, 0.33575230836868286, 0.3284195363521576]}}
{"id": "WebQTest-234", "question": "what countries does the panama canal go through", "prediction": [["location.location.containedby"], ["geography.body_of_water.bridges", "location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does the panama canal go through [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> geography.body_of_water.bridges <SEP> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.0010540266521275043, -0.07125312089920044, -0.08089365065097809], "norm_scores": [0.350204735994339, 0.3264637291431427, 0.3233315646648407]}}
{"id": "WebQTest-235", "question": "what is charlie chaplin famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["film.film.directed_by"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is charlie chaplin famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> film.film.directed_by </PATH>"], "scores": [-0.0037536313757300377, -0.034299127757549286, -0.12956734001636505], "norm_scores": [0.3506685793399811, 0.3401191532611847, 0.30921223759651184]}}
{"id": "WebQTest-237", "question": "what did fred durst do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre", "music.artist.genre"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did fred durst do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>"], "scores": [-0.0033418869134038687, -0.057716771960258484, -0.07061879336833954], "norm_scores": [0.34697964787483215, 0.3286164402961731, 0.32440388202667236]}}
{"id": "WebQTest-239", "question": "where do american bulldogs originate from", "prediction": [["biology.animal_breed.breed_of"], ["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.place_of_origin"]], "ground_paths": [["biology.animal_breed.place_of_origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do american bulldogs originate from [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.place_of_origin </PATH>"], "scores": [-0.00850644987076521, -0.01601340062916279, -0.08114629983901978], "norm_scores": [0.3421778678894043, 0.3396187722682953, 0.3182034194469452]}}
{"id": "WebQTest-240", "question": "what colleges did albert einstein teach at", "prediction": [["people.person.employment_history", "business.employer.employees"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["business.employment_tenure.person", "business.employer.employees"], ["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "organization.organization.founders"], ["education.department.academics_old", "education.university.departments"], ["people.person.employment_history", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did albert einstein teach at [/INST]", "raw_output": {"paths": ["<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01670975424349308, -0.029038524255156517, -0.03210700675845146], "norm_scores": [0.33642080426216125, 0.33229860663414, 0.33128052949905396]}}
{"id": "WebQTest-241", "question": "who plays the voice of brian on family guy", "prediction": [["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the voice of brian on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>"], "scores": [-0.022110460326075554, -0.02241756021976471, -0.03121498040854931], "norm_scores": [0.3343777358531952, 0.3342750668525696, 0.3313472270965576]}}
{"id": "WebQTest-243", "question": "where did george w bush live as a child", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george w bush live as a child [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01093711145222187, -0.026350047439336777, -0.0388239324092865], "norm_scores": [0.3381572961807251, 0.3329852521419525, 0.3288574516773224]}}
{"id": "WebQTest-245", "question": "what year was the first miss america pageant held", "prediction": [["time.event.instance_of_recurring_event"], ["time.recurring_event.instances"], ["time.recurring_event.current_frequency"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was the first miss america pageant held [/INST]", "raw_output": {"paths": ["<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>"], "scores": [-0.007885039784014225, -0.02365933358669281, -0.052008338272571564], "norm_scores": [0.3399989604949951, 0.3346778154373169, 0.3253232538700104]}}
{"id": "WebQTest-246", "question": "what was nikola tesla inventions", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.inventor.inventions"], ["symbols.name_source.namesakes"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was nikola tesla inventions [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.0063222916796803474, -0.037830900400877, -0.054789621382951736], "norm_scores": [0.3422698974609375, 0.33165356516838074, 0.32607656717300415]}}
{"id": "WebQTest-247", "question": "where did flemish people come from", "prediction": [["people.ethnicity.geographic_distribution"], ["base.popstra.ethnicity.geographic_distribution"], ["people.ethnicity.included_in_group"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did flemish people come from [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> base.popstra.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>"], "scores": [-7.599715900141746e-05, -0.13121521472930908, -0.13167014718055725], "norm_scores": [0.36313557624816895, 0.3185046315193176, 0.31835976243019104]}}
{"id": "WebQTest-248", "question": "what state new york city belong to", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state new york city belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.011841027066111565, -0.04914278909564018, -0.05058487504720688], "norm_scores": [0.341835618019104, 0.32931944727897644, 0.32884490489959717]}}
{"id": "WebQTest-249", "question": "what was the cause of death for bruce lee", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for bruce lee [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0010751079535111785, -0.04777029529213905, -0.10848033428192139], "norm_scores": [0.35056474804878235, 0.33457139134407043, 0.31486383080482483]}}
{"id": "WebQTest-250", "question": "what type of government does usa follow", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does usa follow [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>"], "scores": [-0.008075812831521034, -0.017437154427170753, -0.15664716064929962], "norm_scores": [0.35055482387542725, 0.34728848934173584, 0.3021567165851593]}}
{"id": "WebQTest-251", "question": "what type of currency do they use in england", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in england [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.002311194781213999, -0.03943889960646629, -0.1166021078824997], "norm_scores": [0.3501950204372406, 0.33743152022361755, 0.31237342953681946]}}
{"id": "WebQTest-252", "question": "where was benjamin franklin educated", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was benjamin franklin educated [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.013249126262962818, -0.02503465861082077, -0.027914127334952354], "norm_scores": [0.33627849817276, 0.3323385417461395, 0.33138296008110046]}}
{"id": "WebQTest-253", "question": "what countries included in the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries included in the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.02422354929149151, -0.025002963840961456, -0.03126268833875656], "norm_scores": [0.33420151472091675, 0.33394116163253784, 0.331857293844223]}}
{"id": "WebQTest-254", "question": "what countries in the world speak chinese", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries in the world speak chinese [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.01825050264596939, -0.021281905472278595, -0.03211801499128342], "norm_scores": [0.3352103531360626, 0.33419573307037354, 0.33059388399124146]}}
{"id": "WebQTest-255", "question": "where obama went to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where obama went to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.010980868712067604, -0.02874743565917015, -0.029987845569849014], "norm_scores": [0.3374316990375519, 0.331489622592926, 0.3310786783695221]}}
{"id": "WebQTest-256", "question": "what is the primary language of china", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the primary language of china [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.0068495930172502995, -0.0371965616941452, -0.03789925575256348], "norm_scores": [0.34018969535827637, 0.33002108335494995, 0.3297892212867737]}}
{"id": "WebQTest-257", "question": "who was the soviet leader during world war ii", "prediction": [["military.military_person.participated_in_conflicts"], ["people.person.nationality"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the soviet leader during world war ii [/INST]", "raw_output": {"paths": ["<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.010958927683532238, -0.013103339821100235, -0.08868078887462616], "norm_scores": [0.342104971408844, 0.34137216210365295, 0.3165229558944702]}}
{"id": "WebQTest-258", "question": "which of the following does australia export the most", "prediction": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which of the following does australia export the most [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.major_exports <SEP> location.imports_exports_by_industry.industry </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.008986164815723896, -0.030061420053243637, -0.03325202688574791], "norm_scores": [0.3383898138999939, 0.331332802772522, 0.33027732372283936]}}
{"id": "WebQTest-260", "question": "where was the city of david", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the city of david [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.005853659473359585, -0.03432253748178482, -0.08036750555038452], "norm_scores": [0.34481245279312134, 0.33513447642326355, 0.3200530707836151]}}
{"id": "WebQTest-261", "question": "what is cindy sherman known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is cindy sherman known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.005722261965274811, -0.03305647894740105, -0.05413130298256874], "norm_scores": [0.341789186000824, 0.33257317543029785, 0.3256376087665558]}}
{"id": "WebQTest-262", "question": "what tv shows did shawnee smith play in", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.performance.actor", "film.film.starring"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what tv shows did shawnee smith play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>"], "scores": [-0.020726019516587257, -0.022731132805347443, -0.032778941094875336], "norm_scores": [0.3348943591117859, 0.33422350883483887, 0.33088213205337524]}}
{"id": "WebQTest-264", "question": "what does australia use for currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does australia use for currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.011384960263967514, -0.01565898396074772, -0.12136466801166534], "norm_scores": [0.3458307981491089, 0.34435585141181946, 0.30981332063674927]}}
{"id": "WebQTest-265", "question": "which airport to fly into rome", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which airport to fly into rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.00606395723298192, -0.029907187446951866, -0.0616409033536911], "norm_scores": [0.3421870768070221, 0.33412471413612366, 0.32368817925453186]}}
{"id": "WebQTest-266", "question": "what is there to do for fun in kansas city", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.place_with_neighborhoods.neighborhoods"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to do for fun in kansas city [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01154292281717062, -0.017222220078110695, -0.03111281804740429], "norm_scores": [0.3361392915248871, 0.33423566818237305, 0.3296250104904175]}}
{"id": "WebQTest-267", "question": "where was martin luther king jr raised", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was martin luther king jr raised [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.000681087956763804, -0.06094876676797867, -0.11063297837972641], "norm_scores": [0.3524365723133087, 0.3318234384059906, 0.3157399296760559]}}
{"id": "WebQTest-268", "question": "what year was lebron james rookie season", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was lebron james rookie season [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.010250522755086422, -0.015257731080055237, -0.03445042297244072], "norm_scores": [0.3365761339664459, 0.3348950147628784, 0.3285287916660309]}}
{"id": "WebQTest-269", "question": "where does delaware river start", "prediction": [["geography.river.origin"], ["location.location.containedby"], ["location.location.partial_containedby"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does delaware river start [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partial_containedby </PATH>"], "scores": [-0.0009191731223836541, -0.08076000958681107, -0.10598187148571014], "norm_scores": [0.3541664183139801, 0.3269888460636139, 0.318844735622406]}}
{"id": "WebQTest-270", "question": "what kind of monarchy does japan have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["royalty.monarch.kingdom"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of monarchy does japan have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> royalty.monarch.kingdom </PATH>"], "scores": [-0.007640712428838015, -0.023927681148052216, -0.04602646455168724], "norm_scores": [0.3394217789173126, 0.3339384198188782, 0.32663974165916443]}}
{"id": "WebQTest-271", "question": "where did mitt romney 's parents come from", "prediction": [["people.person.place_of_birth"], ["people.person.parents"], ["people.person.children"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mitt romney 's parents come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.015498257242143154, -0.026084663346409798, -0.02901308424770832], "norm_scores": [0.33601635694503784, 0.3324779272079468, 0.3315057158470154]}}
{"id": "WebQTest-273", "question": "when did michael jordan return to the nba", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did michael jordan return to the nba [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> common.topic.image <SEP> common.image.appears_in_topic_gallery </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.01100771501660347, -0.023556118831038475, -0.026713678613305092], "norm_scores": [0.3364797830581665, 0.3322838842868805, 0.331236332654953]}}
{"id": "WebQTest-276", "question": "where was mission san rafael located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was mission san rafael located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.0006929083028808236, -0.07318456470966339, -0.10732639580965042], "norm_scores": [0.3534907400608063, 0.3287723660469055, 0.317736953496933]}}
{"id": "WebQTest-277", "question": "who was esther 's husband", "prediction": [["people.marriage.spouse", "people.marriage.spouse"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"]], "ground_paths": [["visual_art.art_subject.artwork_on_the_subject", "visual_art.art_subject.artwork_on_the_subject"], ["people.person.spouse_s", "people.person.spouse_s"], ["visual_art.art_subject.artwork_on_the_subject", "visual_art.artwork.art_subject"], ["common.topic.notable_types", "common.topic.notable_types"], ["book.translated_work.translations", "book.book_subject.works"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was esther 's husband [/INST]", "raw_output": {"paths": ["<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> freebase.valuenotation.has_value <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.029017718508839607, -0.033112071454524994, -0.04819799214601517], "norm_scores": [0.33591803908348083, 0.3345454931259155, 0.32953643798828125]}}
{"id": "WebQTest-278", "question": "who 's dating claire danes", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.infidelity.perpetrator", "base.popstra.infidelity.participant"], ["people.person.profession", "people.person.profession"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who 's dating claire danes [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.018090745434165, -0.026966862380504608, -0.030086124315857887], "norm_scores": [0.33565613627433777, 0.3326900005340576, 0.3316538631916046]}}
{"id": "WebQTest-279", "question": "what county is st paul va in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul va in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.018306061625480652, -0.025361379608511925, -0.031179549172520638], "norm_scores": [0.33555036783218384, 0.33319127559661865, 0.3312583565711975]}}
{"id": "WebQTest-281", "question": "what type of sports do japanese play", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of sports do japanese play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>"], "scores": [-0.009562252089381218, -0.01552958507090807, -0.13171103596687317], "norm_scores": [0.34733471274375916, 0.34526821970939636, 0.3073970079421997]}}
{"id": "WebQTest-282", "question": "which countries share a border with russia", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "location.location.partially_containedby"]], "ground_paths": [["location.location.partially_contains", "location.location.partially_contains"], ["government.form_of_government.countries", "location.country.form_of_government"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.partially_contains", "geography.river.basin_countries"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["base.rivalries.rival.rivals", "base.rivalries.rival.rivals"], ["location.location.partially_containedby", "base.locations.continents.countries_within"], ["location.location.partially_contains", "location.location.containedby"], ["time.event.locations", "time.event.locations"], ["location.country.languages_spoken", "language.human_language.main_country"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["language.human_language.main_country", "language.human_language.countries_spoken_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "base.locations.countries.continent"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_containedby", "location.location.containedby"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.events", "location.location.events"], ["base.locations.continents.countries_within", "base.locations.continents.countries_within"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["location.location.containedby", "location.location.containedby"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.location.partially_contains", "geography.river.origin"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "geography.lake.basin_countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["base.locations.continents.countries_within", "base.locations.countries.continent"], ["base.locations.continents.countries_within", "location.location.partially_contains"], ["language.human_language.main_country", "location.country.languages_spoken"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.languages_spoken", "location.country.official_language"], ["time.event.locations", "location.location.events"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["sports.multi_event_tournament.participating_countries", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.rivalries.rival.rivals", "base.rivalries.rivalry.rival"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["geography.river.basin_countries", "location.location.partially_contains"], ["location.location.partially_containedby", "geography.river.origin"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["organization.organization.founders", "organization.organization.founders"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.time_zones", "time.time_zone.locations_in_this_time_zone"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["location.location.partially_containedby", "base.schemastaging.disputed_location.claimed_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries share a border with russia [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>"], "scores": [-0.03556356951594353, -0.036956075578927994, -0.04149729758501053], "norm_scores": [0.33414727449417114, 0.3336822986602783, 0.33217039704322815]}}
{"id": "WebQTest-284", "question": "where did the latin language originate from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the latin language originate from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.00485164625570178, -0.03279950097203255, -0.050736065953969955], "norm_scores": [0.3415777385234833, 0.3321635127067566, 0.3262587785720825]}}
{"id": "WebQTest-286", "question": "what school did karl benz go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did karl benz go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.017585260793566704, -0.020904146134853363, -0.021048784255981445], "norm_scores": [0.3340873420238495, 0.3329803943634033, 0.3329322338104248]}}
{"id": "WebQTest-287", "question": "what movies has carmen electra been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["film.film_character.portrayed_in_films", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has carmen electra been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.021229930222034454, -0.03751144930720329, -0.03796464949846268], "norm_scores": [0.33701184391975403, 0.33156922459602356, 0.3314189612865448]}}
{"id": "WebQTest-289", "question": "when do world war ii end", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when do world war ii end [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.009728704579174519, -0.035311855375766754, -0.04381716623902321], "norm_scores": [0.3399941325187683, 0.3314063251018524, 0.32859957218170166]}}
{"id": "WebQTest-291", "question": "where did morgan freeman graduate", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did morgan freeman graduate [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.012913700193166733, -0.012938248924911022, -0.040972400456666946], "norm_scores": [0.3364390432834625, 0.33643078804016113, 0.32713019847869873]}}
{"id": "WebQTest-294", "question": "who was carolus linnaeus", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was carolus linnaeus [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0008631148375570774, -0.07102327793836594, -0.07835865020751953], "norm_scores": [0.34993475675582886, 0.32622474431991577, 0.32384052872657776]}}
{"id": "WebQTest-295", "question": "what type of government does germany have now", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does germany have now [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009210927411913872, -0.014999163337051868, -0.14042434096336365], "norm_scores": [0.34827926754951477, 0.3462691605091095, 0.30545157194137573]}}
{"id": "WebQTest-296", "question": "what kind of government does libya have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does libya have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.005730032920837402, -0.021116157993674278, -0.1479133665561676], "norm_scores": [0.350607305765152, 0.345254123210907, 0.3041386008262634]}}
{"id": "WebQTest-297", "question": "what time zone is anaheim california", "prediction": [["location.location.time_zones"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is anaheim california [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.044400687533198e-05, -0.1723269373178482, -0.182396799325943], "norm_scores": [0.3738328814506531, 0.3146599233150482, 0.3115071952342987]}}
{"id": "WebQTest-298", "question": "what year did the orioles go to the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the orioles go to the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.007048764266073704, -0.017593083903193474, -0.07528433948755264], "norm_scores": [0.3420497179031372, 0.3384619951248169, 0.3194882571697235]}}
{"id": "WebQTest-300", "question": "who is sir francis bacon", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is sir francis bacon [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.00010518470662645996, -0.11803019791841507, -0.12730875611305237], "norm_scores": [0.36109983921051025, 0.3209320902824402, 0.3179680407047272]}}
{"id": "WebQTest-301", "question": "what were amelia earhart 's achievements", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["base.activism.activist.area_of_activism"]], "ground_paths": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what were amelia earhart 's achievements [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.006186417303979397, -0.033627692610025406, -0.05182267725467682], "norm_scores": [0.34149259328842163, 0.3322490155696869, 0.3262583911418915]}}
{"id": "WebQTest-302", "question": "what are the official colors of the dallas cowboys", "prediction": [["sports.sports_team.colors"], ["sports.sports_team.colors", "sports.sports_team.colors"], ["sports.sports_team_colors.team"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official colors of the dallas cowboys [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.colors <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team_colors.team </PATH>"], "scores": [-0.00011491386248962954, -0.0957033708691597, -0.11335193365812302], "norm_scores": [0.3569163680076599, 0.3243791460990906, 0.3187045156955719]}}
{"id": "WebQTest-303", "question": "where did margaret hoover go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did margaret hoover go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.00912833958864212, -0.02551678568124771, -0.02786676026880741], "norm_scores": [0.3372475206851959, 0.3317655920982361, 0.330986887216568]}}
{"id": "WebQTest-304", "question": "what is jamie oliver children names", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jamie oliver children names [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.011575018987059593, -0.016249466687440872, -0.1554788500070572], "norm_scores": [0.34949055314064026, 0.3478606939315796, 0.3026486933231354]}}
{"id": "WebQTest-305", "question": "where does bradley walsh live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.places_lived", "people.place_lived.location"], ["location.location.people_born_here"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does bradley walsh live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.017480887472629547, -0.017938509583473206, -0.034579988569021225], "norm_scores": [0.3352791965007782, 0.3351258337497711, 0.3295949697494507]}}
{"id": "WebQTest-306", "question": "who is the head coach of inter milan", "prediction": [["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["base.schemastaging.sports_team_extra.managers", "base.schemastaging.sports_team_manager_tenure.manager"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"], ["base.schemastaging.sports_team_manager_tenure.team", "base.schemastaging.sports_team_manager_tenure.manager"], ["soccer.football_team_management_tenure.team", "soccer.football_team_manager.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the head coach of inter milan [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> base.schemastaging.sports_team_extra.managers <SEP> base.schemastaging.sports_team_manager_tenure.manager </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.020064465701580048, -0.022676942870020866, -0.02488023228943348], "norm_scores": [0.3341590464115143, 0.33328720927238464, 0.3325536847114563]}}
{"id": "WebQTest-308", "question": "who did reese witherspoon get married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did reese witherspoon get married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.017637161538004875, -0.028088262304663658, -0.031028617173433304], "norm_scores": [0.33598750829696655, 0.33249431848526, 0.33151814341545105]}}
{"id": "WebQTest-310", "question": "what kind of money should i take to costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money should i take to costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.006824721582233906, -0.02072126232087612, -0.11912402510643005], "norm_scores": [0.3472250699996948, 0.34243321418762207, 0.3103417158126831]}}
{"id": "WebQTest-311", "question": "where is the ufc headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["sports.sports_league.sport", "sports.sports_team.location"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["common.topic.subject_of", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the ufc headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> sports.sports_league.sport <SEP> sports.sports_team.location </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>"], "scores": [-0.009403098374605179, -0.037972066551446915, -0.0472973994910717], "norm_scores": [0.3407561480998993, 0.331158846616745, 0.3280850350856781]}}
{"id": "WebQTest-312", "question": "who plays captain kirk in star trek", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays captain kirk in star trek [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.026248035952448845, -0.027742305770516396, -0.0284428708255291], "norm_scores": [0.3337433338165283, 0.33324500918388367, 0.3330116271972656]}}
{"id": "WebQTest-313", "question": "who does albert pujols play for 2012", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does albert pujols play for 2012 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>"], "scores": [-0.011294684372842312, -0.022896181792020798, -0.02642415091395378], "norm_scores": [0.33630967140197754, 0.33243054151535034, 0.3312597870826721]}}
{"id": "WebQTest-314", "question": "what kind of government is sweden", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is sweden [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.01153345126658678, -0.013090136460959911, -0.14619898796081543], "norm_scores": [0.3481345474720001, 0.3475930094718933, 0.30427244305610657]}}
{"id": "WebQTest-315", "question": "where was country singer george jones born", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was country singer george jones born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0106413085013628, -0.01851203478872776, -0.05142625793814659], "norm_scores": [0.33873093128204346, 0.3360753655433655, 0.32519373297691345]}}
{"id": "WebQTest-316", "question": "what did shakespeare become famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did shakespeare become famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.005264324136078358, -0.033455051481723785, -0.05598946288228035], "norm_scores": [0.3421443700790405, 0.33263373374938965, 0.32522186636924744]}}
{"id": "WebQTest-317", "question": "where does russia import from", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does russia import from [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.027510251849889755, -0.02933926321566105, -0.030788052827119827], "norm_scores": [0.33390092849731445, 0.33329081535339355, 0.3328082859516144]}}
{"id": "WebQTest-318", "question": "what is the money of switzerland called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the money of switzerland called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency </PATH>"], "scores": [-0.003908462822437286, -0.03153446689248085, -0.16530685126781464], "norm_scores": [0.3541446328163147, 0.34449490904808044, 0.30136042833328247]}}
{"id": "WebQTest-321", "question": "what do people in australia speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in australia speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.006711448077112436, -0.022872256115078926, -0.07176753133535385], "norm_scores": [0.34235042333602905, 0.3368622064590454, 0.3207874298095703]}}
{"id": "WebQTest-322", "question": "what has ian somerhalder acted in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.performance.actor", "film.film.starring"], ["film.film.produced_by"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has ian somerhalder acted in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.01836332120001316, -0.03113834746181965, -0.034840311855077744], "norm_scores": [0.33659103512763977, 0.33231842517852783, 0.3310904800891876]}}
{"id": "WebQTest-323", "question": "what movies did ron howard director", "prediction": [["film.film.directed_by"], ["film.film.produced_by"], ["film.director.film"]], "ground_paths": [["film.director.film"], ["film.film.produced_by"], ["film.film.directed_by"], ["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did ron howard director [/INST]", "raw_output": {"paths": ["<PATH> film.film.directed_by </PATH>", "<PATH> film.film.produced_by </PATH>", "<PATH> film.director.film </PATH>"], "scores": [-0.022693725302815437, -0.03201766684651375, -0.033082857728004456], "norm_scores": [0.335527241230011, 0.33241337537765503, 0.33205944299697876]}}
{"id": "WebQTest-326", "question": "what did gerald r ford die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did gerald r ford die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.0012683018576353788, -0.048256516456604004, -0.10342425107955933], "norm_scores": [0.35001903772354126, 0.3339526951313019, 0.31602826714515686]}}
{"id": "WebQTest-327", "question": "what two continents is turkey on", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["base.locations.countries.continent"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what two continents is turkey on [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.011804109439253807, -0.029284827411174774, -0.04837552458047867], "norm_scores": [0.3393557071685791, 0.3334750831127167, 0.3271692097187042]}}
{"id": "WebQTest-328", "question": "where did kevin love go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kevin love go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.01629754528403282, -0.021234923973679543, -0.026009898632764816], "norm_scores": [0.3349624276161194, 0.33331266045570374, 0.3317249119281769]}}
{"id": "WebQTest-329", "question": "what type of government was formed when italy unified", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["government.form_of_government.countries", "government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government was formed when italy unified [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.01026125531643629, -0.01288442499935627, -0.11123902350664139], "norm_scores": [0.3446691036224365, 0.3437661826610565, 0.31156474351882935]}}
{"id": "WebQTest-331", "question": "where was the temple of karnak built", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["architecture.structure.architect"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the temple of karnak built [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> architecture.structure.architect </PATH>"], "scores": [-0.0017458173679187894, -0.06523927301168442, -0.07632296532392502], "norm_scores": [0.3488433361053467, 0.3273825943470001, 0.3237740099430084]}}
{"id": "WebQTest-333", "question": "what movies did morgan freeman star in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["film.producer.films_executive_produced"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["common.topic.notable_types", "common.topic.notable_types"], ["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["theater.theater_actor.theater_roles", "theater.theater_role.play"], ["film.film.executive_produced_by"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did morgan freeman star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.017674114555120468, -0.03784990310668945, -0.039344463497400284], "norm_scores": [0.3379989564418793, 0.33124789595603943, 0.3307531774044037]}}
{"id": "WebQTest-334", "question": "who did john kennedy have affairs with", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["music.recording.featured_artists", "music.recording.artist"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did john kennedy have affairs with [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.020576534792780876, -0.033120039850473404, -0.03358958661556244], "norm_scores": [0.3361789584159851, 0.33198845386505127, 0.3318325877189636]}}
{"id": "WebQTest-335", "question": "what type of money to take to cuba", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of money to take to cuba [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.010979871265590191, -0.015113958157598972, -0.10992247611284256], "norm_scores": [0.3446292579174042, 0.343207448720932, 0.3121633231639862]}}
{"id": "WebQTest-336", "question": "what currency should you take to morocco", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should you take to morocco [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.011543001048266888, -0.014413627795875072, -0.10962409526109695], "norm_scores": [0.34438714385032654, 0.34339994192123413, 0.31221288442611694]}}
{"id": "WebQTest-337", "question": "who is mary mcleod bethune for kids", "prediction": [["people.person.profession"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is mary mcleod bethune for kids [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.0005289179389365017, -0.08408214896917343, -0.08536169677972794], "norm_scores": [0.35229775309562683, 0.32405829429626465, 0.32364392280578613]}}
{"id": "WebQTest-338", "question": "what do they call money in japan", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they call money in japan [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.012995055876672268, -0.018224267289042473, -0.03830672428011894], "norm_scores": [0.33672410249710083, 0.3349679112434387, 0.32830801606178284]}}
{"id": "WebQTest-339", "question": "what nationality is mother teresa", "prediction": [["people.person.nationality"], ["people.person.ethnicity"], ["people.ethnicity.people"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is mother teresa [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.006228034384548664, -0.03564406558871269, -0.06136208772659302], "norm_scores": [0.3427744209766388, 0.3328382074832916, 0.32438740134239197]}}
{"id": "WebQTest-340", "question": "who is richard pryor son", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is richard pryor son [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.003438670886680484, -0.03768429160118103, -0.09595771133899689], "norm_scores": [0.3474676311016083, 0.3357698321342468, 0.3167625665664673]}}
{"id": "WebQTest-341", "question": "what countries share borders with spain", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"]], "ground_paths": [["location.location.partially_contains", "location.location.partially_contains"], ["government.form_of_government.countries", "location.country.form_of_government"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.containedby", "location.location.partially_containedby"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_contains", "location.location.contains"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["finance.currency.countries_used", "location.country.currency_used"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_contains", "location.location.primarily_containedby"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.partially_containedby", "location.location.containedby"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["royalty.royal_line.kingdom_s_ruled", "people.family.country"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["film.film.country", "film.film.country"], ["location.location.primarily_containedby", "base.locations.continents.countries_within"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["location.country.form_of_government", "government.form_of_government.countries"], ["freebase.valuenotation.has_value", "freebase.valuenotation.is_reviewed"], ["location.location.primarily_containedby", "location.location.contains_major_portion_of"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["location.country.currency_formerly_used", "finance.currency.countries_formerly_used"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.primarily_containedby", "location.location.containedby"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.country.internet_tld", "location.country.internet_tld"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["geography.river.basin_countries", "location.location.partially_contains"], ["organization.organization.founders", "organization.organization.founders"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["location.location.primarily_containedby", "location.location.primarily_containedby"], ["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries share borders with spain [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.03523505479097366, -0.040529362857341766, -0.04514002427458763], "norm_scores": [0.3350236713886261, 0.3332546353340149, 0.3317216634750366]}}
{"id": "WebQTest-342", "question": "what countries have english as their official language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have english as their official language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016910500824451447, -0.019798733294010162, -0.02342105098068714], "norm_scores": [0.33437809348106384, 0.3334137201309204, 0.33220818638801575]}}
{"id": "WebQTest-343", "question": "what international organizations is china part of", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization_member.member_of", "organization.organization_membership.organization"], ["organization.organization_membership.member", "organization.organization_membership.organization"]], "ground_paths": [["organization.organization_membership.member", "organization.organization_membership.organization"], ["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization_membership.member", "organization.membership_organization.members"], ["organization.organization_member.member_of", "organization.organization_membership.organization"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what international organizations is china part of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization_member.member_of <SEP> organization.organization_membership.organization </PATH>", "<PATH> organization.organization_membership.member <SEP> organization.organization_membership.organization </PATH>"], "scores": [-0.025229057297110558, -0.026249684393405914, -0.02810269594192505], "norm_scores": [0.33376607298851013, 0.3334256112575531, 0.33280831575393677]}}
{"id": "WebQTest-344", "question": "what county is west st paul in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is west st paul in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.01374482735991478, -0.02492496371269226, -0.040844451636075974], "norm_scores": [0.33759304881095886, 0.3338397145271301, 0.3285672664642334]}}
{"id": "WebQTest-346", "question": "who does donnie wahlberg play in the sixth sense", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.film.starring", "film.film_character.portrayed_in_films"], ["film.performance.film", "film.performance.character"]], "ground_paths": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does donnie wahlberg play in the sixth sense [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film.starring <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>"], "scores": [-0.02046758122742176, -0.03023255057632923, -0.03668452426791191], "norm_scores": [0.3362252712249756, 0.33295801281929016, 0.33081668615341187]}}
{"id": "WebQTest-347", "question": "what did baron de montesquie influence", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did baron de montesquie influence [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.004848892334848642, -0.02315245196223259, -0.10736868530511856], "norm_scores": [0.34668976068496704, 0.3404017984867096, 0.312908411026001]}}
{"id": "WebQTest-349", "question": "who did carlos boozer play for", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did carlos boozer play for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.009615148417651653, -0.015189419500529766, -0.053536806255578995], "norm_scores": [0.3388141989707947, 0.3369308114051819, 0.32425498962402344]}}
{"id": "WebQTest-351", "question": "who owns the portland press herald", "prediction": [["book.newspaper.owner"], ["periodicals.newspaper_circulation_area.newspapers", "business.business_operation.industry"], ["book.newspaper_owner.newspapers_owned"]], "ground_paths": [["book.newspaper_owner.newspapers_owned"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns the portland press herald [/INST]", "raw_output": {"paths": ["<PATH> book.newspaper.owner </PATH>", "<PATH> periodicals.newspaper_circulation_area.newspapers <SEP> business.business_operation.industry </PATH>", "<PATH> book.newspaper_owner.newspapers_owned </PATH>"], "scores": [-0.022233054041862488, -0.037801630795001984, -0.039494890719652176], "norm_scores": [0.33699101209640503, 0.3317851722240448, 0.33122384548187256]}}
{"id": "WebQTest-352", "question": "what did albert speer design", "prediction": [["architecture.architect.structures_designed"], ["architecture.structure.architect"], ["architecture.architecture.architect"]], "ground_paths": [["architecture.structure.architect"], ["architecture.architect.structures_designed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did albert speer design [/INST]", "raw_output": {"paths": ["<PATH> architecture.architect.structures_designed </PATH>", "<PATH> architecture.structure.architect </PATH>", "<PATH> architecture.architecture.architect </PATH>"], "scores": [-0.0005806020344607532, -0.08583181351423264, -0.10849443078041077], "norm_scores": [0.3551153242588043, 0.326095849275589, 0.3187887966632843]}}
{"id": "WebQTest-354", "question": "what art movement did leonardo da vinci belong to", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what art movement did leonardo da vinci belong to [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.004015048034489155, -0.02565457671880722, -0.062463004142045975], "norm_scores": [0.3422524034976959, 0.3349258005619049, 0.32282182574272156]}}
{"id": "WebQTest-356", "question": "what airport is near arlington tx", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport is near arlington tx [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011029399931430817, -0.021680083125829697, -0.042185310274362564], "norm_scores": [0.33798283338546753, 0.3344022035598755, 0.32761502265930176]}}
{"id": "WebQTest-359", "question": "what team does heskey play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does heskey play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.02822270430624485, -0.029089080169796944, -0.03293710947036743], "norm_scores": [0.33395329117774963, 0.33366408944129944, 0.3323826193809509]}}
{"id": "WebQTest-360", "question": "where did emperor hirohito live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did emperor hirohito live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>"], "scores": [-0.023698024451732635, -0.025751912966370583, -0.027191169559955597], "norm_scores": [0.3339499235153198, 0.33326470851898193, 0.33278539776802063]}}
{"id": "WebQTest-361", "question": "what to do and see in vienna austria", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["base.arthist.int_zm_nyek.helynev"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do and see in vienna austria [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013609899207949638, -0.017794329673051834, -0.02581223100423813], "norm_scores": [0.33515477180480957, 0.3337552547454834, 0.33108994364738464]}}
{"id": "WebQTest-362", "question": "when did conflict start in ireland", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["location.location.events"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did conflict start in ireland [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.004669526591897011, -0.04405616223812103, -0.05162428319454193], "norm_scores": [0.34299319982528687, 0.32974645495414734, 0.3272603154182434]}}
{"id": "WebQTest-365", "question": "what is there to do in peoria illinois", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["location.location.contains"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to do in peoria illinois [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.010250682011246681, -0.018025659024715424, -0.04256463050842285], "norm_scores": [0.33778560161590576, 0.3351694941520691, 0.32704487442970276]}}
{"id": "WebQTest-366", "question": "what language do british speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do british speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01648060791194439, -0.020109189674258232, -0.03766040876507759], "norm_scores": [0.3360868990421295, 0.3348695933818817, 0.32904350757598877]}}
{"id": "WebQTest-367", "question": "what makes elvis presley famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre"]], "ground_paths": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what makes elvis presley famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre </PATH>"], "scores": [-0.0043255845084786415, -0.042485810816287994, -0.06825311481952667], "norm_scores": [0.3447524905204773, 0.3318445384502411, 0.323403000831604]}}
{"id": "WebQTest-370", "question": "what language do people speak in the netherlands", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in the netherlands [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01346267107874155, -0.016436630859971046, -0.04173579812049866], "norm_scores": [0.3367963433265686, 0.33579620718955994, 0.3274074196815491]}}
{"id": "WebQTest-371", "question": "what are the major languages spoken in greece", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major languages spoken in greece [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016957534477114677, -0.018641028553247452, -0.026356693357229233], "norm_scores": [0.334564208984375, 0.33400145173072815, 0.33143430948257446]}}
{"id": "WebQTest-374", "question": "what type of artist is henri matisse", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_art_form.artists"], ["visual_art.art_form.artists"]], "ground_paths": [["visual_art.visual_art_form.artists"], ["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of artist is henri matisse [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>", "<PATH> visual_art.art_form.artists </PATH>"], "scores": [-0.0006438438431359828, -0.07165791839361191, -0.08945085853338242], "norm_scores": [0.35131222009658813, 0.3272293210029602, 0.32145845890045166]}}
{"id": "WebQTest-375", "question": "what was the first book charles dickens wrote", "prediction": [["book.written_work.author"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the first book charles dickens wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>"], "scores": [-0.00290249427780509, -0.03553957864642143, -0.05640801042318344], "norm_scores": [0.3429601490497589, 0.3319476246833801, 0.3250921666622162]}}
{"id": "WebQTest-376", "question": "what books did agatha christie wrote", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["book.written_work.author"], ["film.film_story_contributor.film_story_credits"], ["film.film.story_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did agatha christie wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.015089800581336021, -0.01830693893134594, -0.03973594680428505], "norm_scores": [0.3364236056804657, 0.33534300327301025, 0.32823339104652405]}}
{"id": "WebQTest-377", "question": "what school did michael jordan attend", "prediction": [["people.person.education", "education.education.institution"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did michael jordan attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.school </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01862332783639431, -0.022216249257326126, -0.025488534942269325], "norm_scores": [0.33449605107307434, 0.33329638838768005, 0.3322075605392456]}}
{"id": "WebQTest-379", "question": "what year does hitler die", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death"], ["base.activism.activist.area_of_activism"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year does hitler die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.013868334703147411, -0.020695634186267853, -0.03754453361034393], "norm_scores": [0.336723268032074, 0.3344321846961975, 0.3288446068763733]}}
{"id": "WebQTest-380", "question": "where is mount st helens volcano", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mount st helens volcano [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>"], "scores": [-0.007908967323601246, -0.028226282447576523, -0.04659322649240494], "norm_scores": [0.33991149067878723, 0.33307507634162903, 0.32701337337493896]}}
{"id": "WebQTest-381", "question": "what shows are shot in new york", "prediction": [["film.film.featured_film_locations"], ["tv.tv_program.filming_locations"], ["tv.tv_location.tv_shows_filmed_here"]], "ground_paths": [["tv.tv_program.filming_locations"], ["tv.tv_location.tv_shows_filmed_here"], ["film.film.featured_film_locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what shows are shot in new york [/INST]", "raw_output": {"paths": ["<PATH> film.film.featured_film_locations </PATH>", "<PATH> tv.tv_program.filming_locations </PATH>", "<PATH> tv.tv_location.tv_shows_filmed_here </PATH>"], "scores": [-0.015283872373402119, -0.022170329466462135, -0.022692229598760605], "norm_scores": [0.33492353558540344, 0.33262500166893005, 0.3324514627456665]}}
{"id": "WebQTest-382", "question": "what book did tupac write", "prediction": [["book.book_edition.author_editor"], ["book.written_work.author"], ["book.author.works_written"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did tupac write [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.013969476334750652, -0.019698766991496086, -0.03881024569272995], "norm_scores": [0.33672842383384705, 0.3348047435283661, 0.32846686244010925]}}
{"id": "WebQTest-383", "question": "what language do chinese people write in", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.writing_system"], ["language.language_writing_system.languages"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do chinese people write in [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.008547883480787277, -0.03396117314696312, -0.038075171411037445], "norm_scores": [0.33946505188941956, 0.33094683289527893, 0.3295881152153015]}}
{"id": "WebQTest-384", "question": "what was robert burns famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["influence.influence_node.influenced_by", "people.person.profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was robert burns famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.profession </PATH>"], "scores": [-0.00609091529622674, -0.02711508609354496, -0.07147886604070663], "norm_scores": [0.3429473638534546, 0.33581244945526123, 0.3212401866912842]}}
{"id": "WebQTest-385", "question": "when did william mckinley died", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did william mckinley died [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.003935947548598051, -0.03459911048412323, -0.053459011018276215], "norm_scores": [0.34229162335395813, 0.3319551646709442, 0.32575321197509766]}}
{"id": "WebQTest-386", "question": "how deep is lake merritt oakland", "prediction": [["geography.lake.basin_countries"], ["location.location.containedby"], ["geography.lake.depth"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how deep is lake merritt oakland [/INST]", "raw_output": {"paths": ["<PATH> geography.lake.basin_countries </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> geography.lake.depth </PATH>"], "scores": [-0.011797353625297546, -0.040634885430336, -0.04414599761366844], "norm_scores": [0.34016576409339905, 0.33049631118774414, 0.3293379545211792]}}
{"id": "WebQTest-388", "question": "what are the important holidays of islam", "prediction": [["time.holiday.featured_in_religions"], ["religion.religion.holidays"], ["location.location.events"]], "ground_paths": [["time.holiday.featured_in_religions"], ["religion.religion.holidays"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the important holidays of islam [/INST]", "raw_output": {"paths": ["<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> religion.religion.holidays </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.007993224076926708, -0.016869327053427696, -0.14372071623802185], "norm_scores": [0.34913232922554016, 0.3460471034049988, 0.3048206567764282]}}
{"id": "WebQTest-389", "question": "what type of music did claude debussy play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did claude debussy play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.004381190054118633, -0.02948271855711937, -0.09579891711473465], "norm_scores": [0.34627866744995117, 0.3376947343349457, 0.3160265386104584]}}
{"id": "WebQTest-390", "question": "when was the last time the toronto maple leafs were in the stanley cup finals", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the toronto maple leafs were in the stanley cup finals [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.01024605706334114, -0.014889668673276901, -0.02667902782559395], "norm_scores": [0.3356753885746002, 0.3341202735900879, 0.3302043378353119]}}
{"id": "WebQTest-392", "question": "what team is hank baskett on 2010", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is hank baskett on 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.014776887372136116, -0.01962040737271309, -0.021524032577872276], "norm_scores": [0.3346223533153534, 0.33300548791885376, 0.33237218856811523]}}
{"id": "WebQTest-393", "question": "what are the two official languages of paraguay", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the two official languages of paraguay [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.015246345661580563, -0.02529621124267578, -0.027077196165919304], "norm_scores": [0.33576884865760803, 0.3324113190174103, 0.3318198025226593]}}
{"id": "WebQTest-395", "question": "what state is the steelers from", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.location", "location.location.containedby"]], "ground_paths": [["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is the steelers from [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.location <SEP> location.location.containedby </PATH>"], "scores": [-0.0037425211630761623, -0.03267861157655716, -0.0969904437661171], "norm_scores": [0.34692758321762085, 0.337032675743103, 0.3160397708415985]}}
{"id": "WebQTest-396", "question": "what college did sir isaac newton go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did sir isaac newton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.010143354535102844, -0.0184556245803833, -0.02658850885927677], "norm_scores": [0.3360879719257355, 0.33330589532852173, 0.3306061625480652]}}
{"id": "WebQTest-397", "question": "what are the school colors for harvard university", "prediction": [["education.educational_institution.colors"], ["education.school_mascot.school"], ["education.educational_institution.colors", "education.color.colors"]], "ground_paths": [["education.educational_institution.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the school colors for harvard university [/INST]", "raw_output": {"paths": ["<PATH> education.educational_institution.colors </PATH>", "<PATH> education.school_mascot.school </PATH>", "<PATH> education.educational_institution.colors <SEP> education.color.colors </PATH>"], "scores": [-0.003178544109687209, -0.04857593774795532, -0.06519332528114319], "norm_scores": [0.34536510705947876, 0.330036997795105, 0.32459795475006104]}}
{"id": "WebQTest-398", "question": "where is the university of the rockies located", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the university of the rockies located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.002846603747457266, -0.04973011091351509, -0.06087071821093559], "norm_scores": [0.34508630633354187, 0.3292808532714844, 0.32563284039497375]}}
{"id": "WebQTest-399", "question": "what disease does robin roberts have", "prediction": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"], ["people.person.medical_condition"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease does robin roberts have [/INST]", "raw_output": {"paths": ["<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.010470923036336899, -0.012334035709500313, -0.09633415937423706], "norm_scores": [0.3429521918296814, 0.34231382608413696, 0.31473398208618164]}}
{"id": "WebQTest-400", "question": "what was the capital city of the east roman empire", "prediction": [["location.location.containedby"], ["location.country.capital"], ["location.location.contains"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the capital city of the east roman empire [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.country.capital </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0067308819852769375, -0.030088242143392563, -0.04922562092542648], "norm_scores": [0.34067967534065247, 0.332814484834671, 0.3265058696269989]}}
{"id": "WebQTest-401", "question": "who plays edward scissorhands", "prediction": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays edward scissorhands [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.017099274322390556, -0.042769905179739, -0.04920938238501549], "norm_scores": [0.33978286385536194, 0.3311714231967926, 0.32904571294784546]}}
{"id": "WebQTest-402", "question": "what does joey jordison play in slipknot", "prediction": [["music.group_member.instruments_played"], ["music.group_membership.group", "music.group_membership.role"], ["music.group_member.instruments_played", "music.group_member.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"], ["music.group_membership.group", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does joey jordison play in slipknot [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>"], "scores": [-0.018039913848042488, -0.01839834451675415, -0.04132137447595596], "norm_scores": [0.33595046401023865, 0.33583009243011475, 0.3282194137573242]}}
{"id": "WebQTest-403", "question": "what kind of language does turkey speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does turkey speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.006892270874232054, -0.030322464182972908, -0.046754803508520126], "norm_scores": [0.3403949737548828, 0.33251217007637024, 0.32709285616874695]}}
{"id": "WebQTest-404", "question": "what was lebron james first team", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "ground_paths": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was lebron james first team [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>"], "scores": [-0.013913404196500778, -0.01640239916741848, -0.029335813596844673], "norm_scores": [0.3353218138217926, 0.3344882130622864, 0.3301900029182434]}}
{"id": "WebQTest-405", "question": "who fought the battle of gettysburg", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"], ["military.military_person.participated_in_conflicts"]], "ground_paths": [["military.military_person.participated_in_conflicts", "people.person.nationality"], ["military.military_person.participated_in_conflicts"], ["military.military_conflict.military_personnel_involved", "people.person.nationality"], ["time.event.included_in_event", "time.event.locations"], ["location.location.events", "location.administrative_division.second_level_division_of"], ["base.culturalevent.event.entity_involved"], ["military.military_command.military_conflict", "military.military_combatant.military_commanders"], ["location.location.events", "location.administrative_division.first_level_division_of"], ["book.written_work.subjects", "book.written_work.subjects"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["base.culturalevent.event.entity_involved", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought the battle of gettysburg [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>"], "scores": [-0.00661573838442564, -0.026984501630067825, -0.04638098552823067], "norm_scores": [0.3400374948978424, 0.3331814110279083, 0.32678115367889404]}}
{"id": "WebQTest-406", "question": "who is the coach of the sf giants", "prediction": [["baseball.baseball_team.current_coaches"], ["baseball.baseball_team.current_manager"], ["baseball.baseball_team.current_coach"]], "ground_paths": [["baseball.current_coaching_tenure.baseball_team", "baseball.current_coaching_tenure.baseball_coach"], ["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_coaching"], ["baseball.current_coaching_tenure.baseball_team", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the coach of the sf giants [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.current_coaches </PATH>", "<PATH> baseball.baseball_team.current_manager </PATH>", "<PATH> baseball.baseball_team.current_coach </PATH>"], "scores": [-0.0108146658167243, -0.030410269275307655, -0.04726686701178551], "norm_scores": [0.339581698179245, 0.33299216628074646, 0.32742613554000854]}}
{"id": "WebQTest-408", "question": "who was the first president of the afl", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "people.person.nationality"]], "ground_paths": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the first president of the afl [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>"], "scores": [-0.0006133459974080324, -0.07960569113492966, -0.08686310052871704], "norm_scores": [0.3519376814365387, 0.3252069652080536, 0.3228553533554077]}}
{"id": "WebQTest-409", "question": "where did giuliana rancic grow up", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did giuliana rancic grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0034815939143300056, -0.04086962342262268, -0.048151057213544846], "norm_scores": [0.34251081943511963, 0.32994145154953003, 0.32754772901535034]}}
{"id": "WebQTest-410", "question": "what are the major cities in ukraine", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major cities in ukraine [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.020654916763305664, -0.023771703243255615, -0.031160207465291023], "norm_scores": [0.334847092628479, 0.3338050842285156, 0.33134785294532776]}}
{"id": "WebQTest-414", "question": "who does brian dawkins play for 2011", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does brian dawkins play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.012067463248968124, -0.013354018330574036, -0.05192135274410248], "norm_scores": [0.33787843585014343, 0.3374440371990204, 0.3246774971485138]}}
{"id": "WebQTest-415", "question": "what movies did tupac star in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["music.recording.artist", "music.recording.releases"], ["music.producer.releases_produced"], ["film.performance.actor", "film.film.starring"], ["music.featured_artist.recordings", "music.recording.releases"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did tupac star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.022208206355571747, -0.022598911076784134, -0.03984306752681732], "norm_scores": [0.33533087372779846, 0.33519989252090454, 0.3294692039489746]}}
{"id": "WebQTest-416", "question": "what form of currency does china have", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of currency does china have [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.012242747470736504, -0.01405735220760107, -0.12074011564254761], "norm_scores": [0.34537920355796814, 0.34475305676460266, 0.3098677396774292]}}
{"id": "WebQTest-417", "question": "who plays kenneth", "prediction": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays kenneth [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.02352655678987503, -0.024071842432022095, -0.03133696690201759], "norm_scores": [0.3342609107494354, 0.33407872915267944, 0.3316603899002075]}}
{"id": "WebQTest-418", "question": "what league are the indianapolis colts in", "prediction": [["american_football.football_team.division"], ["sports.sports_team.league"], ["american_football.football_team.current_division"]], "ground_paths": [["sports.sports_league_participation.team", "sports.sports_league_participation.league"], ["american_football.football_team.division"], ["sports.sports_team.sport", "sports.sports_league.sport"], ["american_football.football_team.conference", "american_football.football_conference.league"], ["american_football.football_team.conference"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what league are the indianapolis colts in [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_team.division </PATH>", "<PATH> sports.sports_team.league </PATH>", "<PATH> american_football.football_team.current_division </PATH>"], "scores": [-0.01740977354347706, -0.037906162440776825, -0.04932842776179314], "norm_scores": [0.33917877078056335, 0.33229756355285645, 0.3285236060619354]}}
{"id": "WebQTest-419", "question": "what country did magellan represent", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "people.person.nationality"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did magellan represent [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> people.person.nationality </PATH>"], "scores": [-0.0032963508274406195, -0.046179741621017456, -0.07806207239627838], "norm_scores": [0.34650227427482605, 0.3319571614265442, 0.32154056429862976]}}
{"id": "WebQTest-420", "question": "where is perpignan located", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.capital"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is perpignan located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0079569760710001, -0.02509603649377823, -0.059305474162101746], "norm_scores": [0.34095311164855957, 0.3351592719554901, 0.32388758659362793]}}
{"id": "WebQTest-421", "question": "who did kim richards marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.spouse_s", "people.marriage.spouse"]], "ground_paths": [["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kim richards marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>"], "scores": [-0.017387015745043755, -0.025920594111084938, -0.047501642256975174], "norm_scores": [0.3376283049583435, 0.334759384393692, 0.3276122808456421]}}
{"id": "WebQTest-422", "question": "what happened in bosnia in the 90s", "prediction": [["time.event.locations"], ["location.location.events"], ["time.event.included_in_event"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened in bosnia in the 90s [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> time.event.included_in_event </PATH>"], "scores": [-0.00652255630120635, -0.03489523380994797, -0.0556492805480957], "norm_scores": [0.34198713302612305, 0.3324204087257385, 0.32559242844581604]}}
{"id": "WebQTest-423", "question": "when 's the last time the steelers won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when 's the last time the steelers won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.010438496246933937, -0.01346096582710743, -0.05221468582749367], "norm_scores": [0.3382873833179474, 0.3372664451599121, 0.3244461715221405]}}
{"id": "WebQTest-425", "question": "where english is spoken", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where english is spoken [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014614954590797424, -0.02486923709511757, -0.027866994962096214], "norm_scores": [0.33594998717308044, 0.3325226604938507, 0.33152735233306885]}}
{"id": "WebQTest-426", "question": "where is giza pyramids located", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is giza pyramids located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0023912431206554174, -0.047312479466199875, -0.07998894900083542], "norm_scores": [0.3470523953437805, 0.33180734515190125, 0.3211402893066406]}}
{"id": "WebQTest-427", "question": "what is the zip code for midland tx", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for midland tx [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-7.822932093404233e-05, -0.10891324281692505, -0.14994336664676666], "norm_scores": [0.3626207113265991, 0.32522666454315186, 0.312152624130249]}}
{"id": "WebQTest-428", "question": "what states make up the midwest us", "prediction": [["location.location.contains"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states make up the midwest us [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.011606907472014427, -0.015967754647135735, -0.13029102981090546], "norm_scores": [0.34677228331565857, 0.34526336193084717, 0.30796438455581665]}}
{"id": "WebQTest-430", "question": "what instruments does justin bieber use", "prediction": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_member.instruments_played"]], "ground_paths": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.membership", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instruments does justin bieber use [/INST]", "raw_output": {"paths": ["<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>"], "scores": [-0.004407551605254412, -0.03937424719333649, -0.05107220634818077], "norm_scores": [0.3424604535102844, 0.33069270849227905, 0.32684680819511414]}}
{"id": "WebQTest-431", "question": "what language does australians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does australians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.007622682489454746, -0.022587135434150696, -0.06278569251298904], "norm_scores": [0.34112489223480225, 0.3360581398010254, 0.32281702756881714]}}
{"id": "WebQTest-432", "question": "who did annie oakley married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did annie oakley married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.025059031322598457, -0.03818391636013985, -0.03907497972249985], "norm_scores": [0.3363557457923889, 0.3319699764251709, 0.3316742777824402]}}
{"id": "WebQTest-433", "question": "who has played lex luthor", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has played lex luthor [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.01919238083064556, -0.025560202077031136, -0.02911941520869732], "norm_scores": [0.3351459801197052, 0.33301860094070435, 0.33183541893959045]}}
{"id": "WebQTest-434", "question": "where does the zambezi river start", "prediction": [["geography.river.origin"], ["location.location.containedby"], []], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the zambezi river start [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> </PATH>"], "scores": [-0.00037480928585864604, -0.10213623940944672, -0.11839409917593002], "norm_scores": [0.35817593336105347, 0.32352063059806824, 0.3183034062385559]}}
{"id": "WebQTest-436", "question": "what is the nigeria time", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the nigeria time [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00017763482173904777, -0.0835585668683052, -0.11932291090488434], "norm_scores": [0.3561660051345825, 0.32767295837402344, 0.31616103649139404]}}
{"id": "WebQTest-437", "question": "where did they find jenni rivera 's body", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did they find jenni rivera 's body [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0004890894633717835, -0.08351289480924606, -0.08518797904253006], "norm_scores": [0.3522220551967621, 0.32416024804115295, 0.32361772656440735]}}
{"id": "WebQTest-438", "question": "what university did romney graduated from", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.religion", "religion.religion.organizations"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did romney graduated from [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.014601560309529305, -0.022181611508131027, -0.02291097678244114], "norm_scores": [0.3351011574268341, 0.3325706720352173, 0.332328200340271]}}
{"id": "WebQTest-439", "question": "where is the time zone line in south dakota", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the time zone line in south dakota [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0018089934019371867, -0.047908246517181396, -0.08096715807914734], "norm_scores": [0.3473620116710663, 0.33171236515045166, 0.32092562317848206]}}
{"id": "WebQTest-440", "question": "where is jason mraz from", "prediction": [["music.artist.origin"], ["location.location.people_born_here"], ["people.person.place_of_birth"]], "ground_paths": [["music.artist.origin"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is jason mraz from [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0048196567222476006, -0.03939496725797653, -0.0507771335542202], "norm_scores": [0.3423370122909546, 0.33070287108421326, 0.32696011662483215]}}
{"id": "WebQTest-441", "question": "what killed john bonham", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["music.group_member.instruments_played", "people.deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what killed john bonham [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> music.group_member.instruments_played <SEP> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0016555238980799913, -0.04577470198273659, -0.0927235409617424], "norm_scores": [0.34845688939094543, 0.33341744542121887, 0.3181256651878357]}}
{"id": "WebQTest-442", "question": "what industry does walmart operate in", "prediction": [["business.business_operation.industry"], ["business.industry.companies"], ["business.business_operation.industry", "business.business_operation.industry"]], "ground_paths": [["organization.organization.sectors"], ["business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what industry does walmart operate in [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>"], "scores": [-0.00561385415494442, -0.04754846915602684, -0.051620736718177795], "norm_scores": [0.34317466616630554, 0.32908132672309875, 0.3277439475059509]}}
{"id": "WebQTest-443", "question": "what county is texarkana arkansas in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is texarkana arkansas in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.008054391480982304, -0.021757403388619423, -0.0575394481420517], "norm_scores": [0.340354859828949, 0.33572277426719666, 0.323922336101532]}}
{"id": "WebQTest-446", "question": "what music period did beethoven live in", "prediction": [["music.artist.genre"], ["music.artist.period"], ["music.artist.periods"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what music period did beethoven live in [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.artist.period </PATH>", "<PATH> music.artist.periods </PATH>"], "scores": [-0.001651555416174233, -0.06999330967664719, -0.0887293741106987], "norm_scores": [0.35080987215042114, 0.32763582468032837, 0.3215543329715729]}}
{"id": "WebQTest-447", "question": "what did laura ingalls wilder wrote", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"], ["book.literary_series.author_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did laura ingalls wilder wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.010552079416811466, -0.025661611929535866, -0.036762263625860214], "norm_scores": [0.337936669588089, 0.3328689634799957, 0.3291943669319153]}}
{"id": "WebQTest-448", "question": "what type of government system does italy have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government system does italy have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009620330296456814, -0.014514119364321232, -0.14458146691322327], "norm_scores": [0.34856951236724854, 0.3468678593635559, 0.30456259846687317]}}
{"id": "WebQTest-449", "question": "what did john irving wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"]], "ground_paths": [["film.film_story_contributor.film_story_credits"], ["book.written_work.author"], ["book.author.book_editions_published"], ["film.film.story_by"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did john irving wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.015028578229248524, -0.025137752294540405, -0.030393056571483612], "norm_scores": [0.33616894483566284, 0.3327876627445221, 0.3310433626174927]}}
{"id": "WebQTest-450", "question": "what is the national flower of hawaii", "prediction": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the national flower of hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.010425419546663761, -0.019741814583539963, -0.022735532373189926], "norm_scores": [0.335740327835083, 0.33262696862220764, 0.33163267374038696]}}
{"id": "WebQTest-451", "question": "what did alice paul accomplish", "prediction": [["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"], ["people.person.profession"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did alice paul accomplish [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.005925919394940138, -0.026209425181150436, -0.07625437527894974], "norm_scores": [0.3434055745601654, 0.3365103006362915, 0.3200840950012207]}}
{"id": "WebQTest-453", "question": "what languages do they speak in costa rica", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages do they speak in costa rica [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.004444383550435305, -0.028008807450532913, -0.08871279656887054], "norm_scores": [0.3453163206577301, 0.3372742831707001, 0.3174093961715698]}}
{"id": "WebQTest-454", "question": "where did hank marvin come from", "prediction": [["people.person.place_of_birth"], ["music.artist.origin"], ["music.recording.artist"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did hank marvin come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.011029141955077648, -0.025587445124983788, -0.05785268917679787], "norm_scores": [0.3401588499546051, 0.3352425992488861, 0.3245985805988312]}}
{"id": "WebQTest-455", "question": "when did chipper jones get drafted", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did chipper jones get drafted [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.022832822054624557, -0.02310386672616005, -0.02391241118311882], "norm_scores": [0.33348339796066284, 0.33339303731918335, 0.3331235647201538]}}
{"id": "WebQTest-456", "question": "what part did winona ryder play in star trek", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part did winona ryder play in star trek [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.02022845484316349, -0.029912235215306282, -0.02999756671488285], "norm_scores": [0.33549827337265015, 0.33226507902145386, 0.33223670721054077]}}
{"id": "WebQTest-459", "question": "where did jovan belcher kill himself", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["location.location.people_born_here", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jovan belcher kill himself [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> location.location.people_born_here <SEP> location.location.containedby </PATH>"], "scores": [-0.0018544950289651752, -0.07020928710699081, -0.08054738491773605], "norm_scores": [0.34986409544944763, 0.32674822211265564, 0.32338768243789673]}}
{"id": "WebQTest-461", "question": "where did clay matthews go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did clay matthews go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.013015737757086754, -0.014129572547972202, -0.03394782170653343], "norm_scores": [0.3357764184474945, 0.33540263772010803, 0.32882094383239746]}}
{"id": "WebQTest-462", "question": "what time is it in texas houston right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time is it in texas houston right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-1.566494938742835e-05, -0.13148798048496246, -0.15854968130588531], "norm_scores": [0.3662738800048828, 0.3211502432823181, 0.31257590651512146]}}
{"id": "WebQTest-463", "question": "who is princess leia in star wars", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is princess leia in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.01129718404263258, -0.029804710298776627, -0.03566683828830719], "norm_scores": [0.3381134271621704, 0.3319133222103119, 0.3299732804298401]}}
{"id": "WebQTest-464", "question": "what is the political structure of china", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["organization.organization.founders"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political structure of china [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.007296734489500523, -0.01885034330189228, -0.16513556241989136], "norm_scores": [0.3518029451370239, 0.34776175022125244, 0.30043527483940125]}}
{"id": "WebQTest-465", "question": "what is new york city airport", "prediction": [["aviation.airport.serves"], ["location.location.containedby"], ["location.location.nearby_airports"]], "ground_paths": [["location.location.nearby_airports"], ["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is new york city airport [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.nearby_airports </PATH>"], "scores": [-0.007211236283183098, -0.031249435618519783, -0.046219296753406525], "norm_scores": [0.3403685986995697, 0.3322843015193939, 0.3273470997810364]}}
{"id": "WebQTest-466", "question": "what language brazil speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language brazil speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.0001814692805055529, -0.10125590115785599, -0.11268513649702072], "norm_scores": [0.3574671447277069, 0.32310232520103455, 0.31943053007125854]}}
{"id": "WebQTest-467", "question": "who played obi wan in episode 2", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played obi wan in episode 2 [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.02985195629298687, -0.03364019840955734, -0.034094709903001785], "norm_scores": [0.33422625064849854, 0.3329625427722931, 0.33281123638153076]}}
{"id": "WebQTest-468", "question": "where did venus williams come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did venus williams come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.004332507960498333, -0.03741389140486717, -0.050265952944755554], "norm_scores": [0.34216514229774475, 0.33103102445602417, 0.3268038034439087]}}
{"id": "WebQTest-469", "question": "what currency is used in panama", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency is used in panama [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.009318233467638493, -0.018396254628896713, -0.1546342819929123], "norm_scores": [0.3501753807067871, 0.34701088070869446, 0.3028137981891632]}}
{"id": "WebQTest-470", "question": "where does the zambezi river originate", "prediction": [["geography.river.origin"], ["location.location.containedby"], ["location.location.partial_containedby"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the zambezi river originate [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partial_containedby </PATH>"], "scores": [-0.0005695696454495192, -0.0930144265294075, -0.09313258528709412], "norm_scores": [0.35419654846191406, 0.3229207992553711, 0.32288265228271484]}}
{"id": "WebQTest-472", "question": "what team did ronaldo play for in 2003", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did ronaldo play for in 2003 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.02027425915002823, -0.023252449929714203, -0.03831402212381363], "norm_scores": [0.3356664180755615, 0.33466824889183044, 0.3296653926372528]}}
{"id": "WebQTest-473", "question": "where does robin williams live 2011", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.children", "people.person.place_of_birth"]], "ground_paths": [["film.film.produced_by", "film.film_location.featured_in_films"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.second_level_division_of"], ["people.person.parents", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does robin williams live 2011 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.014450118876993656, -0.0230267196893692, -0.03001229278743267], "norm_scores": [0.3360194265842438, 0.33314985036849976, 0.33083072304725647]}}
{"id": "WebQTest-474", "question": "who played jacob black in twilight", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_winner.awards_won"]], "ground_paths": [["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film.starring", "film.actor.film"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"], ["award.award_honor.honored_for", "award.award_winner.awards_won"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played jacob black in twilight [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_honor.honored_for <SEP> award.award_winner.awards_won </PATH>"], "scores": [-0.02921925112605095, -0.030447496101260185, -0.03230570629239082], "norm_scores": [0.3338128328323364, 0.3334030508995056, 0.33278411626815796]}}
{"id": "WebQTest-477", "question": "which airport to fly into in buenos aires", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"], ["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which airport to fly into in buenos aires [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.007108223158866167, -0.026256369426846504, -0.04769613966345787], "norm_scores": [0.3399904668331146, 0.33354219794273376, 0.32646727561950684]}}
{"id": "WebQTest-479", "question": "what team did adrian peterson play for in college", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did adrian peterson play for in college [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>"], "scores": [-0.009904038161039352, -0.01737731322646141, -0.037935417145490646], "norm_scores": [0.3372780978679657, 0.3347669243812561, 0.3279549777507782]}}
{"id": "WebQTest-480", "question": "where does name pennsylvania come from", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes", "symbols.namesake.named_after"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does name pennsylvania come from [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes <SEP> symbols.namesake.named_after </PATH>"], "scores": [-0.004685812164098024, -0.030424322932958603, -0.09485571086406708], "norm_scores": [0.3462165296077728, 0.33741915225982666, 0.3163643479347229]}}
{"id": "WebQTest-481", "question": "what kind of money do they use in germany", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do they use in germany [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.0050700572319328785, -0.02606532722711563, -0.11074494570493698], "norm_scores": [0.34735000133514404, 0.34013330936431885, 0.3125167489051819]}}
{"id": "WebQTest-482", "question": "who plays blaine in batman", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays blaine in batman [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.014896693639457226, -0.024706672877073288, -0.0334453284740448], "norm_scores": [0.33648958802223206, 0.3332047462463379, 0.33030566573143005]}}
{"id": "WebQTest-484", "question": "what language does australia use", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does australia use [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0077465674839913845, -0.022347476333379745, -0.064931720495224], "norm_scores": [0.3413057327270508, 0.33635857701301575, 0.32233569025993347]}}
{"id": "WebQTest-487", "question": "who did veronica lake mary", "prediction": [["people.person.children"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did veronica lake mary [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>"], "scores": [-0.027758242562413216, -0.030903439968824387, -0.03859797865152359], "norm_scores": [0.33488738536834717, 0.33383575081825256, 0.33127689361572266]}}
{"id": "WebQTest-488", "question": "who did sir francis drake marry", "prediction": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did sir francis drake marry [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.025784920901060104, -0.02971779741346836, -0.0395100899040699], "norm_scores": [0.33529552817344666, 0.33397945761680603, 0.3307249844074249]}}
{"id": "WebQTest-489", "question": "who plays stephanie plum in one for the money", "prediction": [["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["film.film.executive_produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays stephanie plum in one for the money [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.022474708035588264, -0.034754153341054916, -0.03679099306464195], "norm_scores": [0.33629482984542847, 0.33219054341316223, 0.3315146267414093]}}
{"id": "WebQTest-490", "question": "what did the ancient romans speak", "prediction": [["fictional_universe.fictional_setting.languages"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the ancient romans speak [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.languages </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012584170326590538, -0.03056669794023037, -0.041924212127923965], "norm_scores": [0.33860835433006287, 0.3325737416744232, 0.3288179039955139]}}
{"id": "WebQTest-491", "question": "what does ringo sing", "prediction": [["music.recording.artist"], ["music.artist.track"], ["music.album.artist"]], "ground_paths": [["music.featured_artist.recordings"], ["music.artist.label", "music.record_label.releases"], ["music.artist.album"], ["theater.theatrical_composer.plays_composed", "film.film.sequel"], ["film.performance.actor", "film.performance.film"], ["music.album.artist"], ["music.lyricist.lyrics_written"], ["music.composition.composer"], ["music.recording.featured_artists"], ["organization.organization.founders", "film.film.production_companies"], ["people.person.nationality", "film.film.country"], ["music.composer.compositions"], ["music.album.artist", "music.album.compositions"], ["music.recording.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does ringo sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.track </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.0204360019415617, -0.023631105199456215, -0.05859081819653511], "norm_scores": [0.3379093110561371, 0.33683139085769653, 0.3252592980861664]}}
{"id": "WebQTest-492", "question": "where was st. lucy born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was st. lucy born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.006336802151054144, -0.02480245567858219, -0.08469041436910629], "norm_scores": [0.3440752327442169, 0.3377799689769745, 0.318144828081131]}}
{"id": "WebQTest-493", "question": "what type of government does australia have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does australia have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00695564690977335, -0.01931184157729149, -0.1337175965309143], "norm_scores": [0.34859442710876465, 0.3443136513233185, 0.30709195137023926]}}
{"id": "WebQTest-497", "question": "who was the father of king george vi", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the father of king george vi [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.011536912061274052, -0.01911770924925804, -0.053453478962183], "norm_scores": [0.33882251381874084, 0.33626365661621094, 0.32491379976272583]}}
{"id": "WebQTest-500", "question": "what is the zip code for trenton", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for trenton [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-9.46640720940195e-05, -0.10475947707891464, -0.1700717955827713], "norm_scores": [0.36439022421836853, 0.3281794488430023, 0.3074302673339844]}}
{"id": "WebQTest-501", "question": "what has ashley greene been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has ashley greene been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.01779543049633503, -0.031330887228250504, -0.03564447537064552], "norm_scores": [0.3368290364742279, 0.3323006331920624, 0.33087030053138733]}}
{"id": "WebQTest-502", "question": "where did al qaeda attack", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["organization.organization_founder.organizations_founded", "base.culturalevent.event.entity_involved"]], "ground_paths": [["organization.organization_founder.organizations_founded", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["film.film.subjects", "film.film.subjects"], ["military.military_combatant.military_conflicts", "military.military_combatant_group.conflict"], ["base.disaster2.attack_process.attacker", "base.disaster2.attack.attack"], ["base.terrorism.terrorist_organization.involved_in_attacks"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did al qaeda attack [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.019007442519068718, -0.03181314468383789, -0.034633271396160126], "norm_scores": [0.3364996016025543, 0.3322179615497589, 0.331282377243042]}}
{"id": "WebQTest-503", "question": "what language is spoken in switzerland", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.017137560993433, -0.021531138569116592, -0.02323031984269619], "norm_scores": [0.33449938893318176, 0.3330329656600952, 0.33246755599975586]}}
{"id": "WebQTest-504", "question": "who was philip in acts chapter 8", "prediction": [["people.person.gender", "people.person.gender"], ["people.person.profession"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["fictional_universe.person_in_fiction.representations_in_fiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was philip in acts chapter 8 [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.013824725523591042, -0.017651380971074104, -0.0649712011218071], "norm_scores": [0.33940643072128296, 0.3381101191043854, 0.32248347997665405]}}
{"id": "WebQTest-506", "question": "what the time zone in england", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.country.form_of_government"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the time zone in england [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.001213258015923202, -0.04689422994852066, -0.15219591557979584], "norm_scores": [0.35521337389945984, 0.33935192227363586, 0.3054347038269043]}}
{"id": "WebQTest-508", "question": "what is the best currency to take to turkey", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the best currency to take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.016938649117946625, -0.01819694973528385, -0.0352523997426033], "norm_scores": [0.3355034291744232, 0.335081547498703, 0.3294150233268738]}}
{"id": "WebQTest-509", "question": "what system of government is practiced in nigeria", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.continent"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what system of government is practiced in nigeria [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.005084630101919174, -0.022293824702501297, -0.14239190518856049], "norm_scores": [0.35030680894851685, 0.3443298935890198, 0.305363267660141]}}
{"id": "WebQTest-512", "question": "who were mary shelley", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were mary shelley [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0018257710617035627, -0.07043267041444778, -0.07390918582677841], "norm_scores": [0.3491441011428833, 0.3259936273097992, 0.3248622715473175]}}
{"id": "WebQTest-513", "question": "what language do most australians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most australians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.006134350784122944, -0.025282900780439377, -0.06492583453655243], "norm_scores": [0.342004656791687, 0.3355180621147156, 0.3224773406982422]}}
{"id": "WebQTest-514", "question": "where is denmark situated", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is denmark situated [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.018971551209688187, -0.02255846932530403, -0.03491177037358284], "norm_scores": [0.3355022966861725, 0.33430102467536926, 0.33019670844078064]}}
{"id": "WebQTest-515", "question": "where was teddy roosevelt 's house", "prediction": [["symbols.name_source.namesakes", "location.location.containedby"], ["symbols.namesake.named_after", "location.location.containedby"], ["people.person.parents", "people.person.place_of_birth"]], "ground_paths": [["people.person.parents", "people.deceased_person.place_of_death"], ["people.person.children", "people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.children", "people.person.place_of_birth"], ["base.usnris.nris_listing.significant_person", "travel.travel_destination.tourist_attractions"], ["symbols.namesake.named_after", "transportation.bridge.locale"], ["symbols.name_source.namesakes", "location.location.containedby"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["book.written_work.subjects", "location.location.containedby"], ["symbols.namesake.named_after", "location.location.containedby"], ["people.person.place_of_birth", "base.aareas.schema.administrative_area.administrative_children"], ["book.written_work.subjects", "location.administrative_division.country"], ["book.written_work.author", "people.deceased_person.place_of_death"], ["people.person.parents", "people.person.place_of_birth"], ["book.written_work.author", "base.biblioness.bibs_location.state"], ["book.book_edition.author_editor", "book.book_edition.place_of_publication"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was teddy roosevelt 's house [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes <SEP> location.location.containedby </PATH>", "<PATH> symbols.namesake.named_after <SEP> location.location.containedby </PATH>", "<PATH> people.person.parents <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.023976927623152733, -0.027656199410557747, -0.03766138479113579], "norm_scores": [0.335262656211853, 0.3340313732624054, 0.33070600032806396]}}
{"id": "WebQTest-516", "question": "what is the government system of malaysia", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "government.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the government system of malaysia [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> government.form_of_government.countries </PATH>"], "scores": [-0.010567372664809227, -0.013318863697350025, -0.11293628811836243], "norm_scores": [0.34483370184898376, 0.34388622641563416, 0.31128016114234924]}}
{"id": "WebQTest-517", "question": "what do people speak in canada", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in canada [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.016051264479756355, -0.01772981323301792, -0.03848884999752045], "norm_scores": [0.336006224155426, 0.3354427218437195, 0.3285510241985321]}}
{"id": "WebQTest-518", "question": "what type of money do they use in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of money do they use in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.006146134342998266, -0.022285182029008865, -0.11899901926517487], "norm_scores": [0.3475513458251953, 0.3419872224330902, 0.3104614019393921]}}
{"id": "WebQTest-519", "question": "what timezone is nashville tn", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is nashville tn [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-8.858848559611943e-06, -0.18927772343158722, -0.18927831947803497], "norm_scores": [0.3766297399997711, 0.3116852045059204, 0.3116850256919861]}}
{"id": "WebQTest-521", "question": "who was anakin skywalker", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was anakin skywalker [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.003954038955271244, -0.040188029408454895, -0.04890356957912445], "norm_scores": [0.3424117863178253, 0.33022695779800415, 0.3273613154888153]}}
{"id": "WebQTest-522", "question": "where did adolf hitler die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_death.people"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did adolf hitler die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people </PATH>"], "scores": [-0.0005319537594914436, -0.08310534060001373, -0.10213732719421387], "norm_scores": [0.3540913760662079, 0.32602745294570923, 0.3198812007904053]}}
{"id": "WebQTest-523", "question": "what is the largest nation in europe", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the largest nation in europe [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.012477055191993713, -0.023997973650693893, -0.040733106434345245], "norm_scores": [0.33775973320007324, 0.33389073610305786, 0.3283495306968689]}}
{"id": "WebQTest-524", "question": "where was the first gulf war fought", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the first gulf war fought [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.004490511026233435, -0.03208574652671814, -0.11623670905828476], "norm_scores": [0.34879016876220703, 0.339296817779541, 0.31191301345825195]}}
{"id": "WebQTest-525", "question": "who are the senators of virginia 2013", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.administrative_division.country", "people.person.nationality"], ["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of virginia 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.016987036913633347, -0.025495098903775215, -0.02639378421008587], "norm_scores": [0.33532682061195374, 0.3324859142303467, 0.3321872353553772]}}
{"id": "WebQTest-526", "question": "what band was george clinton in", "prediction": [["music.artist.genre", "music.artist.genre"], ["music.artist.label", "music.artist.label"], ["music.recording.artist", "music.recording.artist"]], "ground_paths": [["music.composer.compositions", "music.artist.track"], ["music.recording.artist", "music.recording.artist"], ["music.composition.composer", "music.artist.track"], ["music.featured_artist.recordings", "music.recording.artist"], ["music.recording.producer", "music.album.artist"], ["music.artist.genre", "music.artist.genre"], ["base.saturdaynightlive.snl_episode.musical_guest", "base.saturdaynightlive.snl_musical_guest.episodes_as_musical_guest"], ["music.recording.featured_artists", "music.recording.featured_artists"], ["music.artist.genre", "music.genre.artists"], ["music.artist.album", "music.artist.album"], ["music.artist.album", "music.album.artist"], ["music.recording.producer", "music.recording.artist"], ["music.composer.compositions", "music.recording.artist"], ["music.recording.featured_artists", "music.artist.album"], ["music.composition.composer", "music.recording.artist"], ["music.featured_artist.recordings", "music.recording.featured_artists"], ["music.group_membership.member", "music.group_membership.group"], ["broadcast.artist.content", "broadcast.artist.content"], ["music.record_label.artist", "music.record_label.artist"], ["common.topic.notable_types", "common.topic.notable_types"], ["music.group_member.membership", "music.musical_group.member"], ["people.person.nationality", "music.artist.origin"], ["music.producer.tracks_produced", "music.recording.artist"], ["music.recording.artist", "music.album.artist"], ["music.group_membership.member", "music.musical_group.member"], ["music.album.artist", "music.artist.album"], ["music.album.artist", "music.album.artist"], ["music.producer.releases_produced", "music.album.artist"], ["music.artist.label", "music.artist.label"], ["music.artist.origin", "music.artist.origin"], ["music.recording.featured_artists", "music.featured_artist.recordings"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what band was george clinton in [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> music.artist.label <SEP> music.artist.label </PATH>", "<PATH> music.recording.artist <SEP> music.recording.artist </PATH>"], "scores": [-0.015359166078269482, -0.03684552013874054, -0.0502706915140152], "norm_scores": [0.3396238684654236, 0.3324044644832611, 0.3279716670513153]}}
{"id": "WebQTest-527", "question": "what country borders slovakia", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains", "location.location.partially_contains"], ["government.form_of_government.countries", "location.country.form_of_government"], ["location.location.partially_contains", "geography.river.basin_countries"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.partially_containedby", "location.location.partially_contains"], ["finance.currency.countries_formerly_used", "location.country.currency_formerly_used"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.partially_containedby", "location.location.containedby"], ["location.location.events", "location.location.events"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["location.location.containedby", "location.location.containedby"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["military.military_combatant_group.combatants", "military.military_combatant.military_conflicts"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.location.containedby", "location.location.contains"], ["common.topic.notable_types", "common.topic.notable_types"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.locations.countries.continent", "base.locations.continents.countries_within"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["location.location.partially_containedby", "geography.river.origin"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["organization.organization.founders", "organization.organization.founders"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.locations.countries.continent", "base.locations.countries.continent"], ["location.country.currency_used", "location.country.currency_used"], ["location.location.time_zones", "location.location.time_zones"], ["base.datedlocationtest.dated_location_break_up.new_locations", "base.datedlocationtest.dated_location_break_up.new_locations"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country borders slovakia [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03667080029845238, -0.04105810075998306, -0.042053334414958954], "norm_scores": [0.33441972732543945, 0.3329557478427887, 0.33262452483177185]}}
{"id": "WebQTest-528", "question": "what was james garfield most known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was james garfield most known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0011190795339643955, -0.05936052277684212, -0.08577271550893784], "norm_scores": [0.3493751883506775, 0.329608291387558, 0.3210165500640869]}}
{"id": "WebQTest-529", "question": "where did paula deen go to school", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did paula deen go to school [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.017551064491271973, -0.018906204029917717, -0.027394849807024002], "norm_scores": [0.334576815366745, 0.33412373065948486, 0.33129945397377014]}}
{"id": "WebQTest-530", "question": "where does robert irvine come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does robert irvine come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.003949934151023626, -0.03874993696808815, -0.04901399835944176], "norm_scores": [0.3422624170780182, 0.3305565416812897, 0.32718104124069214]}}
{"id": "WebQTest-531", "question": "what style of music does michael jackson perform", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.artist.album", "music.album.genre"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music does michael jackson perform [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.artist.album <SEP> music.album.genre </PATH>"], "scores": [-0.0009559657773934305, -0.058870989829301834, -0.13133619725704193], "norm_scores": [0.3544224500656128, 0.33447912335395813, 0.3110984265804291]}}
{"id": "WebQTest-532", "question": "what does david beckham play", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "ground_paths": [["sports.pro_sports_played.athlete", "sports.pro_sports_played.sport"], ["book.written_work.subjects", "book.book_subject.works"], ["soccer.football_player.position_s", "sports.sport.positions"], ["symbols.namesake.named_after", "cvg.computer_videogame.cvg_genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does david beckham play [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.030412713065743446, -0.032501108944416046, -0.042329926043748856], "norm_scores": [0.33488863706588745, 0.3341900110244751, 0.33092138171195984]}}
{"id": "WebQTest-533", "question": "what does julian lennon do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does julian lennon do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre </PATH>"], "scores": [-0.0009710972080938518, -0.06370244175195694, -0.11700543016195297], "norm_scores": [0.3534018099308014, 0.33191347122192383, 0.31468474864959717]}}
{"id": "WebQTest-534", "question": "what is the milwaukee brewers mascot", "prediction": [["sports.mascot.team"], ["sports.sports_team.team_mascot"], ["sports.mascot.team", "sports.mascot.team"]], "ground_paths": [["sports.sports_team.team_mascot"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the milwaukee brewers mascot [/INST]", "raw_output": {"paths": ["<PATH> sports.mascot.team </PATH>", "<PATH> sports.sports_team.team_mascot </PATH>", "<PATH> sports.mascot.team <SEP> sports.mascot.team </PATH>"], "scores": [-0.0011279928730800748, -0.04614170268177986, -0.140949547290802], "norm_scores": [0.3539199233055115, 0.33834192156791687, 0.30773812532424927]}}
{"id": "WebQTest-535", "question": "what is the second biggest state in the united states", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby"]], "ground_paths": [["location.administrative_division.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the second biggest state in the united states [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.0016288759652525187, -0.05827796086668968, -0.07624125480651855], "norm_scores": [0.34806475043296814, 0.3288952708244324, 0.3230400085449219]}}
{"id": "WebQTest-537", "question": "what has charlie hunnam been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has charlie hunnam been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.022621359676122665, -0.02702144905924797, -0.03161080181598663], "norm_scores": [0.3348221182823181, 0.33335211873054504, 0.33182573318481445]}}
{"id": "WebQTest-538", "question": "what are all the movies taylor swift has been in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["music.album.genre", "media_common.netflix_title.netflix_genres"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the movies taylor swift has been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.027191607281565666, -0.037468474358320236, -0.03936474025249481], "norm_scores": [0.3358323276042938, 0.3323987126350403, 0.3317689895629883]}}
{"id": "WebQTest-540", "question": "what college did brett favre", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did brett favre [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.01589723490178585, -0.019739873707294464, -0.028104767203330994], "norm_scores": [0.3351171314716339, 0.3338318467140198, 0.3310510516166687]}}
{"id": "WebQTest-542", "question": "where did alex chilton die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did alex chilton die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0002907175221480429, -0.09616413712501526, -0.10073134303092957], "norm_scores": [0.3554900884628296, 0.3229908347129822, 0.32151904702186584]}}
{"id": "WebQTest-543", "question": "what college did martin luther king jr go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did martin luther king jr go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.014658048748970032, -0.017564579844474792, -0.02797497808933258], "norm_scores": [0.3351353406906128, 0.3341626822948456, 0.3307019770145416]}}
{"id": "WebQTest-545", "question": "what kind of cancer did farrah fawcett die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of cancer did farrah fawcett die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.007839047349989414, -0.018970677629113197, -0.04292421042919159], "norm_scores": [0.33847206830978394, 0.3347252607345581, 0.32680264115333557]}}
{"id": "WebQTest-546", "question": "who is michael j fox 's wife", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["common.image.appears_in_topic_gallery", "common.topic.image"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox 's wife [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.021450158208608627, -0.03215446695685387, -0.04142680764198303], "norm_scores": [0.3367486000061035, 0.33316317200660706, 0.33008822798728943]}}
{"id": "WebQTest-547", "question": "what money do i use in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money do i use in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.0057317656464874744, -0.02482389286160469, -0.12756280601024628], "norm_scores": [0.34887126088142395, 0.3422737419605255, 0.30885499715805054]}}
{"id": "WebQTest-548", "question": "what guitar did joe walsh play", "prediction": [["music.guitarist.guitars_played"], ["music.group_member.instruments_played"], ["music.guitar.guitarists"]], "ground_paths": [["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar did joe walsh play [/INST]", "raw_output": {"paths": ["<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.019176175817847252, -0.02043459750711918, -0.03249244764447212], "norm_scores": [0.33495065569877625, 0.33452942967414856, 0.3305199444293976]}}
{"id": "WebQTest-549", "question": "what are the official languages of the eu", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "language.human_language.countries_spoken_in"], ["organization.organization.geographic_scope", "location.country.languages_spoken"]], "ground_paths": [["organization.organization_founder.organizations_founded", "location.country.official_language"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_language"], ["organization.organization.geographic_scope", "language.human_language.region"], ["organization.organization.founders", "location.country.languages_spoken"], ["organization.organization.founders", "location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official languages of the eu [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.023402037099003792, -0.030522605404257774, -0.038321610540151596], "norm_scores": [0.33578503131866455, 0.333402544260025, 0.3308124542236328]}}
{"id": "WebQTest-550", "question": "what is ryan seacrest radio show", "prediction": [["broadcast.content.producer"], ["internet.website.owner"], ["broadcast.producer.content", "broadcast.content.producer"]], "ground_paths": [["broadcast.artist.content"], ["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is ryan seacrest radio show [/INST]", "raw_output": {"paths": ["<PATH> broadcast.content.producer </PATH>", "<PATH> internet.website.owner </PATH>", "<PATH> broadcast.producer.content <SEP> broadcast.content.producer </PATH>"], "scores": [-0.03533179312944412, -0.0549495555460453, -0.05789143592119217], "norm_scores": [0.33803582191467285, 0.33146893978118896, 0.3304952383041382]}}
{"id": "WebQTest-552", "question": "what language do most italians speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most italians speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016771171241998672, -0.021322984248399734, -0.02652200497686863], "norm_scores": [0.3349236249923706, 0.3334026038646698, 0.3316737115383148]}}
{"id": "WebQTest-554", "question": "what money do they use in spain", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money do they use in spain [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.004687115550041199, -0.028905387967824936, -0.1382131725549698], "norm_scores": [0.3507446050643921, 0.3423522114753723, 0.3069031536579132]}}
{"id": "WebQTest-555", "question": "what did drew barrymore go to rehab for", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["people.person.profession", "people.profession.specializations"], ["people.person.profession", "people.profession.specializations_of"]], "ground_paths": [["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did drew barrymore go to rehab for [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations_of </PATH>"], "scores": [-0.022028058767318726, -0.02627035416662693, -0.040238671004772186], "norm_scores": [0.3358273208141327, 0.33440566062927246, 0.32976704835891724]}}
{"id": "WebQTest-556", "question": "when is saint george day celebrated", "prediction": [["time.recurring_event.current_frequency"], ["time.day_of_year.holidays"], ["time.holiday.featured_in_religions"]], "ground_paths": [["time.holiday.day_of_year"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is saint george day celebrated [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.current_frequency </PATH>", "<PATH> time.day_of_year.holidays </PATH>", "<PATH> time.holiday.featured_in_religions </PATH>"], "scores": [-0.015563873574137688, -0.02544432133436203, -0.034126635640859604], "norm_scores": [0.33649906516075134, 0.3331906795501709, 0.33031031489372253]}}
{"id": "WebQTest-557", "question": "what government did the roman empire have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["fictional_universe.fictional_setting.languages", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government did the roman empire have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> location.country.form_of_government </PATH>"], "scores": [-0.005484818946570158, -0.023615222424268723, -0.08269479125738144], "norm_scores": [0.3439110815525055, 0.3377320170402527, 0.3183569312095642]}}
{"id": "WebQTest-559", "question": "what instrument did robin gibb play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.group_membership.group", "music.group_membership.role"]], "ground_paths": [["music.track_contribution.contributor", "music.track_contribution.role"], ["music.group_membership.member", "music.performance_role.regular_performances"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.instruments_played"], ["music.group_membership.member", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instrument did robin gibb play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.group_membership.group <SEP> music.group_membership.role </PATH>"], "scores": [-0.000822718080598861, -0.07680472731590271, -0.07915134727954865], "norm_scores": [0.3506934344768524, 0.3250342309474945, 0.32427236437797546]}}
{"id": "WebQTest-560", "question": "who voiced darth vader", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who voiced darth vader [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.026521986350417137, -0.028155459091067314, -0.029627103358507156], "norm_scores": [0.33385998010635376, 0.33331507444381714, 0.3328249156475067]}}
{"id": "WebQTest-561", "question": "when was abe lincoln president", "prediction": [["government.us_president.vice_president"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"]], "ground_paths": [["base.inaugurations.inauguration_speaker.inauguration"], ["base.inaugurations.inauguration.president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was abe lincoln president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.02545815147459507, -0.02699444070458412, -0.04145281761884689], "norm_scores": [0.3352782428264618, 0.3347635567188263, 0.3299582302570343]}}
{"id": "WebQTest-562", "question": "where are the texas rangers playing", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team_location.teams"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the texas rangers playing [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.0013614273630082607, -0.050312113016843796, -0.09110923111438751], "norm_scores": [0.3488709032535553, 0.3322046995162964, 0.3189244270324707]}}
{"id": "WebQTest-564", "question": "what is arkansas state capitol", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.state"], ["location.us_state.capital"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is arkansas state capitol [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> location.us_state.capital </PATH>"], "scores": [-0.019831784069538116, -0.024245886132121086, -0.028857706114649773], "norm_scores": [0.334827721118927, 0.3333530128002167, 0.33181920647621155]}}
{"id": "WebQTest-567", "question": "what is the zip code for nogales az", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for nogales az [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0003353056381456554, -0.08246855437755585, -0.11265116930007935], "norm_scores": [0.3552510142326355, 0.32723918557167053, 0.31750982999801636]}}
{"id": "WebQTest-568", "question": "when did michael vick start playing for the eagles", "prediction": [["american_football.football_player.rushing", "american_football.player_rushing_statistics.season"], ["american_football.football_player.games", "american_football.player_game_statistics.season"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did michael vick start playing for the eagles [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.season </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.season </PATH>", "<PATH> american_football.player_game_statistics.player <SEP> american_football.player_game_statistics.season </PATH>"], "scores": [-0.020744970068335533, -0.022390814498066902, -0.027987325564026833], "norm_scores": [0.3343207538127899, 0.3337709605693817, 0.3319082260131836]}}
{"id": "WebQTest-570", "question": "what is william taft famous for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is william taft famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.009835577569901943, -0.039317719638347626, -0.04199562594294548], "norm_scores": [0.34021708369255066, 0.3303331732749939, 0.32944974303245544]}}
{"id": "WebQTest-572", "question": "where did robin cook die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robin cook die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0003113683487754315, -0.11134310811758041, -0.11308348178863525], "norm_scores": [0.3586460053920746, 0.3209560215473175, 0.3203979432582855]}}
{"id": "WebQTest-573", "question": "where did melba beals live", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth", "location.us_state.capital"], ["people.person.nationality", "location.country.first_level_divisions"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did melba beals live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01648242026567459, -0.02789848856627941, -0.03654978796839714], "norm_scores": [0.3368385434150696, 0.3330150246620178, 0.3301464319229126]}}
{"id": "WebQTest-575", "question": "who is louisiana state senator", "prediction": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.place_of_birth"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.partially_containedby", "people.deceased_person.place_of_death"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["location.location.containedby", "symbols.name_source.namesakes"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["base.locations.countries.states_provinces_within", "people.person.nationality"], ["government.election.district", "government.election.winner"], ["government.political_district.representatives", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is louisiana state senator [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.02251056768000126, -0.02617301233112812, -0.028463322669267654], "norm_scores": [0.3344024121761322, 0.33317992091178894, 0.33241766691207886]}}
{"id": "WebQTest-576", "question": "what are the 7 countries that are part of central america", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the 7 countries that are part of central america [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.007879904471337795, -0.027341129258275032, -0.03492746502161026], "norm_scores": [0.33851921558380127, 0.3319949209690094, 0.32948583364486694]}}
{"id": "WebQTest-577", "question": "where did derek fisher go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did derek fisher go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.017574699595570564, -0.02347397245466709, -0.024809278547763824], "norm_scores": [0.33479422330856323, 0.3328250050544739, 0.33238086104393005]}}
{"id": "WebQTest-578", "question": "who was kobe bryant drafted by", "prediction": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was kobe bryant drafted by [/INST]", "raw_output": {"paths": ["<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.team </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.professional_sports_team.draft_picks </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>"], "scores": [-0.02479739673435688, -0.026339884847402573, -0.03365224227309227], "norm_scores": [0.3344881236553192, 0.3339725732803345, 0.3315393626689911]}}
{"id": "WebQTest-579", "question": "where are the new orleans hornets moving to", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.previously_known_as"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the new orleans hornets moving to [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>"], "scores": [-0.01056532934308052, -0.030300138518214226, -0.046255167573690414], "norm_scores": [0.3395126461982727, 0.33287811279296875, 0.32760918140411377]}}
{"id": "WebQTest-580", "question": "what language does algerian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does algerian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01207704097032547, -0.017726853489875793, -0.05004115030169487], "norm_scores": [0.33816757798194885, 0.33626240491867065, 0.3255699872970581]}}
{"id": "WebQTest-581", "question": "where was anne boleyn executed", "prediction": [["people.deceased_person.place_of_death"], ["location.location.people_executed"], ["location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was anne boleyn executed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_executed </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.0012023785384371877, -0.0820869505405426, -0.0849490612745285], "norm_scores": [0.3518693149089813, 0.32452911138534546, 0.3236016035079956]}}
{"id": "WebQTest-582", "question": "what did galileo do to become famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did galileo do to become famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0018442000728100538, -0.05746247619390488, -0.07372450083494186], "norm_scores": [0.3476395606994629, 0.3288322985172272, 0.32352808117866516]}}
{"id": "WebQTest-583", "question": "what school did martin luther king jr received his doctorate degree from", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did martin luther king jr received his doctorate degree from [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth <SEP> location.location.containedby </PATH>"], "scores": [-0.006786205805838108, -0.023797746747732162, -0.02983817644417286], "norm_scores": [0.3377985656261444, 0.3321007192134857, 0.3301007151603699]}}
{"id": "WebQTest-584", "question": "where buddha come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where buddha come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.004784639924764633, -0.04314502701163292, -0.04623885452747345], "norm_scores": [0.3422595262527466, 0.32937896251678467, 0.32836148142814636]}}
{"id": "WebQTest-585", "question": "what did king henry viii rule", "prediction": [["royalty.kingdom.rulers"], ["royalty.monarch.kingdom"], ["people.person.nationality"]], "ground_paths": [["royalty.monarch.kingdom"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did king henry viii rule [/INST]", "raw_output": {"paths": ["<PATH> royalty.kingdom.rulers </PATH>", "<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.015091127716004848, -0.029429800808429718, -0.03083714097738266], "norm_scores": [0.33668434619903564, 0.33189117908477783, 0.33142441511154175]}}
{"id": "WebQTest-586", "question": "what type of cancer did larry hagman have", "prediction": [["people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did larry hagman have [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.014218509197235107, -0.015385824255645275, -0.032264430075883865], "norm_scores": [0.33546358346939087, 0.33507224917411804, 0.3294641077518463]}}
{"id": "WebQTest-588", "question": "what wars did robert e lee served in", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_person.participated_in_conflicts"]], "ground_paths": [["people.person.nationality", "base.culturalevent.event.entity_involved"], ["military.military_person.participated_in_conflicts"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["military.military_commander.military_commands", "military.military_command.military_conflict"], ["base.culturalevent.event.entity_involved"], ["military.military_commander.military_commands", "military.military_conflict.commanders"], ["military.military_conflict.military_personnel_involved"], ["military.military_command.military_commander", "military.military_conflict.commanders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what wars did robert e lee served in [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.included_in_event </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>"], "scores": [-0.006417246535420418, -0.025605428963899612, -0.047361910343170166], "norm_scores": [0.3400346338748932, 0.3335721790790558, 0.3263932168483734]}}
{"id": "WebQTest-589", "question": "what russian language called", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what russian language called [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.014249810948967934, -0.026126019656658173, -0.026850620284676552], "norm_scores": [0.33605852723121643, 0.33209100365638733, 0.33185046911239624]}}
{"id": "WebQTest-590", "question": "who is in charge of libya now", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in charge of libya now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.010023074224591255, -0.02409922517836094, -0.036796826869249344], "norm_scores": [0.3378830552101135, 0.33316028118133545, 0.3289566934108734]}}
{"id": "WebQTest-592", "question": "when did shays rebellion start", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did shays rebellion start [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.014378163032233715, -0.020016014575958252, -0.03523171320557594], "norm_scores": [0.3362768292427063, 0.33438628911972046, 0.32933688163757324]}}
{"id": "WebQTest-593", "question": "what kind of disease does montel williams have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.person.medical_condition"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of disease does montel williams have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.0091974763199687, -0.013245687820017338, -0.0839148685336113], "norm_scores": [0.3420012891292572, 0.3406195640563965, 0.3173791468143463]}}
{"id": "WebQTest-595", "question": "when is the last time the ravens won a superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the ravens won a superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.009779012762010098, -0.02142159827053547, -0.03456583619117737], "norm_scores": [0.33738842606544495, 0.33348315954208374, 0.3291284441947937]}}
{"id": "WebQTest-596", "question": "where did kansas city royals come from", "prediction": [["sports.sports_team_location.teams"], ["sports.sports_team.location"], ["sports.sports_team.previously_known_as"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kansas city royals come from [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team.previously_known_as </PATH>"], "scores": [-0.015281053259968758, -0.021598098799586296, -0.03637874498963356], "norm_scores": [0.3363802433013916, 0.33426201343536377, 0.32935771346092224]}}
{"id": "WebQTest-597", "question": "what style of music does john mayer play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.album.genre"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music does john mayer play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.album.genre </PATH>"], "scores": [-0.0006025805487297475, -0.06549350172281265, -0.15029999613761902], "norm_scores": [0.3573805093765259, 0.33492618799209595, 0.30769333243370056]}}
{"id": "WebQTest-599", "question": "what inventions did thomas edison invent list", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "ground_paths": [["law.inventor.inventions"], ["law.inventor.us_patents"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inventions did thomas edison invent list [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>"], "scores": [-0.009282405488193035, -0.021811921149492264, -0.05517294257879257], "norm_scores": [0.33982452750205994, 0.33559325337409973, 0.32458221912384033]}}
{"id": "WebQTest-600", "question": "where did djokovic come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did djokovic come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.00886186771094799, -0.02115994319319725, -0.05550650134682655], "norm_scores": [0.3398813009262085, 0.3357270061969757, 0.3243917226791382]}}
{"id": "WebQTest-602", "question": "what part of the country is new england", "prediction": [["location.location.contains"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is new england [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.006233295425772667, -0.024865850806236267, -0.13873131573200226], "norm_scores": [0.3499630093574524, 0.3435027003288269, 0.3065342605113983]}}
{"id": "WebQTest-603", "question": "what money to take to turkey", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money to take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.012675977312028408, -0.02539285458624363, -0.036056339740753174], "norm_scores": [0.3373529613018036, 0.33309006690979004, 0.32955703139305115]}}
{"id": "WebQTest-604", "question": "what kind of music did jimi hendrix play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of music did jimi hendrix play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.000821993569843471, -0.05765951797366142, -0.14304091036319733], "norm_scores": [0.3555961549282074, 0.3359485864639282, 0.30845528841018677]}}
{"id": "WebQTest-605", "question": "what degrees did obama get in college", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees did obama get in college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.011187664233148098, -0.01988276280462742, -0.02688094973564148], "norm_scores": [0.33604729175567627, 0.33313798904418945, 0.33081474900245667]}}
{"id": "WebQTest-606", "question": "who were alexander graham bell 's siblings", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.gender", "people.person.gender"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were alexander graham bell 's siblings [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.02058478072285652, -0.022709405049681664, -0.04160037636756897], "norm_scores": [0.3358994722366333, 0.33518657088279724, 0.32891398668289185]}}
{"id": "WebQTest-608", "question": "when did liverpool fc last win the champions league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did liverpool fc last win the champions league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.004596122074872255, -0.028946300968527794, -0.057280175387859344], "norm_scores": [0.34192436933517456, 0.33369898796081543, 0.3243767023086548]}}
{"id": "WebQTest-609", "question": "who played jeannie", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played jeannie [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.015135285444557667, -0.030980629846453667, -0.033263977617025375], "norm_scores": [0.3371186852455139, 0.3318190276622772, 0.3310622572898865]}}
{"id": "WebQTest-611", "question": "where was st peter killed", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was st peter killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0009365552105009556, -0.08029750734567642, -0.09041394293308258], "norm_scores": [0.3523465096950531, 0.32546472549438477, 0.3221887946128845]}}
{"id": "WebQTest-612", "question": "who does jeremy shockey play for in 2012", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does jeremy shockey play for in 2012 [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.013982790522277355, -0.0168024729937315, -0.028064215555787086], "norm_scores": [0.3352103531360626, 0.3342664837837219, 0.33052316308021545]}}
{"id": "WebQTest-613", "question": "what college did deion sanders jr go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did deion sanders jr go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.school </PATH>"], "scores": [-0.020010249689221382, -0.02332872338593006, -0.024066658690571785], "norm_scores": [0.33415326476097107, 0.33304622769355774, 0.3328005373477936]}}
{"id": "WebQTest-614", "question": "when did the battle of alamo start", "prediction": [["time.event.locations"], ["time.event.included_in_event"], ["time.event.includes_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the battle of alamo start [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.010181761346757412, -0.029815835878252983, -0.04595857486128807], "norm_scores": [0.3395109474658966, 0.3329099714756012, 0.3275790512561798]}}
{"id": "WebQTest-615", "question": "what were the names of henry viii 's three children", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what were the names of henry viii 's three children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.012461909092962742, -0.012689230963587761, -0.1309128701686859], "norm_scores": [0.3462522029876709, 0.3461735248565674, 0.30757424235343933]}}
{"id": "WebQTest-616", "question": "what was queen elizabeth ii childhood nickname", "prediction": [["people.person.children"], ["people.person.parents"], ["royalty.monarch.kingdom"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was queen elizabeth ii childhood nickname [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> royalty.monarch.kingdom </PATH>"], "scores": [-0.02381979301571846, -0.029487822204828262, -0.03478304669260979], "norm_scores": [0.3351830244064331, 0.3332885503768921, 0.33152836561203003]}}
{"id": "WebQTest-617", "question": "where did tim tebow grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did tim tebow grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.009881717152893543, -0.019946429878473282, -0.060093771666288376], "norm_scores": [0.3400188684463501, 0.33661386370658875, 0.3233673572540283]}}
{"id": "WebQTest-618", "question": "where is chesapeake bay bridge", "prediction": [["transportation.bridge.locale"], ["transportation.bridge.body_of_water_spanned"], ["location.location.containedby"]], "ground_paths": [["transportation.bridge.locale"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chesapeake bay bridge [/INST]", "raw_output": {"paths": ["<PATH> transportation.bridge.locale </PATH>", "<PATH> transportation.bridge.body_of_water_spanned </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0010087013943120837, -0.06339053809642792, -0.08602439612150192], "norm_scores": [0.34989234805107117, 0.32873228192329407, 0.32137539982795715]}}
{"id": "WebQTest-619", "question": "who played victoria barkley", "prediction": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played victoria barkley [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.019861547276377678, -0.02113940194249153, -0.023364132270216942], "norm_scores": [0.3338645398616791, 0.3334381878376007, 0.33269721269607544]}}
{"id": "WebQTest-620", "question": "where did kennedy 's inaugural address take place", "prediction": [["time.event.locations"], ["location.location.events"], ["architecture.architect.structures_designed"]], "ground_paths": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.country"], ["symbols.name_source.namesakes", "location.location.containedby"], ["common.image.appears_in_topic_gallery", "common.image.image_location"], ["base.inaugurations.inauguration.president", "time.event.locations"], ["base.inaugurations.inauguration.president", "location.location.events"], ["symbols.name_source.namesakes", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kennedy 's inaugural address take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> architecture.architect.structures_designed </PATH>"], "scores": [-0.0007491806172765791, -0.06792137026786804, -0.09787114709615707], "norm_scores": [0.35180553793907166, 0.3289501965045929, 0.31924426555633545]}}
{"id": "WebQTest-621", "question": "what countries have german as the official language", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have german as the official language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01530338916927576, -0.02087540365755558, -0.024114998057484627], "norm_scores": [0.3349331319332123, 0.33307206630706787, 0.33199480175971985]}}
{"id": "WebQTest-622", "question": "where did the taliban began", "prediction": [["organization.organization.place_founded"], ["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the taliban began [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.place_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.019995102658867836, -0.024906354025006294, -0.030288178473711014], "norm_scores": [0.33502402901649475, 0.3333826959133148, 0.3315933048725128]}}
{"id": "WebQTest-624", "question": "who plays elena gilbert on vampire diaries", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays elena gilbert on vampire diaries [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.025400293990969658, -0.03133464604616165, -0.03243691101670265], "norm_scores": [0.3347760736942291, 0.33279529213905334, 0.33242863416671753]}}
{"id": "WebQTest-626", "question": "what does the missouri river bisect", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does the missouri river bisect [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011442592367529869, -0.01898883283138275, -0.05783338099718094], "norm_scores": [0.33931076526641846, 0.33675986528396606, 0.32392939925193787]}}
{"id": "WebQTest-627", "question": "what nationality is taylor lautner jacob black", "prediction": [["people.person.nationality"], ["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is taylor lautner jacob black [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.020258590579032898, -0.023358261212706566, -0.04253600910305977], "norm_scores": [0.3361486792564392, 0.33510836958885193, 0.32874295115470886]}}
{"id": "WebQTest-628", "question": "what animal represents california", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["government.governmental_jurisdiction.official_symbols", "location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what animal represents california [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.018339330330491066, -0.02741994522511959, -0.037482209503650665], "norm_scores": [0.3364737927913666, 0.33343222737312317, 0.33009397983551025]}}
{"id": "WebQTest-629", "question": "what company does nike own", "prediction": [["business.business_operation.industry"], ["business.business_operation.industry", "business.business_operation.industry"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"]], "ground_paths": [["business.business_operation.industry", "business.business_operation.industry"], ["business.industry.companies", "business.industry.companies"], ["business.acquisition.acquiring_company", "business.acquisition.company_acquired"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what company does nike own [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>"], "scores": [-0.028818899765610695, -0.029446180909872055, -0.031353142112493515], "norm_scores": [0.3336845934391022, 0.33347535133361816, 0.33284005522727966]}}
{"id": "WebQTest-630", "question": "what language do colombian people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do colombian people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.009055190719664097, -0.017876669764518738, -0.07012668251991272], "norm_scores": [0.34106722474098206, 0.33807173371315, 0.3208610415458679]}}
{"id": "WebQTest-631", "question": "what year did ray allen join the nba", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did ray allen join the nba [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.018733568489551544, -0.02203458361327648, -0.024331675842404366], "norm_scores": [0.3343227207660675, 0.3332209289073944, 0.3324563801288605]}}
{"id": "WebQTest-633", "question": "what is claire danes famous for", "prediction": [["people.person.profession"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is claire danes famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.0010076687904074788, -0.07469752430915833, -0.07523970305919647], "norm_scores": [0.34996652603149414, 0.32510480284690857, 0.3249286115169525]}}
{"id": "WebQTest-634", "question": "what county is san antonio in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.hud_county_place"], ["location.us_county.county_seat"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san antonio in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.008536029607057571, -0.026582449674606323, -0.03908316418528557], "norm_scores": [0.3387499153614044, 0.3326915204524994, 0.3285585343837738]}}
{"id": "WebQTest-635", "question": "what characters does matt stone do", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"]], "ground_paths": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["tv.tv_guest_role.actor", "tv.tv_character.appeared_in_tv_episodes"], ["film.performance.actor", "film.performance.character"], ["base.popstra.friendship.participant", "base.popstra.friendship.participant"], ["freebase.valuenotation.has_value", "freebase.valuenotation.has_no_value"], ["people.person.ethnicity", "people.person.ethnicity"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["people.person.gender", "people.person.gender"], ["tv.tv_actor.guest_roles", "tv.tv_character.appeared_in_tv_episodes"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what characters does matt stone do [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.character </PATH>"], "scores": [-0.01724134013056755, -0.03871791437268257, -0.03872945159673691], "norm_scores": [0.33812415599823, 0.3309398293495178, 0.3309360146522522]}}
{"id": "WebQTest-636", "question": "what timezone is the state of michigan and", "prediction": [["location.location.time_zones"], ["base.locations.states_and_provences.country", "location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is the state of michigan and [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> base.locations.states_and_provences.country <SEP> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>"], "scores": [-1.1586519576667342e-05, -0.14492984116077423, -0.1556846648454666], "norm_scores": [0.36752113699913025, 0.31793996691703796, 0.3145388960838318]}}
{"id": "WebQTest-637", "question": "what type of government did soviet union have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government did soviet union have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.007172017358243465, -0.018516065552830696, -0.14540313184261322], "norm_scores": [0.3496971130371094, 0.3457525670528412, 0.30455031991004944]}}
{"id": "WebQTest-638", "question": "what languages are there in japan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages are there in japan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.014465522952377796, -0.02180773951113224, -0.032066743820905685], "norm_scores": [0.33610764145851135, 0.3336488902568817, 0.3302434980869293]}}
{"id": "WebQTest-639", "question": "what year did the bulls get rodman", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.sports_team_roster.player", "sports.sports_team.roster"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the bulls get rodman [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.017621560022234917, -0.03302232176065445, -0.039989370852708817], "norm_scores": [0.3375415802001953, 0.33238303661346436, 0.33007532358169556]}}
{"id": "WebQTest-640", "question": "who was saint paul the apostle", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was saint paul the apostle [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.003088430967181921, -0.05135778710246086, -0.06990032643079758], "norm_scores": [0.3462306261062622, 0.32991522550582886, 0.32385414838790894]}}
{"id": "WebQTest-642", "question": "where did vince scully go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did vince scully go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.014401218853890896, -0.02114052139222622, -0.02297578938305378], "norm_scores": [0.3350369334220886, 0.3327866196632385, 0.33217641711235046]}}
{"id": "WebQTest-643", "question": "where does princess leia live", "prediction": [["fictional_universe.fictional_character.places_lived"], ["fictional_universe.fictional_character.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["fictional_universe.fictional_character.places_lived"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does princess leia live [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.places_lived </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0037972647696733475, -0.037484556436538696, -0.06801298260688782], "norm_scores": [0.3442724347114563, 0.33286798000335693, 0.32285958528518677]}}
{"id": "WebQTest-644", "question": "what famous people are from el salvador", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people are from el salvador [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0065482184290885925, -0.03573816642165184, -0.04709697514772415], "norm_scores": [0.34112295508384705, 0.3313095271587372, 0.32756751775741577]}}
{"id": "WebQTest-645", "question": "what type of art did leonardo da vinci make", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did leonardo da vinci make [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>"], "scores": [-0.00020276685245335102, -0.09941596537828445, -0.10300169885158539], "norm_scores": [0.35614335536956787, 0.32250547409057617, 0.32135114073753357]}}
{"id": "WebQTest-646", "question": "where did mary bell live", "prediction": [["people.person.place_of_birth"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mary bell live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.015702243894338608, -0.03210999071598053, -0.03614438697695732], "norm_scores": [0.33743980526924133, 0.3319483697414398, 0.33061185479164124]}}
{"id": "WebQTest-648", "question": "who did kimberly williams marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["film.film.produced_by", "film.music_contributor.film"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kimberly williams marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.01923813298344612, -0.021399425342679024, -0.043031301349401474], "norm_scores": [0.33621034026145935, 0.33548447489738464, 0.3283052444458008]}}
{"id": "WebQTest-649", "question": "what other movies has ryan gosling been in", "prediction": [["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["people.person.nationality", "film.film.featured_film_locations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["people.person.languages", "tv.tv_program.languages"], ["film.performance.actor", "film.performance.film"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["film.film.executive_produced_by"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_honor.honored_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies has ryan gosling been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.024431852623820305, -0.028541818261146545, -0.034733474254608154], "norm_scores": [0.3349354863166809, 0.3335617482662201, 0.3315027952194214]}}
{"id": "WebQTest-650", "question": "what political party does julia gillard belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.politician.party", "government.political_party_tenure.party"]], "ground_paths": [["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party does julia gillard belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.008116055279970169, -0.022294333204627037, -0.02247711271047592], "norm_scores": [0.33651185035705566, 0.33177435398101807, 0.3317137360572815]}}
{"id": "WebQTest-651", "question": "what is the currency of germany now", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of germany now [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.009088405407965183, -0.0209218580275774, -0.05841726437211037], "norm_scores": [0.3401239812374115, 0.3361228406429291, 0.32375314831733704]}}
{"id": "WebQTest-652", "question": "what kind of hispanic is selena gomez", "prediction": [["people.ethnicity.included_in_group"], ["people.person.ethnicity"], ["people.ethnicity.people"]], "ground_paths": [["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of hispanic is selena gomez [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>"], "scores": [-0.011680039577186108, -0.027078328654170036, -0.03317127376794815], "norm_scores": [0.33744364976882935, 0.33228740096092224, 0.330268919467926]}}
{"id": "WebQTest-653", "question": "who did denise van outen married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did denise van outen married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.017007436603307724, -0.035066328942775726, -0.03751396760344505], "norm_scores": [0.33763188123703003, 0.33158934116363525, 0.33077874779701233]}}
{"id": "WebQTest-654", "question": "what are egyptian money called", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are egyptian money called [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.008287007920444012, -0.02932082675397396, -0.05741138756275177], "norm_scores": [0.34115153551101685, 0.3340507447719574, 0.324797660112381]}}
{"id": "WebQTest-655", "question": "what is the zip code for wyoming michigan", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for wyoming michigan [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0015031069051474333, -0.051668815314769745, -0.08238302171230316], "norm_scores": [0.3480226695537567, 0.33099454641342163, 0.32098284363746643]}}
{"id": "WebQTest-656", "question": "who is the president of costa rica in 2012", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of costa rica in 2012 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.0033160699531435966, -0.03308204561471939, -0.042986705899238586], "norm_scores": [0.3410898745059967, 0.3310866057872772, 0.3278235197067261]}}
{"id": "WebQTest-657", "question": "where did martin luther king got shot at", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did martin luther king got shot at [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.00020954853971488774, -0.11456847935914993, -0.11957839876413345], "norm_scores": [0.3597875237464905, 0.3209080994129181, 0.31930437684059143]}}
{"id": "WebQTest-658", "question": "who wrote the gospel according to john", "prediction": [["book.book_subject.works"], ["book.author.works_written"], ["book.written_work.author"]], "ground_paths": [["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote the gospel according to john [/INST]", "raw_output": {"paths": ["<PATH> book.book_subject.works </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.012889927253127098, -0.024074804037809372, -0.03496985882520676], "norm_scores": [0.33703628182411194, 0.3332875669002533, 0.32967609167099]}}
{"id": "WebQTest-659", "question": "what form of government is the uk", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["form.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government is the uk [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> form.form_of_government.countries </PATH>"], "scores": [-0.007830287329852581, -0.018200401216745377, -0.1494985818862915], "norm_scores": [0.34994491934776306, 0.3463347256183624, 0.3037203550338745]}}
{"id": "WebQTest-661", "question": "who did nasri play for before arsenal", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did nasri play for before arsenal [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.025256238877773285, -0.035477884113788605, -0.0388665497303009], "norm_scores": [0.33598625659942627, 0.332569420337677, 0.3314443826675415]}}
{"id": "WebQTest-662", "question": "what currency does the czech republic use now", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does the czech republic use now [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.013189214281737804, -0.02232547290623188, -0.028104577213525772], "norm_scores": [0.3360101282596588, 0.3329542279243469, 0.3310356140136719]}}
{"id": "WebQTest-663", "question": "what age did william penn get married", "prediction": [["people.person.spouse_s", "people.marriage.spouse"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what age did william penn get married [/INST]", "raw_output": {"paths": ["<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.014113451354205608, -0.03570825606584549, -0.036798521876335144], "norm_scores": [0.33827129006385803, 0.3310447037220001, 0.33068394660949707]}}
{"id": "WebQTest-664", "question": "what was firefox programmed in", "prediction": [["computer.software_developer.software"], ["film.film.story_by"], ["computer.software.language"]], "ground_paths": [["computer.software.languages_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was firefox programmed in [/INST]", "raw_output": {"paths": ["<PATH> computer.software_developer.software </PATH>", "<PATH> film.film.story_by </PATH>", "<PATH> computer.software.language </PATH>"], "scores": [-0.0529789961874485, -0.053585201501846313, -0.05850045382976532], "norm_scores": [0.3340138792991638, 0.3338114321231842, 0.332174688577652]}}
{"id": "WebQTest-665", "question": "what government does new zealand have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.main_country", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government does new zealand have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.form_of_government </PATH>"], "scores": [-0.005880257580429316, -0.022817902266979218, -0.07845937460660934], "norm_scores": [0.34326547384262085, 0.33750033378601074, 0.3192342221736908]}}
{"id": "WebQTest-666", "question": "where is isthmus of panama located on the map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is isthmus of panama located on the map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.001141913584433496, -0.059081196784973145, -0.09194383025169373], "norm_scores": [0.35002902150154114, 0.3303249478340149, 0.31964603066444397]}}
{"id": "WebQTest-667", "question": "what is the capital of republic of ireland", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.location.contains"]], "ground_paths": [["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of republic of ireland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.007481846958398819, -0.02795267291367054, -0.0567978098988533], "norm_scores": [0.3411085903644562, 0.3341968059539795, 0.32469457387924194]}}
{"id": "WebQTest-668", "question": "what has kristen stewart acted in", "prediction": [["people.person.languages", "film.film.language"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["people.person.place_of_birth", "film.film.subjects"], ["film.actor.film", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has kristen stewart acted in [/INST]", "raw_output": {"paths": ["<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.026440000161528587, -0.028147397562861443, -0.034357666969299316], "norm_scores": [0.33440256118774414, 0.3338320851325989, 0.331765353679657]}}
{"id": "WebQTest-669", "question": "what did dr josef mengele do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["base.crime.crime_victim.perpetrator"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr josef mengele do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> base.crime.crime_victim.perpetrator </PATH>"], "scores": [-0.0021379119716584682, -0.05700906738638878, -0.10399717837572098], "norm_scores": [0.35090625286102295, 0.33217036724090576, 0.3169233202934265]}}
{"id": "WebQTest-670", "question": "what kind of language does egypt speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does egypt speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.01944185607135296, -0.02405478060245514, -0.02582344226539135], "norm_scores": [0.3345559537410736, 0.33301621675491333, 0.3324277400970459]}}
{"id": "WebQTest-671", "question": "who are the senators of hawaii 2012", "prediction": [["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of hawaii 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.022334469482302666, -0.02517608553171158, -0.031558260321617126], "norm_scores": [0.3346741497516632, 0.3337244987487793, 0.3316013813018799]}}
{"id": "WebQTest-672", "question": "what form of government does north korea have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does north korea have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010412340052425861, -0.013107052072882652, -0.13172289729118347], "norm_scores": [0.34685274958610535, 0.34591934084892273, 0.3072279095649719]}}
{"id": "WebQTest-673", "question": "where did robert kardashian go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robert kardashian go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> location.location.containedby </PATH>"], "scores": [-0.013366088271141052, -0.017495667561888695, -0.030838703736662865], "norm_scores": [0.33573290705680847, 0.3343493342399597, 0.3299177587032318]}}
{"id": "WebQTest-675", "question": "what county is st james city fl in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.us_county.hud_county_place"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st james city fl in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>"], "scores": [-0.020870808511972427, -0.023972725495696068, -0.02851690724492073], "norm_scores": [0.3345280587673187, 0.3334920108318329, 0.3319799602031708]}}
{"id": "WebQTest-676", "question": "what did the akkadian empire include", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.country.languages_spoken"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the akkadian empire include [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.012516744434833527, -0.029453473165631294, -0.03346448019146919], "norm_scores": [0.337555468082428, 0.33188652992248535, 0.33055800199508667]}}
{"id": "WebQTest-677", "question": "after how many years are the olympic games held", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.current_frequency"]], "ground_paths": [["time.recurring_event.current_frequency"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: after how many years are the olympic games held [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>"], "scores": [-0.011187717318534851, -0.0158309917896986, -0.04898130148649216], "norm_scores": [0.33803436160087585, 0.3364683985710144, 0.32549723982810974]}}
{"id": "WebQTest-678", "question": "what system of government does guyana have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what system of government does guyana have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.007932656444609165, -0.01712295599281788, -0.14281833171844482], "norm_scores": [0.3490806221961975, 0.3458871841430664, 0.3050321936607361]}}
{"id": "WebQTest-680", "question": "what team did aguero play for", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did aguero play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.matches_played <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.02410808391869068, -0.025861872360110283, -0.030383069068193436], "norm_scores": [0.33422544598579407, 0.33363980054855347, 0.33213475346565247]}}
{"id": "WebQTest-681", "question": "who is the president of the european union 2012", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["organization.organization_membership.organization", "organization.organization_membership.member"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of the european union 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_membership.member </PATH>"], "scores": [-0.011061207391321659, -0.016802283003926277, -0.02990795113146305], "norm_scores": [0.3360660970211029, 0.3341422379016876, 0.3297916650772095]}}
{"id": "WebQTest-683", "question": "what year was kenya moore crowned miss usa", "prediction": [["people.person.nationality", "location.location.events"], ["people.person.nationality", "time.event.locations"], ["award.competitor.competitions", "award.competitor.competitions"]], "ground_paths": [["time.recurring_event.instances"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was kenya moore crowned miss usa [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.events </PATH>", "<PATH> people.person.nationality <SEP> time.event.locations </PATH>", "<PATH> award.competitor.competitions <SEP> award.competitor.competitions </PATH>"], "scores": [-0.016726866364479065, -0.03080092743039131, -0.04743453860282898], "norm_scores": [0.3383197486400604, 0.3335915803909302, 0.328088641166687]}}
{"id": "WebQTest-684", "question": "what sort of government does nigeria have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.continent"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sort of government does nigeria have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.005838526878505945, -0.021498652175068855, -0.14606060087680817], "norm_scores": [0.3504312336444855, 0.3449861705303192, 0.3045826554298401]}}
{"id": "WebQTest-685", "question": "where does chef ina garten live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does chef ina garten live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.019194841384887695, -0.022812265902757645, -0.02927723340690136], "norm_scores": [0.3348560929298401, 0.3336469531059265, 0.331496924161911]}}
{"id": "WebQTest-686", "question": "when the atlanta braves won the world series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.mascot.team"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when the atlanta braves won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.mascot.team </PATH>"], "scores": [-0.00992826372385025, -0.015542292036116123, -0.05310848727822304], "norm_scores": [0.33873724937438965, 0.3368408977985382, 0.32442182302474976]}}
{"id": "WebQTest-688", "question": "what kind of language do they speak in iran", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language do they speak in iran [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013942128978669643, -0.018614277243614197, -0.034862764179706573], "norm_scores": [0.3361756503582001, 0.33460864424705505, 0.3292157053947449]}}
{"id": "WebQTest-689", "question": "what was the first language spoken in spain", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the first language spoken in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.0189115758985281, -0.020631218329072, -0.020840588957071304], "norm_scores": [0.33373886346817017, 0.3331654667854309, 0.3330956995487213]}}
{"id": "WebQTest-690", "question": "what did scott joplin play", "prediction": [["music.group_member.instruments_played"], ["people.person.profession"], ["music.instrument.family"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did scott joplin play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.0021023054141551256, -0.07484890520572662, -0.08470336347818375], "norm_scores": [0.35080888867378235, 0.3261948823928833, 0.32299619913101196]}}
{"id": "WebQTest-691", "question": "what school did miley cyrus", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did miley cyrus [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.019494827836751938, -0.024216940626502037, -0.02658076025545597], "norm_scores": [0.33464646339416504, 0.33306998014450073, 0.33228355646133423]}}
{"id": "WebQTest-693", "question": "what language do most people speak in afghanistan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do most people speak in afghanistan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01602083258330822, -0.018996400758624077, -0.030705854296684265], "norm_scores": [0.3352946639060974, 0.3342984616756439, 0.3304068446159363]}}
{"id": "WebQTest-694", "question": "who is the prime minister of jamaica west indies", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the prime minister of jamaica west indies [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.010598130524158478, -0.027632277458906174, -0.03195066377520561], "norm_scores": [0.3376115560531616, 0.33190932869911194, 0.33047911524772644]}}
{"id": "WebQTest-695", "question": "what role did paul mccartney play in the beatles", "prediction": [["music.musical_group.member", "music.group_membership.role"], ["music.musical_group.member", "music.performance_role.regular_performances"], ["music.producer.releases_produced", "music.group_membership.member", "music.group_membership.role"]], "ground_paths": [["music.group_member.instruments_played", "music.instrument.variation"], ["music.group_member.membership", "music.group_membership.role"], ["music.group_member.instruments_played", "music.instrument.family"], ["music.group_membership.member", "music.group_membership.role"], ["music.group_member.membership", "music.performance_role.regular_performances"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what role did paul mccartney play in the beatles [/INST]", "raw_output": {"paths": ["<PATH> music.musical_group.member <SEP> music.group_membership.role </PATH>", "<PATH> music.musical_group.member <SEP> music.performance_role.regular_performances </PATH>", "<PATH> music.producer.releases_produced <SEP> music.group_membership.member <SEP> music.group_membership.role </PATH>"], "scores": [-0.013588828034698963, -0.021988485008478165, -0.024305220693349838], "norm_scores": [0.3354605734348297, 0.33265459537506104, 0.33188483119010925]}}
{"id": "WebQTest-696", "question": "when did juventus win the champions league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did juventus win the champions league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.005607016384601593, -0.02326078526675701, -0.08536548167467117], "norm_scores": [0.34413450956344604, 0.3381125330924988, 0.3177529573440552]}}
{"id": "WebQTest-698", "question": "who is robin roberts father", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is robin roberts father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.007955169305205345, -0.0234242994338274, -0.10013262182474136], "norm_scores": [0.34523314237594604, 0.3399337828159332, 0.3148330748081207]}}
{"id": "WebQTest-699", "question": "what is the capital of australia victoria state", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.state"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of australia victoria state [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.01530381478369236, -0.03804276883602142, -0.03804550692439079], "norm_scores": [0.3384057581424713, 0.33079758286476135, 0.33079665899276733]}}
{"id": "WebQTest-700", "question": "where is french spoken most", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["base.rosetta.local_name.locale"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is french spoken most [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.016496572643518448, -0.023390280082821846, -0.02410156838595867], "norm_scores": [0.3349462151527405, 0.33264514803886414, 0.3324086368083954]}}
{"id": "WebQTest-701", "question": "what team is reggie bush on 2011", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"]], "ground_paths": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is reggie bush on 2011 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>"], "scores": [-0.01998712494969368, -0.023113775998353958, -0.023193279281258583], "norm_scores": [0.3340373635292053, 0.3329945504665375, 0.3329681158065796]}}
{"id": "WebQTest-702", "question": "what highschool did emily osment go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did emily osment go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.014239100739359856, -0.021657831966876984, -0.023460598662495613], "norm_scores": [0.3351846933364868, 0.3327072560787201, 0.3321080207824707]}}
{"id": "WebQTest-703", "question": "what currency should i take to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should i take to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.005572874564677477, -0.03259359300136566, -0.05882921442389488], "norm_scores": [0.3422924876213074, 0.33316734433174133, 0.3245401382446289]}}
{"id": "WebQTest-704", "question": "who is thor heyerdahl", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is thor heyerdahl [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0012894209939986467, -0.05796881020069122, -0.09766703844070435], "norm_scores": [0.3505060076713562, 0.3311920762062073, 0.31830188632011414]}}
{"id": "WebQTest-706", "question": "where did troy aikman play football", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did troy aikman play football [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.011599279940128326, -0.013737235218286514, -0.05952067673206329], "norm_scores": [0.33885976672172546, 0.33813610672950745, 0.3230040967464447]}}
{"id": "WebQTest-707", "question": "who played cletus hogg", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played cletus hogg [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.02017044462263584, -0.02792137861251831, -0.03217277675867081], "norm_scores": [0.33553123474121094, 0.33294060826301575, 0.3315281569957733]}}
{"id": "WebQTest-708", "question": "who won fa cup 1976", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won fa cup 1976 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.019275769591331482, -0.02317194826900959, -0.023731807246804237], "norm_scores": [0.3342619836330414, 0.33296218514442444, 0.3327758312225342]}}
{"id": "WebQTest-709", "question": "what did general robert e lee die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["base.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did general robert e lee die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> base.cause_of_death.people </PATH>"], "scores": [-0.001725686015561223, -0.04064738377928734, -0.11228219419717789], "norm_scores": [0.34999769926071167, 0.33663687109947205, 0.3133654296398163]}}
{"id": "WebQTest-710", "question": "what style of art did henri matisse do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of art did henri matisse do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.0017901250394061208, -0.03758346289396286, -0.08259253948926926], "norm_scores": [0.3463544547557831, 0.33417654037475586, 0.31946903467178345]}}
{"id": "WebQTest-711", "question": "who plays chuck bass in gossip girl", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["base.gossipgirl.character.played_by"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays chuck bass in gossip girl [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.021277206018567085, -0.030415523797273636, -0.03431584686040878], "norm_scores": [0.33580154180526733, 0.33274686336517334, 0.33145156502723694]}}
{"id": "WebQTest-712", "question": "what countries have territories in oceania", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries have territories in oceania [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.008477597497403622, -0.035778023302555084, -0.04442363977432251], "norm_scores": [0.3403952419757843, 0.33122801780700684, 0.3283767104148865]}}
{"id": "WebQTest-713", "question": "what county is st. louis park in", "prediction": [["location.location.containedby"], ["location.us_county.county_seat"], ["location.us_county.hud_county_place"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st. louis park in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.county_seat </PATH>", "<PATH> location.us_county.hud_county_place </PATH>"], "scores": [-0.006800736766308546, -0.038032542914152145, -0.04500081017613411], "norm_scores": [0.34109073877334595, 0.3306024670600891, 0.32830676436424255]}}
{"id": "WebQTest-714", "question": "what is the political system of south africa", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political system of south africa [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010798376984894276, -0.013561197556555271, -0.07660624384880066], "norm_scores": [0.34088370203971863, 0.33994320034980774, 0.31917309761047363]}}
{"id": "WebQTest-716", "question": "what language group does germany belong to", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language group does germany belong to [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.013081441633403301, -0.027482032775878906, -0.03488296642899513], "norm_scores": [0.3373663127422333, 0.33254286646842957, 0.33009079098701477]}}
{"id": "WebQTest-717", "question": "what timezone is tucson arizona in right now", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is tucson arizona in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-8.680021892359946e-06, -0.17523188889026642, -0.18192844092845917], "norm_scores": [0.37412017583847046, 0.31398773193359375, 0.3118921220302582]}}
{"id": "WebQTest-718", "question": "when was lucy lawless born", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was lucy lawless born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.0006703877006657422, -0.078405000269413, -0.08287879079580307], "norm_scores": [0.3513345718383789, 0.3250582218170166, 0.3236072361469269]}}
{"id": "WebQTest-719", "question": "which political party does barack obama represent", "prediction": [["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "ground_paths": [["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.election.winner", "government.primary_election.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which political party does barack obama represent [/INST]", "raw_output": {"paths": ["<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.018641410395503044, -0.021254321560263634, -0.02254941314458847], "norm_scores": [0.3340582251548767, 0.3331865072250366, 0.3327552378177643]}}
{"id": "WebQTest-721", "question": "what kind of leukemia did jill clayburgh have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.person.medical_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of leukemia did jill clayburgh have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.person.medical_condition </PATH>"], "scores": [-0.01122350711375475, -0.013208212330937386, -0.06024964526295662], "norm_scores": [0.338963121175766, 0.3382910490036011, 0.32274582982063293]}}
{"id": "WebQTest-723", "question": "what is there to see in sydney australia", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is there to see in sydney australia [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013794082216918468, -0.016031702980399132, -0.029829228296875954], "norm_scores": [0.33536139130592346, 0.3346118628978729, 0.3300267159938812]}}
{"id": "WebQTest-724", "question": "where was marilyn monroe found dead", "prediction": [["people.deceased_person.place_of_death"], ["people.cause_of_death.people"], ["people.place_of_death.people_found_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was marilyn monroe found dead [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.place_of_death.people_found_here </PATH>"], "scores": [-0.0003174618468619883, -0.08374091237783432, -0.11592523753643036], "norm_scores": [0.35577237606048584, 0.3272968530654907, 0.31693077087402344]}}
{"id": "WebQTest-725", "question": "what states does the sierra nevada run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states does the sierra nevada run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.00691524613648653, -0.027620725333690643, -0.043754108250141144], "norm_scores": [0.339750200510025, 0.3327878713607788, 0.32746192812919617]}}
{"id": "WebQTest-726", "question": "what language is spoken in greek", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in greek [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01476929523050785, -0.017849529162049294, -0.042231496423482895], "norm_scores": [0.33671891689300537, 0.33568334579467773, 0.3275977075099945]}}
{"id": "WebQTest-728", "question": "what is the name of the speaker of the house of representatives now 2011", "prediction": [["common.topic.image", "common.topic.image"], ["government.government_office_or_title.office_holders", "government.government_position_held.office_holder"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"]], "ground_paths": [["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["common.topic.image", "common.image.appears_in_topic_gallery"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the speaker of the house of representatives now 2011 [/INST]", "raw_output": {"paths": ["<PATH> common.topic.image <SEP> common.topic.image </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.010470789857208729, -0.015966985374689102, -0.025506850332021713], "norm_scores": [0.3356160521507263, 0.3337765038013458, 0.33060744404792786]}}
{"id": "WebQTest-729", "question": "who plays stanley hudson", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays stanley hudson [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.02241339534521103, -0.02945798449218273, -0.033021073788404465], "norm_scores": [0.33529722690582275, 0.33294349908828735, 0.3317592740058899]}}
{"id": "WebQTest-730", "question": "what record label is kelly clarkson signed with", "prediction": [["music.artist.label"], ["music.record_label.artist"], ["music.recording.artist"]], "ground_paths": [["music.artist.label"], ["music.record_label.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what record label is kelly clarkson signed with [/INST]", "raw_output": {"paths": ["<PATH> music.artist.label </PATH>", "<PATH> music.record_label.artist </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.0018291735323145986, -0.058004021644592285, -0.07431916892528534], "norm_scores": [0.3477717936038971, 0.3287743330001831, 0.3234538435935974]}}
{"id": "WebQTest-731", "question": "where does tim roth come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does tim roth come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.00677964324131608, -0.03299471735954285, -0.04888422042131424], "norm_scores": [0.3409600257873535, 0.3321378827095032, 0.3269020915031433]}}
{"id": "WebQTest-733", "question": "what currency should you take to vietnam", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"]], "ground_paths": [["base.monetarydebase.debased_currency.issuing_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should you take to vietnam [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.011126648634672165, -0.016597799956798553, -0.070782370865345], "norm_scores": [0.3405260741710663, 0.3386681079864502, 0.3208058178424835]}}
{"id": "WebQTest-734", "question": "who did roger federer married", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did roger federer married [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.dated.participant </PATH>"], "scores": [-0.024341756477952003, -0.035989657044410706, -0.04144516959786415], "norm_scores": [0.336534708738327, 0.3326375186443329, 0.3308277428150177]}}
{"id": "WebQTest-735", "question": "what kind of money do i need in costa rica", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["government.form_of_government.countries"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do i need in costa rica [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> government.form_of_government.countries </PATH>"], "scores": [-0.006843741983175278, -0.02073807641863823, -0.10536708682775497], "norm_scores": [0.34573647379875183, 0.3409658968448639, 0.3132975697517395]}}
{"id": "WebQTest-737", "question": "what is the name of the capital of belgium", "prediction": [["location.country.capital"], ["base.biblioness.bibs_location.country"], ["location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.country.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the capital of belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.011870261281728745, -0.02166076935827732, -0.03066720813512802], "norm_scores": [0.33651497960090637, 0.333236426115036, 0.3302485942840576]}}
{"id": "WebQTest-738", "question": "what kind of government does japan have now", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does japan have now [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009219628758728504, -0.015007958747446537, -0.12110157310962677], "norm_scores": [0.34621506929397583, 0.34421682357788086, 0.3095681369304657]}}
{"id": "WebQTest-739", "question": "what pieces of music did claude debussy compose", "prediction": [["music.composition.composer"], ["music.artist.genre"], ["music.recording.artist"]], "ground_paths": [["book.written_work.author"], ["music.composition.composer"], ["opera.librettist.libretti"], ["opera.opera.librettist"], ["music.composition.lyricist"], ["music.lyricist.lyrics_written"], ["book.author.works_written"], ["book.author.works_written", "book.book.editions"], ["music.recording.artist"], ["base.musiteca.music_teacher.composition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what pieces of music did claude debussy compose [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.artist.genre </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.007188628893345594, -0.03411173075437546, -0.05498934164643288], "norm_scores": [0.3416747450828552, 0.3325985372066498, 0.3257266581058502]}}
{"id": "WebQTest-740", "question": "who was selena gomez in barney and friends", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_guest_role.actor", "tv.tv_character.appeared_in_tv_episodes"], ["film.actor.film", "film.performance.character"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.character"], ["tv.tv_actor.guest_roles", "tv.tv_character.appeared_in_tv_episodes"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_actor.guest_roles", "tv.tv_guest_role.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was selena gomez in barney and friends [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.02377137541770935, -0.027016181498765945, -0.03331736475229263], "norm_scores": [0.33475494384765625, 0.3336704671382904, 0.33157455921173096]}}
{"id": "WebQTest-741", "question": "who plays caesar flickerman in the hunger games", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.film.starring", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays caesar flickerman in the hunger games [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.026156749576330185, -0.03022046945989132, -0.03820427879691124], "norm_scores": [0.33512410521507263, 0.33376502990722656, 0.3311109244823456]}}
{"id": "WebQTest-742", "question": "where did the gettysburg address", "prediction": [["book.written_work.subjects"], ["book.author.book_editions_published"], ["book.book_subject.works"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the gettysburg address [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.book_subject.works </PATH>"], "scores": [-0.011472695507109165, -0.044705722481012344, -0.049129609018564224], "norm_scores": [0.34125539660453796, 0.3301008343696594, 0.3286437392234802]}}
{"id": "WebQTest-743", "question": "what was the name of jfk 's brothers", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.family.members", "people.family_member.family"], ["people.person.parents", "people.person.parents"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["people.person.ethnicity", "people.person.ethnicity"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["film.film_subject.films", "film.film.subjects"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.family.members", "people.family.members"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.nationality", "people.person.nationality"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.gender", "people.person.gender"], ["people.person.religion", "people.person.religion"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of jfk 's brothers [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_no_value </PATH>"], "scores": [-0.03162381798028946, -0.03377705067396164, -0.037370793521404266], "norm_scores": [0.3342113196849823, 0.3334924578666687, 0.33229613304138184]}}
{"id": "WebQTest-744", "question": "who was tupac signed to", "prediction": [["music.artist.label"], ["music.record_label.artist"], ["music.recording.artist"]], "ground_paths": [["music.artist.label"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was tupac signed to [/INST]", "raw_output": {"paths": ["<PATH> music.artist.label </PATH>", "<PATH> music.record_label.artist </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.008795956149697304, -0.02534317970275879, -0.057712066918611526], "norm_scores": [0.3406168818473816, 0.33502697944641113, 0.3243561387062073]}}
{"id": "WebQTest-745", "question": "what was augustus caesar famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was augustus caesar famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.004832057747989893, -0.04489472135901451, -0.05169406160712242], "norm_scores": [0.343059241771698, 0.32958704233169556, 0.32735365629196167]}}
{"id": "WebQTest-747", "question": "what language does colombia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does colombia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.010335490107536316, -0.019432906061410904, -0.0617922805249691], "norm_scores": [0.34004485607147217, 0.3369653522968292, 0.3229897916316986]}}
{"id": "WebQTest-748", "question": "what is the zip code for schenectady new york", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for schenectady new york [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00047262184671126306, -0.0658615455031395, -0.10915710031986237], "norm_scores": [0.3528934717178345, 0.33055639266967773, 0.3165501654148102]}}
{"id": "WebQTest-749", "question": "who was president in 1988 in the united states", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_office_or_title.jurisdiction", "people.person.nationality"]], "ground_paths": [["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["freebase.type_profile.equivalent_topic", "common.topic.notable_types"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president in 1988 in the united states [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_office_or_title.jurisdiction <SEP> people.person.nationality </PATH>"], "scores": [-0.024185679852962494, -0.024208394810557365, -0.02577672339975834], "norm_scores": [0.3335125744342804, 0.3335050046443939, 0.3329823911190033]}}
{"id": "WebQTest-751", "question": "who owns google 2012", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["organization.organization.leadership", "business.board_member.leader_of"], ["organization.organization_founder.organizations_founded"], ["organization.leadership.organization", "organization.leadership.person"], ["conferences.conference.focus", "time.event.people_involved"], ["organization.leadership.organization", "business.board_member.leader_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns google 2012 [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.021470634266734123, -0.021786734461784363, -0.05139639973640442], "norm_scores": [0.3366774916648865, 0.33657106757164, 0.3267514407634735]}}
{"id": "WebQTest-752", "question": "what drink did john pemberton create", "prediction": [["food.beer_country_region.beers_from_here"], ["food.beer_country_region.beers_from_here", "food.beer_country_region.beers_from_here"], ["food.beer.from_region"]], "ground_paths": [["law.inventor.inventions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drink did john pemberton create [/INST]", "raw_output": {"paths": ["<PATH> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beer_country_region.beers_from_here <SEP> food.beer_country_region.beers_from_here </PATH>", "<PATH> food.beer.from_region </PATH>"], "scores": [-0.008633927442133427, -0.026826433837413788, -0.044834136962890625], "norm_scores": [0.33939501643180847, 0.3332763612270355, 0.32732856273651123]}}
{"id": "WebQTest-753", "question": "what position did stanley matthews play", "prediction": [["sports.pro_athlete.teams", "sports.sports_position.teams"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["soccer.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did stanley matthews play [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_position.teams </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.031347692012786865, -0.034421373158693314, -0.03805811330676079], "norm_scores": [0.33442094922065735, 0.3333946168422699, 0.332184374332428]}}
{"id": "WebQTest-755", "question": "what was bonnie prince charlie full name", "prediction": [["people.person.nationality"], ["common.topic.notable_types"], ["people.person.children"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was bonnie prince charlie full name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.007929557003080845, -0.051102764904499054, -0.05375620722770691], "norm_scores": [0.343294233083725, 0.32878848910331726, 0.327917218208313]}}
{"id": "WebQTest-756", "question": "who makes the voice of stewie griffin", "prediction": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who makes the voice of stewie griffin [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>"], "scores": [-0.019474724307656288, -0.021859178319573402, -0.022624626755714417], "norm_scores": [0.3339485228061676, 0.33315321803092957, 0.3328982889652252]}}
{"id": "WebQTest-757", "question": "who is angelina jolie mom and dad", "prediction": [["people.person.parents"], ["people.person.children"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolie mom and dad [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.009699203073978424, -0.016824224963784218, -0.09680879861116409], "norm_scores": [0.3437044024467468, 0.34126418828964233, 0.3150314390659332]}}
{"id": "WebQTest-758", "question": "who is the governor of indiana 2009", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["common.topic.image"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of indiana 2009 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.018342820927500725, -0.01942502148449421, -0.023992909118533134], "norm_scores": [0.3340812027454376, 0.3337198793888092, 0.3321989178657532]}}
{"id": "WebQTest-759", "question": "who brad pitt has dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.popstra.lived_with.participant", "base.popstra.celebrity.lived_with"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.nationality", "people.person.place_of_birth"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.insult_victim", "base.popstra.public_insult.perpetrator"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who brad pitt has dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.01619391329586506, -0.02984079159796238, -0.050141774117946625], "norm_scores": [0.33863091468811035, 0.334041029214859, 0.32732805609703064]}}
{"id": "WebQTest-760", "question": "what party does barack obama belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"]], "ground_paths": [["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.election.winner", "government.primary_election.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party does barack obama belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>"], "scores": [-0.01652892678976059, -0.020226575434207916, -0.028520667925477028], "norm_scores": [0.3350769877433777, 0.3338402807712555, 0.331082820892334]}}
{"id": "WebQTest-761", "question": "what is san francisco zip code", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is san francisco zip code [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00022331916261464357, -0.08722072094678879, -0.16570879518985748], "norm_scores": [0.3617733418941498, 0.3316302001476288, 0.3065964877605438]}}
{"id": "WebQTest-762", "question": "who has stephen ireland played for", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has stephen ireland played for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.030754411593079567, -0.03237108886241913, -0.03670058026909828], "norm_scores": [0.33417367935180664, 0.3336338400840759, 0.3321925103664398]}}
{"id": "WebQTest-764", "question": "where did john mayer go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john mayer go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015626437962055206, -0.019356640055775642, -0.02019316703081131], "norm_scores": [0.3342558443546295, 0.33301132917404175, 0.3327328562736511]}}
{"id": "WebQTest-765", "question": "what celebrities has brody jenner dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["base.popstra.celebrity.canoodled", "base.popstra.canoodled.participant"]], "ground_paths": [["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.popstra.friendship.participant", "base.popstra.friendship.participant"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["base.popstra.canoodled.participant", "base.popstra.celebrity.canoodled"], ["base.popstra.breakup.participant", "base.popstra.celebrity.breakup"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["base.popstra.breakup.participant", "base.popstra.breakup.participant"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.gender", "people.person.gender"], ["base.popstra.celebrity.canoodled", "base.popstra.canoodled.participant"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.infidelity.perpetrator", "base.popstra.infidelity.participant"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what celebrities has brody jenner dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> base.popstra.celebrity.canoodled <SEP> base.popstra.canoodled.participant </PATH>"], "scores": [-0.01696406491100788, -0.0278115626424551, -0.03970750793814659], "norm_scores": [0.33707210421562195, 0.33343544602394104, 0.329492449760437]}}
{"id": "WebQTest-766", "question": "what is malaysian currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is malaysian currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.007036953698843718, -0.023567963391542435, -0.12932178378105164], "norm_scores": [0.3486141264438629, 0.34289854764938354, 0.30848732590675354]}}
{"id": "WebQTest-768", "question": "who owns volkswagen car company", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization.board_members", "business.board_member.organization_board_memberships"]], "ground_paths": [["base.schemastaging.corresponding_entity.entities", "base.schemastaging.entity_hint.corresponds_to"], ["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization_relationship.parent"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns volkswagen car company [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization.board_members <SEP> business.board_member.organization_board_memberships </PATH>"], "scores": [-0.026199206709861755, -0.030355403199791908, -0.039032626897096634], "norm_scores": [0.33522161841392517, 0.3338312804698944, 0.33094707131385803]}}
{"id": "WebQTest-769", "question": "which is the main train station in rome", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["trainsdaily.travel_destination.travel_destinations"]], "ground_paths": [["travel.travel_destination.how_to_get_here", "travel.transportation.transport_terminus"], ["location.location.containedby", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which is the main train station in rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> trainsdaily.travel_destination.travel_destinations </PATH>"], "scores": [-0.002839151304215193, -0.06507793068885803, -0.08555186539888382], "norm_scores": [0.3496168553829193, 0.32852041721343994, 0.32186272740364075]}}
{"id": "WebQTest-770", "question": "what did steve mcnair died from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did steve mcnair died from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0009859083220362663, -0.05064291134476662, -0.11019810289144516], "norm_scores": [0.3511117696762085, 0.33410245180130005, 0.31478583812713623]}}
{"id": "WebQTest-771", "question": "who is the newly elected governor of california", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the newly elected governor of california [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.biblioness.bibs_location.state <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.021651409566402435, -0.02339462749660015, -0.029232628643512726], "norm_scores": [0.3343692421913147, 0.33378687500953674, 0.33184388279914856]}}
{"id": "WebQTest-772", "question": "who plays roxanne in ghost rider", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.film.language", "people.person.languages"]], "ground_paths": [["film.film.language", "people.person.languages"], ["film.performance.film", "film.performance.actor"], ["film.film.starring", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays roxanne in ghost rider [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.film.language <SEP> people.person.languages </PATH>"], "scores": [-0.02067366987466812, -0.025043316185474396, -0.029543276876211166], "norm_scores": [0.334805428981781, 0.3333456218242645, 0.33184894919395447]}}
{"id": "WebQTest-773", "question": "where is the sony ericsson company", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the sony ericsson company [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.005042145494371653, -0.03417039290070534, -0.046784862875938416], "norm_scores": [0.34124934673309326, 0.33145272731781006, 0.3272978961467743]}}
{"id": "WebQTest-774", "question": "what kind of currency does the dominican republic have", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does the dominican republic have [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-6.788096652599052e-05, -0.10719644278287888, -0.13369086384773254], "norm_scores": [0.36057737469673157, 0.32394635677337646, 0.31547629833221436]}}
{"id": "WebQTest-775", "question": "what to do this weekend in manchester with kids", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do this weekend in manchester with kids [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012622703798115253, -0.016276121139526367, -0.028916552662849426], "norm_scores": [0.33554893732070923, 0.33432525396347046, 0.3301258385181427]}}
{"id": "WebQTest-776", "question": "what sport is popular in italy", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "ground_paths": [["sports.sports_team.location"], ["cricket.cricket_team.country"], ["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sport is popular in italy [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>"], "scores": [-0.010906653478741646, -0.014786805026233196, -0.13036806881427765], "norm_scores": [0.3467976748943329, 0.3454546630382538, 0.30774763226509094]}}
{"id": "WebQTest-777", "question": "where is the main headquarters of google", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["organization.organization.phone_number", "common.phone_number.service_location"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the main headquarters of google [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.01595688797533512, -0.025094933807849884, -0.03553803265094757], "norm_scores": [0.3365288972854614, 0.3334676921367645, 0.33000338077545166]}}
{"id": "WebQTest-778", "question": "what was jesse owens education", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was jesse owens education [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.013439174741506577, -0.018760409206151962, -0.0341353677213192], "norm_scores": [0.3362238109111786, 0.3344394266605377, 0.3293367624282837]}}
{"id": "WebQTest-779", "question": "where does carlos santana live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.person.children", "people.person.place_of_birth"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.children", "people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.country"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.country.first_level_divisions"], ["music.artist.origin", "location.location.containedby"], ["symbols.namesake.named_after", "music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does carlos santana live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.01963762938976288, -0.023130498826503754, -0.028000881895422935], "norm_scores": [0.3346513509750366, 0.3334844708442688, 0.33186423778533936]}}
{"id": "WebQTest-780", "question": "what is the new movie john carter about", "prediction": [["film.film.subjects"], ["film.film_story_contributor.film_story_credits"], ["film.film.story_by"]], "ground_paths": [["film.film.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the new movie john carter about [/INST]", "raw_output": {"paths": ["<PATH> film.film.subjects </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.021676266565918922, -0.021934689953923225, -0.03271736204624176], "norm_scores": [0.3345867991447449, 0.33450034260749817, 0.33091288805007935]}}
{"id": "WebQTest-782", "question": "what countries does nafta include", "prediction": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["common.topic.notable_types"]], "ground_paths": [["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization_membership.organization", "organization.organization_member.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does nafta include [/INST]", "raw_output": {"paths": ["<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.010553310625255108, -0.026921594515442848, -0.037688978016376495], "norm_scores": [0.3381812572479248, 0.33269086480140686, 0.32912787795066833]}}
{"id": "WebQTest-783", "question": "what county is houston tx in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is houston tx in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.005457909312099218, -0.034829381853342056, -0.04468153789639473], "norm_scores": [0.34099534153938293, 0.3311254382133484, 0.3278791606426239]}}
{"id": "WebQTest-784", "question": "where do the seattle seahawks play", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the seattle seahawks play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.009215916506946087, -0.015651002526283264, -0.08416097611188889], "norm_scores": [0.34230396151542664, 0.34010830521583557, 0.3175877332687378]}}
{"id": "WebQTest-785", "question": "who does michael keaton play in cars", "prediction": [["film.performance.actor", "film.performance.character"], ["film.performance.film", "film.performance.character"], ["film.actor.film", "film.performance.character"]], "ground_paths": [["film.performance.film", "film.film_character.portrayed_in_films"], ["fictional_universe.fictional_universe.works_set_here", "fictional_universe.fictional_universe.characters"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does michael keaton play in cars [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.028455449268221855, -0.030825458467006683, -0.03730254992842674], "norm_scores": [0.3345796763896942, 0.3337876498699188, 0.33163267374038696]}}
{"id": "WebQTest-787", "question": "where is the riviera casino", "prediction": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the riviera casino [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.009487105533480644, -0.03507338836789131, -0.04554328694939613], "norm_scores": [0.34021422266960144, 0.33161982893943787, 0.3281659483909607]}}
{"id": "WebQTest-788", "question": "what is the holy book of the buddhist religion", "prediction": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"], ["book.written_work.subjects"]], "ground_paths": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the holy book of the buddhist religion [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.texts </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.004691580776125193, -0.027167443186044693, -0.06266013532876968], "norm_scores": [0.34229525923728943, 0.3346876800060272, 0.32301709055900574]}}
{"id": "WebQTest-789", "question": "who became president when henry harrison died", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who became president when henry harrison died [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.022625016048550606, -0.02557259425520897, -0.032541800290346146], "norm_scores": [0.3347628712654114, 0.33377760648727417, 0.33145952224731445]}}
{"id": "WebQTest-790", "question": "where did richard arkwright grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did richard arkwright grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0018599015893414617, -0.04277366027235985, -0.1271926462650299], "norm_scores": [0.35185059905052185, 0.3377455770969391, 0.3104037642478943]}}
{"id": "WebQTest-791", "question": "what other books did louis sachar write", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other books did louis sachar write [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.012120037339627743, -0.02676466852426529, -0.028692055493593216], "norm_scores": [0.33681073784828186, 0.33191418647766113, 0.3312751054763794]}}
{"id": "WebQTest-792", "question": "when did roth ira originate", "prediction": [["finance.currency.countries_used"], ["common.topic.notable_types"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did roth ira originate [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.013755055144429207, -0.03050360456109047, -0.03745456412434578], "norm_scores": [0.337841272354126, 0.33223000168800354, 0.3299286961555481]}}
{"id": "WebQTest-794", "question": "who does the voice for darth vader", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does the voice for darth vader [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>"], "scores": [-0.023092439398169518, -0.024808580055832863, -0.02793198451399803], "norm_scores": [0.334061861038208, 0.3334890604019165, 0.3324490785598755]}}
{"id": "WebQTest-796", "question": "what did thomas malthus do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did thomas malthus do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.001839768374338746, -0.05575665831565857, -0.07062171399593353], "norm_scores": [0.3470967411994934, 0.32887792587280273, 0.32402530312538147]}}
{"id": "WebQTest-797", "question": "what is the zip code for newark california", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for newark california [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00012327033618930727, -0.09432146698236465, -0.1521231234073639], "norm_scores": [0.3611293435096741, 0.3286646902561188, 0.31020593643188477]}}
{"id": "WebQTest-798", "question": "what states share a border with tennessee", "prediction": [["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains", "location.location.partially_contains"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.cyclone_affected_area.cyclones"], ["base.locations.countries.states_provinces_within", "location.administrative_division.country"], ["time.event.locations", "time.event.locations"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["location.location.events", "location.location.events"], ["base.locations.countries.states_provinces_within", "location.country.first_level_divisions"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["base.locations.countries.states_provinces_within", "base.locations.countries.states_provinces_within"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["meteorology.cyclone_affected_area.cyclones", "meteorology.tropical_cyclone.affected_areas"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "base.usnationalparks.us_national_park.state"], ["location.location.partially_contains", "geography.river.origin"], ["location.location.containedby", "location.location.contains"], ["base.locations.countries.states_provinces_within", "location.administrative_division.first_level_division_of"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["time.event.locations", "location.location.events"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.locations.countries.states_provinces_within", "base.aareas.schema.administrative_area.administrative_children"], ["base.locations.countries.states_provinces_within", "base.aareas.schema.administrative_area.administrative_parent"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.location.events", "time.event.locations"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["book.written_work.subjects", "book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what states share a border with tennessee [/INST]", "raw_output": {"paths": ["<PATH> meteorology.cyclone_affected_area.cyclones <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones <SEP> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03352750465273857, -0.03471503406763077, -0.040073707699775696], "norm_scores": [0.3341923952102661, 0.33379578590393066, 0.3320118188858032]}}
{"id": "WebQTest-800", "question": "what war did the us lose the most soldiers", "prediction": [["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.casualties.military_conflict"], ["military.casualties.combatant", "military.casualties.military_conflict"]], "ground_paths": [["military.military_command.military_combatant", "military.military_conflict.commanders"], ["base.culturalevent.event.entity_involved", "time.event.included_in_event"], ["location.location.partially_containedby", "location.location.events"], ["military.military_combatant.military_commanders", "military.military_conflict.commanders"], ["military.military_command.military_combatant", "military.military_command.military_conflict"], ["military.military_combatant.casualties", "military.casualties.military_conflict"], ["people.person.nationality", "military.military_person.participated_in_conflicts"], ["military.military_combatant.military_commanders", "military.military_command.military_conflict"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what war did the us lose the most soldiers [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.military_conflict </PATH>", "<PATH> military.casualties.combatant <SEP> military.casualties.military_conflict </PATH>"], "scores": [-0.015300591476261616, -0.02981337159872055, -0.037956032902002335], "norm_scores": [0.33747398853302, 0.33261168003082275, 0.3299143314361572]}}
{"id": "WebQTest-801", "question": "what kind of government is egypt", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is egypt [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.006046778056770563, -0.02228607051074505, -0.12212888896465302], "norm_scores": [0.3479115068912506, 0.34230732917785645, 0.30978113412857056]}}
{"id": "WebQTest-802", "question": "what musical instruments does justin bieber play", "prediction": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.instruments_played"], ["music.track_contribution.contributor", "music.track_contribution.role"]], "ground_paths": [["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.membership", "music.group_membership.role"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what musical instruments does justin bieber play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.track_contribution.contributor <SEP> music.track_contribution.role </PATH>"], "scores": [-0.0034891285467892885, -0.0461103729903698, -0.055334143340587616], "norm_scores": [0.34390848875045776, 0.32955867052078247, 0.32653284072875977]}}
{"id": "WebQTest-805", "question": "where do john lennon die", "prediction": [["people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do john lennon die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0006624497473239899, -0.0777842104434967, -0.10977405309677124], "norm_scores": [0.3543074429035187, 0.3280097544193268, 0.31768280267715454]}}
{"id": "WebQTest-809", "question": "what is the current time in bangalore india", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the current time in bangalore india [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-0.0001803633786039427, -0.0713496208190918, -0.1767154484987259], "norm_scores": [0.36107945442199707, 0.3362748324871063, 0.3026457130908966]}}
{"id": "WebQTest-810", "question": "what language do people from greece speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from greece speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01411346159875393, -0.014192462898790836, -0.04909719526767731], "norm_scores": [0.3372064530849457, 0.3371798098087311, 0.32561367750167847]}}
{"id": "WebQTest-811", "question": "where is the phillies spring training stadium", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["location.location.containedby"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the phillies spring training stadium [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.0033303790260106325, -0.04649800434708595, -0.051064446568489075], "norm_scores": [0.3435082733631134, 0.32899531722068787, 0.32749640941619873]}}
{"id": "WebQTest-812", "question": "what movies does matt dallas play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does matt dallas play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.012339244596660137, -0.03416413813829422, -0.037785328924655914], "norm_scores": [0.3386058211326599, 0.33129584789276123, 0.33009833097457886]}}
{"id": "WebQTest-813", "question": "who was john f kennedy 's family", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.parents"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_no_value"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["people.person.profession", "common.topic.notable_types"], ["people.sibling_relationship.sibling", "people.sibling_relationship.sibling"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.family.members", "people.family_member.family"], ["people.person.parents", "people.person.parents"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["symbols.name_source.namesakes"], ["people.person.ethnicity", "people.person.ethnicity"], ["base.americancomedy.impersonated_celebrity.impersonated_by", "base.americancomedy.impersonated_celebrity.impersonated_by"], ["film.film_subject.films", "film.film.subjects"], ["people.deceased_person.cause_of_death", "people.deceased_person.cause_of_death"], ["people.family.members", "people.family.members"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.nationality", "people.person.nationality"], ["military.military_person.participated_in_conflicts", "military.military_person.participated_in_conflicts"], ["people.sibling_relationship.sibling", "people.person.sibling_s"], ["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.gender", "people.person.gender"], ["people.person.religion", "people.person.religion"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was john f kennedy 's family [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_no_value </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03210928291082382, -0.032726895064115524, -0.03460441157221794], "norm_scores": [0.33367919921875, 0.3334731459617615, 0.3328476548194885]}}
{"id": "WebQTest-814", "question": "where is harley davidson corporate headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded"]], "ground_paths": [["organization.organization.place_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is harley davidson corporate headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded </PATH>"], "scores": [-0.005024528130888939, -0.04278992488980293, -0.042979106307029724], "norm_scores": [0.3417990803718567, 0.3291316032409668, 0.3290693461894989]}}
{"id": "WebQTest-818", "question": "where was the ottoman empire based", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.country.capital"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the ottoman empire based [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.country.capital </PATH>"], "scores": [-0.006763044279068708, -0.042003169655799866, -0.061369143426418304], "norm_scores": [0.34337925910949707, 0.33148929476737976, 0.3251314163208008]}}
{"id": "WebQTest-819", "question": "where was bin laden found and killed", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was bin laden found and killed [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.021105248481035233, -0.03122442215681076, -0.03297074884176254], "norm_scores": [0.33578044176101685, 0.33239975571632385, 0.3318197727203369]}}
{"id": "WebQTest-820", "question": "what are the names of harry potter movies in order", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["fictional_universe.fictional_universe.works_set_here"], ["film.film_series.films_in_series"]], "ground_paths": [["fictional_universe.fictional_universe.works_set_here"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["film.film.film_series"], ["film.film_series.films_in_series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of harry potter movies in order [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>", "<PATH> film.film_series.films_in_series </PATH>"], "scores": [-0.006434849929064512, -0.03258003294467926, -0.03870774060487747], "norm_scores": [0.3398544490337372, 0.3310840427875519, 0.32906147837638855]}}
{"id": "WebQTest-821", "question": "what did pamela courson die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did pamela courson die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0024873416405171156, -0.03617173433303833, -0.1153322085738182], "norm_scores": [0.34963008761405945, 0.33804911375045776, 0.3123207986354828]}}
{"id": "WebQTest-822", "question": "what did dr jack kevorkian do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["base.activism.activism_issue.activists"], ["base.activism.activist.area_of_activism"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did dr jack kevorkian do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0018853266956284642, -0.047861497849226, -0.07160238176584244], "norm_scores": [0.3462936580181122, 0.33073288202285767, 0.32297345995903015]}}
{"id": "WebQTest-823", "question": "what does bolivia border", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains", "location.location.partially_contains"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.partially_contains", "geography.river.basin_countries"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["time.event.locations", "time.event.locations"], ["symbols.name_source.namesakes", "people.person.nationality"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["people.deceased_person.place_of_death", "people.person.nationality"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.contains", "location.location.containedby"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.events", "location.location.events"], ["location.location.contains", "location.location.contains"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["language.language_family.geographic_distribution", "location.country.official_language"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["location.country.form_of_government", "government.form_of_government.countries"], ["location.location.containedby", "location.location.containedby"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["military.military_combatant_group.combatants", "military.military_combatant.military_conflicts"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.official_language", "language.human_language.main_country"], ["language.language_family.geographic_distribution", "language.human_language.main_country"], ["location.country.languages_spoken", "location.country.official_language"], ["time.event.locations", "location.location.events"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["location.location.partially_contains", "location.location.partially_containedby"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.events", "time.event.locations"], ["location.country.official_language", "location.country.languages_spoken"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does bolivia border [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>"], "scores": [-0.036186251789331436, -0.04340488836169243, -0.04682113230228424], "norm_scores": [0.3353196680545807, 0.3329078257083893, 0.33177250623703003]}}
{"id": "WebQTest-825", "question": "who does nolan ryan play for", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does nolan ryan play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.008283665403723717, -0.017266562208533287, -0.04122762382030487], "norm_scores": [0.33799198269844055, 0.3349694311618805, 0.32703858613967896]}}
{"id": "WebQTest-827", "question": "what country does justin bieber come from", "prediction": [["people.person.nationality"], ["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does justin bieber come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.003863482503220439, -0.0445755235850811, -0.04993244633078575], "norm_scores": [0.34304359555244446, 0.3293580412864685, 0.3275984227657318]}}
{"id": "WebQTest-828", "question": "what is the head judge of the supreme court called", "prediction": [["law.judicial_tenure.title", "law.judge.courts"], ["law.court.jurisdiction", "people.person.nationality"], ["law.court.jurisdiction", "government.government_office_or_title.office_holders"]], "ground_paths": [["government.governmental_body.offices_positions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the head judge of the supreme court called [/INST]", "raw_output": {"paths": ["<PATH> law.judicial_tenure.title <SEP> law.judge.courts </PATH>", "<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> law.court.jurisdiction <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.026787105947732925, -0.034296371042728424, -0.04390236735343933], "norm_scores": [0.3360724151134491, 0.33355820178985596, 0.33036938309669495]}}
{"id": "WebQTest-829", "question": "who was frida kahlo influenced by", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was frida kahlo influenced by [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.011721239425241947, -0.012011989019811153, -0.07330640405416489], "norm_scores": [0.34013810753822327, 0.3400392532348633, 0.31982266902923584]}}
{"id": "WebQTest-830", "question": "where is taylor swift from", "prediction": [["music.artist.origin"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is taylor swift from [/INST]", "raw_output": {"paths": ["<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0156997162848711, -0.021458175033330917, -0.031168848276138306], "norm_scores": [0.33569347858428955, 0.3337659537792206, 0.3305405378341675]}}
{"id": "WebQTest-831", "question": "what type of religion does argentina have", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["religion.religion.countries"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religion does argentina have [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> religion.religion.countries </PATH>"], "scores": [-0.008848518133163452, -0.01446661539375782, -0.07519789785146713], "norm_scores": [0.3412734568119049, 0.3393615186214447, 0.319364994764328]}}
{"id": "WebQTest-832", "question": "when is the last time the giants won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the giants won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.007812961004674435, -0.01639278419315815, -0.0819137915968895], "norm_scores": [0.3424616754055023, 0.3395359516143799, 0.3180023729801178]}}
{"id": "WebQTest-833", "question": "what is the main language used in china", "prediction": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the main language used in china [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.006731765810400248, -0.030101750046014786, -0.04223432019352913], "norm_scores": [0.3399023711681366, 0.3320509195327759, 0.32804667949676514]}}
{"id": "WebQTest-834", "question": "where did the battle of waterloo occur", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the battle of waterloo occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.003176851663738489, -0.037557486444711685, -0.11429531872272491], "norm_scores": [0.349523663520813, 0.33771103620529175, 0.3127652406692505]}}
{"id": "WebQTest-836", "question": "who did vladimir lenin marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.has_no_value"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did vladimir lenin marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.025557516142725945, -0.04417199268937111, -0.04424865543842316], "norm_scores": [0.3374911844730377, 0.3312670886516571, 0.3312416672706604]}}
{"id": "WebQTest-837", "question": "what year did the baltimore ravens win superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the baltimore ravens win superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.013881424441933632, -0.014115042984485626, -0.0320875458419323], "norm_scores": [0.3353763222694397, 0.33529800176620483, 0.32932567596435547]}}
{"id": "WebQTest-838", "question": "what team does cristiano ronaldo play for 2011", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does cristiano ronaldo play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.019790971651673317, -0.02189071662724018, -0.025329366326332092], "norm_scores": [0.3341822326183319, 0.3334812521934509, 0.33233651518821716]}}
{"id": "WebQTest-841", "question": "what is money called in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is money called in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.00939187966287136, -0.018512237817049026, -0.10176537185907364], "norm_scores": [0.3445085287094116, 0.341380774974823, 0.314110666513443]}}
{"id": "WebQTest-843", "question": "where did theodor schwann study", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did theodor schwann study [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.021118490025401115, -0.021794790402054787, -0.023810764774680138], "norm_scores": [0.3337076008319855, 0.3334819972515106, 0.3328103721141815]}}
{"id": "WebQTest-845", "question": "who are the virginia senators 2013", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_body.jurisdiction", "people.person.nationality"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.administrative_division.country", "people.person.nationality"], ["government.election.district", "government.election.winner"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the virginia senators 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_body.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.016001002863049507, -0.01992080733180046, -0.03429445996880531], "norm_scores": [0.33580029010772705, 0.33448657393455505, 0.3297131657600403]}}
{"id": "WebQTest-846", "question": "what was dr seuss 's real name", "prediction": [["book.book.interior_illustrations_by", "people.person.nationality"], ["people.person.profession"], ["book.book_edition.author_editor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss 's real name [/INST]", "raw_output": {"paths": ["<PATH> book.book.interior_illustrations_by <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.023256419226527214, -0.03162011504173279, -0.03552353382110596], "norm_scores": [0.3356291353702545, 0.33283376693725586, 0.3315370976924896]}}
{"id": "WebQTest-847", "question": "what countries are members of the security council", "prediction": [["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_body_membership.member"]], "ground_paths": [["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body_membership.body", "base.unitednations.united_nations_member_state.member_of"], ["base.unitednations.united_nations_body.members", "base.unitednations.united_nations_body_membership.member"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are members of the security council [/INST]", "raw_output": {"paths": ["<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_member_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body.members <SEP> base.unitednations.united_nations_member_state.member_of </PATH>", "<PATH> base.unitednations.united_nations_body_membership.body <SEP> base.unitednations.united_nations_body_membership.member </PATH>"], "scores": [-0.010043922811746597, -0.012937618419528008, -0.026299873366951942], "norm_scores": [0.33545947074890137, 0.33449018001556396, 0.3300503194332123]}}
{"id": "WebQTest-848", "question": "where are the nordic countries", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the nordic countries [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.008087711408734322, -0.027763444930315018, -0.05294979363679886], "norm_scores": [0.3405245244503021, 0.3338899314403534, 0.3255855143070221]}}
{"id": "WebQTest-849", "question": "what country sponsored ferdinand magellan", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country sponsored ferdinand magellan [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0037684300914406776, -0.056825924664735794, -0.06688501685857773], "norm_scores": [0.3463612198829651, 0.3284631371498108, 0.3251756727695465]}}
{"id": "WebQTest-850", "question": "what did vera ellen die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did vera ellen die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0022115670144557953, -0.03874809667468071, -0.13171064853668213], "norm_scores": [0.3517833650112152, 0.3391624093055725, 0.3090541660785675]}}
{"id": "WebQTest-851", "question": "what is the capital of modern day egypt", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of modern day egypt [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.010404570959508419, -0.02494666911661625, -0.03868069872260094], "norm_scores": [0.3381025195121765, 0.33322134613990784, 0.32867616415023804]}}
{"id": "WebQTest-852", "question": "what province is toronto in ( 7 letters )", "prediction": [["location.province.capital"], ["base.biblioness.bibs_location.state"], ["base.biblioness.bibs_location.province"]], "ground_paths": [["location.province.capital"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what province is toronto in ( 7 letters ) [/INST]", "raw_output": {"paths": ["<PATH> location.province.capital </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>", "<PATH> base.biblioness.bibs_location.province </PATH>"], "scores": [-0.002846317132934928, -0.04290619492530823, -0.05795912444591522], "norm_scores": [0.3439842164516449, 0.3304766118526459, 0.3255392014980316]}}
{"id": "WebQTest-855", "question": "what is the currency of brazil now", "prediction": [["location.country.currency_used"], ["location.country.currency_formerly_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of brazil now [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.0011684061028063297, -0.06247398257255554, -0.0676419585943222], "norm_scores": [0.34767812490463257, 0.32700371742248535, 0.3253181278705597]}}
{"id": "WebQTest-856", "question": "what types of art did andy warhol do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what types of art did andy warhol do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.006767838727682829, -0.020272521302103996, -0.05946635454893112], "norm_scores": [0.34068623185157776, 0.3361162841320038, 0.3231974244117737]}}
{"id": "WebQTest-857", "question": "what other states border florida", "prediction": [["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "ground_paths": [["location.location.partially_contains", "location.location.partially_contains"], ["location.administrative_division.first_level_division_of", "location.administrative_division.first_level_division_of"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.containedby", "location.location.contains"], ["location.location.events", "time.event.locations"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.contains", "location.location.contains"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.time_zones", "location.location.time_zones"], ["location.administrative_division.first_level_division_of", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other states border florida [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.02630637399852276, -0.03637142851948738, -0.038865916430950165], "norm_scores": [0.3358517289161682, 0.3324882984161377, 0.3316599726676941]}}
{"id": "WebQTest-859", "question": "what are the predominant religions in russia", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["language.human_language.main_country", "location.country.languages_spoken"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the predominant religions in russia [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> language.human_language.main_country <SEP> location.country.languages_spoken </PATH>"], "scores": [-0.008457127958536148, -0.012574134394526482, -0.10932425409555435], "norm_scores": [0.3448341488838196, 0.34341737627983093, 0.3117484152317047]}}
{"id": "WebQTest-861", "question": "who will coach carolina panthers in 2013", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who will coach carolina panthers in 2013 [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>"], "scores": [-0.003951657563447952, -0.03548897057771683, -0.03719809278845787], "norm_scores": [0.3405698835849762, 0.32999682426452637, 0.32943329215049744]}}
{"id": "WebQTest-862", "question": "where did louis sachar grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did louis sachar grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.004182793200016022, -0.02871910110116005, -0.10160582512617111], "norm_scores": [0.3468687832355499, 0.3384614586830139, 0.3146696984767914]}}
{"id": "WebQTest-865", "question": "what to do for fun in houston", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do for fun in houston [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013253502547740936, -0.019088581204414368, -0.02660403586924076], "norm_scores": [0.3354668915271759, 0.33351510763168335, 0.33101800084114075]}}
{"id": "WebQTest-866", "question": "where did otto frank go to college", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did otto frank go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.014642372727394104, -0.017861926928162575, -0.02673291228711605], "norm_scores": [0.33503443002700806, 0.3339574933052063, 0.33100807666778564]}}
{"id": "WebQTest-867", "question": "what else has jennifer lawrence played in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what else has jennifer lawrence played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.023847166448831558, -0.03125544637441635, -0.03219582885503769], "norm_scores": [0.3350863754749298, 0.33261311054229736, 0.3323005139827728]}}
{"id": "WebQTest-868", "question": "who is the governor of pennsylvania state now", "prediction": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_children", "people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of pennsylvania state now [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.023421086370944977, -0.024370485916733742, -0.025333072990179062], "norm_scores": [0.3336513042449951, 0.33333471417427063, 0.33301398158073425]}}
{"id": "WebQTest-869", "question": "where is ancient phoenician", "prediction": [["location.location.containedby"], ["language.human_language.countries_spoken_in"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is ancient phoenician [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.02224409393966198, -0.026977794244885445, -0.030809316784143448], "norm_scores": [0.3348122239112854, 0.3332310914993286, 0.33195674419403076]}}
{"id": "WebQTest-870", "question": "where does the celtics practice", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the celtics practice [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.0022966242395341396, -0.03872981667518616, -0.13979892432689667], "norm_scores": [0.3526397943496704, 0.3400232493877411, 0.3073369860649109]}}
{"id": "WebQTest-872", "question": "where bin laden got killed", "prediction": [["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where bin laden got killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.013471738435328007, -0.027452996000647545, -0.043820906430482864], "norm_scores": [0.33826950192451477, 0.33357298374176025, 0.328157514333725]}}
{"id": "WebQTest-873", "question": "where does toronto get its water from", "prediction": [["location.location.containedby"], ["base.infrastructure.sewage_treatment_plant.serves_location_s"], ["base.infrastructure.sewage_treatment_plant.serves_location_s", "base.infrastructure.sewage_treatment_plant.serves_location_s"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does toronto get its water from [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>", "<PATH> base.infrastructure.sewage_treatment_plant.serves_location_s <SEP> base.infrastructure.sewage_treatment_plant.serves_location_s </PATH>"], "scores": [-0.015416466630995274, -0.024090850725769997, -0.03981876000761986], "norm_scores": [0.3370116949081421, 0.33410099148750305, 0.32888737320899963]}}
{"id": "WebQTest-874", "question": "what language do people from bosnia speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from bosnia speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014285902492702007, -0.022399835288524628, -0.027539458125829697], "norm_scores": [0.33571097254753113, 0.33299803733825684, 0.33129096031188965]}}
{"id": "WebQTest-875", "question": "where did hugo chavez go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did hugo chavez go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015386322513222694, -0.02116355672478676, -0.02847806178033352], "norm_scores": [0.33543169498443604, 0.33349940180778503, 0.3310689330101013]}}
{"id": "WebQTest-876", "question": "who does japan import from", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.imports_and_exports.exported_to", "location.statistical_region.places_exported_to"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.statistical_region.places_imported_from", "location.imports_and_exports.imported_from"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["base.charities.charity.geographic_scope", "organization.organization.geographic_scope"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["location.imports_and_exports.exported_to", "location.imports_and_exports.imported_from"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"], ["base.aareas.schema.administrative_area.administrative_parent", "base.locations.countries.planet"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["organization.organization.founders", "organization.organization.founders"], ["cvg.computer_game_region.versions_released_in_this_region", "cvg.computer_game_region.versions_released_in_this_region"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["travel.travel_destination.tour_operators", "organization.organization.geographic_scope"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["fictional_universe.fictional_universe.locations", "fictional_universe.fictional_universe.locations"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"], ["film.film.featured_film_locations", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does japan import from [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.02684726193547249, -0.035592224448919296, -0.038878049701452255], "norm_scores": [0.3356454074382782, 0.3327229917049408, 0.33163154125213623]}}
{"id": "WebQTest-877", "question": "what is the name of sonia gandhi daughter", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of sonia gandhi daughter [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.009090892970561981, -0.018183201551437378, -0.1105417013168335], "norm_scores": [0.3454858660697937, 0.3423588275909424, 0.3121553063392639]}}
{"id": "WebQTest-878", "question": "who do islamic people pray to", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["base.argumentmaps.intellectual_dispute.positions"]], "ground_paths": [["religion.deity.deity_of"], ["religion.religion.deities"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who do islamic people pray to [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> base.argumentmaps.intellectual_dispute.positions </PATH>"], "scores": [-0.0124181117862463, -0.014425816014409065, -0.06524187326431274], "norm_scores": [0.3393809497356415, 0.3387002646923065, 0.3219188451766968]}}
{"id": "WebQTest-879", "question": "where was mary todd lincoln from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was mary todd lincoln from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0035341293551027775, -0.033022671937942505, -0.10665632039308548], "norm_scores": [0.34807324409484863, 0.3379589021205902, 0.31396788358688354]}}
{"id": "WebQTest-880", "question": "where did charles darwin die", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did charles darwin die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.deceased_person.place_of_death </PATH>"], "scores": [-0.0008593700476922095, -0.06985761970281601, -0.07543656975030899], "norm_scores": [0.34947142004966736, 0.3261715769767761, 0.32435694336891174]}}
{"id": "WebQTest-881", "question": "what movies does nick cannon play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["music.artist.track", "music.recording.releases"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.tv_producer.programs_produced", "tv.tv_program.tv_producer"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does nick cannon play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.016142334789037704, -0.0348382294178009, -0.03698747605085373], "norm_scores": [0.33774104714393616, 0.3314853012561798, 0.33077365159988403]}}
{"id": "WebQTest-882", "question": "when did the big earthquake hit san francisco", "prediction": [["time.event.locations"], ["location.location.events"], ["event.disaster.areas_affected"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the big earthquake hit san francisco [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> event.disaster.areas_affected </PATH>"], "scores": [-0.007522486615926027, -0.028225040063261986, -0.0477248951792717], "norm_scores": [0.34012389183044434, 0.3331548273563385, 0.32672131061553955]}}
{"id": "WebQTest-883", "question": "who was king george vi married to", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["film.film.subjects", "film.film_subject.films"], ["people.family.members", "people.family.members"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.religion", "people.person.religion"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was king george vi married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03548672795295715, -0.03995473310351372, -0.040495410561561584], "norm_scores": [0.3343871235847473, 0.3328964114189148, 0.3327164947986603]}}
{"id": "WebQTest-884", "question": "where was jesus after he died on the cross", "prediction": [["people.deceased_person.place_of_death"], ["location.location.events"], ["symbols.name_source.namesakes"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was jesus after he died on the cross [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.location.events </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.008661442436277866, -0.02871011570096016, -0.06964711099863052], "norm_scores": [0.34234997630119324, 0.33555465936660767, 0.32209542393684387]}}
{"id": "WebQTest-885", "question": "who is sir james dyson", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is sir james dyson [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0006416848045773804, -0.06915341317653656, -0.10404074937105179], "norm_scores": [0.3526653051376343, 0.3293127119541168, 0.3180219531059265]}}
{"id": "WebQTest-886", "question": "who voiced darth vader in the original star wars movies", "prediction": [["film.performance.character", "film.performance.actor"], ["film.performance.special_performance_type", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who voiced darth vader in the original star wars movies [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.022222300991415977, -0.022676121443510056, -0.029599850997328758], "norm_scores": [0.33420273661613464, 0.33405110239982605, 0.3317461907863617]}}
{"id": "WebQTest-887", "question": "what high school did maya angelou go to", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what high school did maya angelou go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01686285249888897, -0.020542653277516365, -0.021932348608970642], "norm_scores": [0.33430612087249756, 0.33307820558547974, 0.3326156735420227]}}
{"id": "WebQTest-888", "question": "what 5 countries border switzerland", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["geography.river.origin", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"], ["geography.lake.basin_countries", "location.location.partially_contains"], ["base.aareas.schema.administrative_area.subdivides_place"], ["location.location.partially_contains", "geography.river.basin_countries"], ["fictional_universe.work_of_fiction.setting", "fictional_universe.fictional_setting.works_set_here"], ["time.event.locations", "time.event.locations"], ["location.country.languages_spoken", "language.human_language.main_country"], ["film.film_location.featured_in_films", "film.film.featured_film_locations"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.contains", "location.location.containedby"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.events", "location.location.events"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["location.location.contains", "location.location.contains"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["location.location.containedby", "base.locations.countries.continent"], ["film.film.country", "film.film.country"], ["location.country.official_language", "location.country.official_language"], ["freebase.valuenotation.has_no_value", "freebase.valuenotation.is_reviewed"], ["book.newspaper.circulation_areas", "book.newspaper.circulation_areas"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.containedby", "location.location.containedby"], ["location.country.form_of_government", "government.form_of_government.countries"], ["location.location.containedby", "base.nations.supranational_body.member"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["geography.lake.basin_countries", "geography.lake.basin_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["time.event.locations", "location.location.events"], ["people.person.nationality", "people.person.nationality"], ["sports.multi_event_tournament.participating_countries", "sports.sport_country.multi_event_tournaments_participated_in"], ["location.location.partially_contains", "location.location.partially_containedby"], ["geography.lake.basin_countries", "location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.location.contains", "location.location.partially_containedby"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["location.country.form_of_government", "location.country.form_of_government"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["location.location.time_zones", "location.location.time_zones"], ["fictional_universe.fictional_setting.universe", "fictional_universe.fictional_setting.universe"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["book.written_work.subjects", "book.written_work.subjects"], ["base.mystery.cryptid_classification.area_of_occurrence", "base.mystery.cryptid_observation_location.cryptid_s_occurring_here"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 5 countries border switzerland [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.03585408627986908, -0.040126778185367584, -0.046526651829481125], "norm_scores": [0.3349948525428772, 0.3335665762424469, 0.3314386010169983]}}
{"id": "WebQTest-889", "question": "what country did benito mussolini govern", "prediction": [["people.person.nationality"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did benito mussolini govern [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.015956275165081024, -0.02744792215526104, -0.03122757375240326], "norm_scores": [0.3363132178783417, 0.33247053623199463, 0.3312162756919861]}}
{"id": "WebQTest-891", "question": "who was the leader of the first communist party of china", "prediction": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"], ["organization.organization.founders", "influence.influence_node.influenced"]], "ground_paths": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the leader of the first communist party of china [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization.founders <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.003961765207350254, -0.028683461248874664, -0.0832301676273346], "norm_scores": [0.3449021279811859, 0.33648011088371277, 0.3186177909374237]}}
{"id": "WebQTest-892", "question": "what did miles davis died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did miles davis died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.002204674994572997, -0.03873464837670326, -0.12532393634319305], "norm_scores": [0.35108816623687744, 0.3384943902492523, 0.3104174733161926]}}
{"id": "WebQTest-893", "question": "what is the zip code for azle texas", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for azle texas [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0001251128560397774, -0.09836932271718979, -0.13340164721012115], "norm_scores": [0.3594987988471985, 0.32585957646369934, 0.3146416246891022]}}
{"id": "WebQTest-894", "question": "what date did sally pearson won gold", "prediction": [["people.person.nationality", "olympics.olympic_participating_country.olympics_participated_in"], ["people.person.profession", "common.topic.notable_types"], ["people.person.nationality", "olympics.olympic_games.participating_countries"]], "ground_paths": [["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.olympics"], ["people.person.nationality", "olympics.olympic_games.participating_countries"], ["olympics.olympic_athlete_affiliation.athlete", "olympics.olympic_athlete_affiliation.olympics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what date did sally pearson won gold [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> people.person.profession <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.nationality <SEP> olympics.olympic_games.participating_countries </PATH>"], "scores": [-0.020199552178382874, -0.0279946681112051, -0.0333402119576931], "norm_scores": [0.3356627821922302, 0.33305642008781433, 0.33128079771995544]}}
{"id": "WebQTest-895", "question": "when did the detroit pistons last win the championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the detroit pistons last win the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.011061420664191246, -0.011570160277187824, -0.1139192059636116], "norm_scores": [0.344620019197464, 0.34444472193717957, 0.31093522906303406]}}
{"id": "WebQTest-896", "question": "when did the philadelphia flyers win the cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the philadelphia flyers win the cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.008903211914002895, -0.014588420279324055, -0.10561303794384003], "norm_scores": [0.3445720672607422, 0.3426186740398407, 0.3128092288970947]}}
{"id": "WebQTest-898", "question": "what county is st paul nc in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul nc in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.010652302764356136, -0.0278757456690073, -0.03669072687625885], "norm_scores": [0.33815526962280273, 0.33238092064857483, 0.3294638395309448]}}
{"id": "WebQTest-899", "question": "where is olympic national park wa", "prediction": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is olympic national park wa [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.00887957401573658, -0.03081088699400425, -0.03133154660463333], "norm_scores": [0.33828288316726685, 0.330944687128067, 0.33077239990234375]}}
{"id": "WebQTest-900", "question": "where did nat turner 1831 rebellion occur", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did nat turner 1831 rebellion occur [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.012029789388179779, -0.013998440466821194, -0.059648677706718445], "norm_scores": [0.33880728483200073, 0.33814096450805664, 0.3230517506599426]}}
{"id": "WebQTest-901", "question": "who would play luke skywalker", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who would play luke skywalker [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.020725686103105545, -0.021461045369505882, -0.023027712479233742], "norm_scores": [0.33367082476615906, 0.3334255516529083, 0.33290359377861023]}}
{"id": "WebQTest-903", "question": "where turkish people originate", "prediction": [["people.ethnicity.geographic_distribution"], ["people.ethnicity.geographic_distribution", "people.ethnicity.geographic_distribution"], ["people.ethnicity.included_in_group"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where turkish people originate [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.geographic_distribution <SEP> people.ethnicity.geographic_distribution </PATH>", "<PATH> people.ethnicity.included_in_group </PATH>"], "scores": [-7.329801883315668e-05, -0.12669259309768677, -0.13178439438343048], "norm_scores": [0.3626258671283722, 0.3194983899593353, 0.3178757131099701]}}
{"id": "WebQTest-904", "question": "what state is barack obama from", "prediction": [["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.nationality", "base.aareas.schema.administrative_area.administrative_children"], ["people.person.place_of_birth", "base.biblioness.bibs_location.state"], ["base.popstra.celebrity.vacations_in", "base.popstra.location.vacationers"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is barack obama from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth <SEP> base.biblioness.bibs_location.state </PATH>", "<PATH> people.person.nationality <SEP> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.017463143914937973, -0.02353895641863346, -0.0438639372587204], "norm_scores": [0.3369400203227997, 0.33489903807640076, 0.3281609117984772]}}
{"id": "WebQTest-905", "question": "what years did joe montana win super bowl", "prediction": [["sports.sports_championship_event.champion"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what years did joe montana win super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.006240069400519133, -0.028588173910975456, -0.047498464584350586], "norm_scores": [0.3404277563095093, 0.332904189825058, 0.32666805386543274]}}
{"id": "WebQTest-908", "question": "what awards gary paulsen won", "prediction": [["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what awards gary paulsen won [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>"], "scores": [-0.020435545593500137, -0.02446221560239792, -0.02502475120127201], "norm_scores": [0.33429133892059326, 0.3329479694366455, 0.3327607214450836]}}
{"id": "WebQTest-910", "question": "where did alexander graham bell die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people", "people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did alexander graham bell die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>"], "scores": [-0.00039798233774490654, -0.08970138430595398, -0.09284032881259918], "norm_scores": [0.3538232445716858, 0.32359543442726135, 0.32258129119873047]}}
{"id": "WebQTest-911", "question": "what year did the golden state warriors win their first nba championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the golden state warriors win their first nba championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.010762114077806473, -0.0112000722438097, -0.10148417204618454], "norm_scores": [0.34330829977989197, 0.34315797686576843, 0.3135336935520172]}}
{"id": "WebQTest-912", "question": "what currency should i take to turkey", "prediction": [["finance.currency.countries_formerly_used"], ["finance.currency.countries_used"], ["location.country.currency_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should i take to turkey [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.016614215448498726, -0.017537159845232964, -0.04108968749642372], "norm_scores": [0.33614581823349, 0.335835725069046, 0.3280183970928192]}}
{"id": "WebQTest-914", "question": "what countries are in the uk yahoo", "prediction": [["base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are in the uk yahoo [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.020610006526112556, -0.02710612863302231, -0.027352550998330116], "norm_scores": [0.3348059356212616, 0.3326380252838135, 0.3325560688972473]}}
{"id": "WebQTest-915", "question": "what superbowl did the baltimore ravens win", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what superbowl did the baltimore ravens win [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.011703412979841232, -0.013649635948240757, -0.04444793611764908], "norm_scores": [0.33717241883277893, 0.3365168571472168, 0.3263106942176819]}}
{"id": "WebQTest-916", "question": "what is the language of russia s spoken", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the language of russia s spoken [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.018580129370093346, -0.01943911984562874, -0.021465634927153587], "norm_scores": [0.333749383687973, 0.33346283435821533, 0.33278775215148926]}}
{"id": "WebQTest-918", "question": "who is the king of cambodia now", "prediction": [["royalty.monarch.kingdom"], ["people.person.nationality"], ["royalty.kingdom.rulers"]], "ground_paths": [["royalty.kingdom.rulers"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the king of cambodia now [/INST]", "raw_output": {"paths": ["<PATH> royalty.monarch.kingdom </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> royalty.kingdom.rulers </PATH>"], "scores": [-0.015865027904510498, -0.016866568475961685, -0.038108568638563156], "norm_scores": [0.3359085023403168, 0.3355722427368164, 0.32851919531822205]}}
{"id": "WebQTest-919", "question": "who are serena williams parents", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.profession"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are serena williams parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.008618788793683052, -0.02155156247317791, -0.15612593293190002], "norm_scores": [0.3508763611316681, 0.3463677763938904, 0.30275586247444153]}}
{"id": "WebQTest-920", "question": "what schools did sir isaac newton go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what schools did sir isaac newton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.010833736509084702, -0.022039102390408516, -0.029147924855351448], "norm_scores": [0.33661988377571106, 0.3328689932823181, 0.33051109313964844]}}
{"id": "WebQTest-921", "question": "what was the cause of death for huell howser", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for huell howser [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0010478443000465631, -0.04985545948147774, -0.09285789728164673], "norm_scores": [0.34908369183540344, 0.3324548602104187, 0.318461537361145]}}
{"id": "WebQTest-922", "question": "what books are written by suzanne collins", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"], ["book.author.book_editions_published"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books are written by suzanne collins [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.013632304966449738, -0.021418727934360504, -0.042174480855464935], "norm_scores": [0.3373699486255646, 0.33475324511528015, 0.3278768062591553]}}
{"id": "WebQTest-923", "question": "what countries does england border", "prediction": [["location.location.events", "time.event.locations"], ["location.location.events", "location.location.events"], ["time.event.locations", "location.location.events"]], "ground_paths": [["cricket.cricket_tournament_event.host", "location.location.events"], ["biology.breed_origin.breeds_originating_here", "biology.animal_breed.place_of_origin"], ["film.film.featured_film_locations", "film.film_location.featured_in_films"], ["sports.sports_team_location.teams", "cricket.cricket_team.country"], ["time.event.locations", "time.event.locations"], ["periodicals.newspaper_circulation_area.newspapers", "book.newspaper.circulation_areas"], ["base.biblioness.bibs_location.country", "base.aareas.schema.administrative_area.administrative_children"], ["book.newspaper.circulation_areas", "periodicals.newspaper_circulation_area.newspapers"], ["government.governmental_jurisdiction.government_bodies", "government.governmental_jurisdiction.government_bodies"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.partially_containedby", "location.location.partially_contains"], ["base.charities.charity.geographic_scope", "organization.organization.geographic_scope"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["finance.currency.countries_used", "location.country.currency_used"], ["location.location.events", "location.location.events"], ["biology.breed_origin.breeds_originating_here", "biology.breed_origin.breeds_originating_here"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["government.government_agency.jurisdiction", "government.governmental_jurisdiction.agencies"], ["organization.organization.geographic_scope", "common.topic.subject_of"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["cricket.cricket_tournament_event.host", "time.event.locations"], ["law.court_jurisdiction_area.courts", "law.court_jurisdiction_area.courts"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["location.country.languages_spoken", "location.country.official_language"], ["time.event.locations", "location.location.events"], ["people.person.nationality", "people.person.nationality"], ["location.country.internet_tld", "location.country.internet_tld"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["location.location.partially_contains", "location.location.partially_containedby"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.form_of_government", "location.country.form_of_government"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["law.court_jurisdiction_area.courts", "law.court.jurisdiction"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["periodicals.newspaper_circulation_area.newspapers", "periodicals.newspaper_circulation_area.newspapers"], ["government.government_agency.jurisdiction", "government.government_agency.jurisdiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does england border [/INST]", "raw_output": {"paths": ["<PATH> location.location.events <SEP> time.event.locations </PATH>", "<PATH> location.location.events <SEP> location.location.events </PATH>", "<PATH> time.event.locations <SEP> location.location.events </PATH>"], "scores": [-0.04209855571389198, -0.0428776741027832, -0.04861169308423996], "norm_scores": [0.33414316177368164, 0.3338829278945923, 0.3319739103317261]}}
{"id": "WebQTest-924", "question": "what team did messi play for in 2010", "prediction": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did messi play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.024900101125240326, -0.025018878281116486, -0.03863810747861862], "norm_scores": [0.3348695635795593, 0.33482977747917175, 0.33030059933662415]}}
{"id": "WebQTest-925", "question": "what time zone is greensburg indiana", "prediction": [["location.location.time_zones"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is greensburg indiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-2.245593350380659e-05, -0.1506171077489853, -0.16255605220794678], "norm_scores": [0.3689786493778229, 0.31739410758018494, 0.3136272728443146]}}
{"id": "WebQTest-927", "question": "who plays voldemort", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["cvg.game_performance.character", "cvg.game_performance.voice_actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays voldemort [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.02064402773976326, -0.02248893678188324, -0.02490987814962864], "norm_scores": [0.3340124785900116, 0.3333968222141266, 0.3325906991958618]}}
{"id": "WebQTest-928", "question": "what colleges did harper lee attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did harper lee attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.014037197455763817, -0.02240763232111931, -0.0303602684289217], "norm_scores": [0.3360809087753296, 0.3332795202732086, 0.3306396007537842]}}
{"id": "WebQTest-929", "question": "who founded the kkk", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["base.activism.activism_issue.activists"]], "ground_paths": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded the kkk [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>"], "scores": [-0.010173703543841839, -0.0179123617708683, -0.08683817833662033], "norm_scores": [0.34264272451400757, 0.3400013744831085, 0.3173559010028839]}}
{"id": "WebQTest-931", "question": "what jobs did john adams have before he was president", "prediction": [["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["people.person.profession"]], "ground_paths": [["common.topic.image", "common.topic.image"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["tv.tv_program.country_of_origin", "government.government_office_or_title.jurisdiction"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jobs did john adams have before he was president [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.021782854571938515, -0.023477941751480103, -0.0245339497923851], "norm_scores": [0.33382752537727356, 0.3332621157169342, 0.33291035890579224]}}
{"id": "WebQTest-932", "question": "what year did super mario bros 2 come out", "prediction": [["film.film.sequel"], ["film.film.country"], ["common.topic.notable_types"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did super mario bros 2 come out [/INST]", "raw_output": {"paths": ["<PATH> film.film.sequel </PATH>", "<PATH> film.film.country </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.021893536671996117, -0.036970868706703186, -0.04706689715385437], "norm_scores": [0.3378176987171173, 0.3327625095844269, 0.3294198513031006]}}
{"id": "WebQTest-933", "question": "what political party did andrew johnson belong to", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.us_president.vice_president", "organization.organization.founders"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["government.politician.party", "government.political_party_tenure.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["government.us_president.vice_president", "organization.organization.founders"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party did andrew johnson belong to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.us_president.vice_president <SEP> organization.organization.founders </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.016003791242837906, -0.026072431355714798, -0.026278343051671982], "norm_scores": [0.33559751510620117, 0.3322354555130005, 0.33216702938079834]}}
{"id": "WebQTest-935", "question": "where was osama bin laden killed", "prediction": [["base.culturalevent.event.entity_involved", "time.event.locations"], ["people.deceased_person.place_of_death"], ["base.culturalevent.event.entity_involved", "location.location.events"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was osama bin laden killed [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> location.location.events </PATH>"], "scores": [-0.01692163571715355, -0.02441472001373768, -0.036417145282030106], "norm_scores": [0.336334764957428, 0.3338240087032318, 0.3298412263393402]}}
{"id": "WebQTest-936", "question": "where were greek temples built", "prediction": [["location.location.containedby"], ["location.location.contains"], ["architecture.building.building_function"]], "ground_paths": [["architecture.building.building_function", "travel.tourist_attraction.near_travel_destination"], ["architecture.building_function.buildings", "location.location.containedby"], ["common.image.appears_in_topic_gallery", "common.image.appears_in_topic_gallery"], ["architecture.building.building_function", "location.location.containedby"], ["architecture.building_function.buildings", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were greek temples built [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> architecture.building.building_function </PATH>"], "scores": [-0.006199149880558252, -0.04724402725696564, -0.07759498804807663], "norm_scores": [0.3459155261516571, 0.3320049047470093, 0.3220796287059784]}}
{"id": "WebQTest-937", "question": "where are people who speak arabic from", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are people who speak arabic from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016871819272637367, -0.022277986630797386, -0.035755157470703125], "norm_scores": [0.33603253960609436, 0.3342207968235016, 0.32974666357040405]}}
{"id": "WebQTest-938", "question": "who was queen elizabeth ii mom", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen elizabeth ii mom [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.005625204648822546, -0.028239047154784203, -0.1278056502342224], "norm_scores": [0.34932929277420044, 0.3415182828903198, 0.3091524839401245]}}
{"id": "WebQTest-939", "question": "what type of art did pablo picasso paint", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did pablo picasso paint [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.0016184577252715826, -0.0414029061794281, -0.07173383235931396], "norm_scores": [0.34562814235687256, 0.3321474492549896, 0.32222434878349304]}}
{"id": "WebQTest-941", "question": "what is ella fitzgerald parents name", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is ella fitzgerald parents name [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.011376334354281425, -0.015964936465024948, -0.13394993543624878], "norm_scores": [0.3472146689891815, 0.345625102519989, 0.30716025829315186]}}
{"id": "WebQTest-942", "question": "who is anne hathaway parents", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is anne hathaway parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.013961908407509327, -0.014247254468500614, -0.15203312039375305], "norm_scores": [0.34834086894989014, 0.3482414782047272, 0.3034176826477051]}}
{"id": "WebQTest-943", "question": "who does don draper gets engaged to", "prediction": [["fictional_universe.fictional_character.character_created_by", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.married_to", "fictional_universe.marriage_of_fictional_characters.spouses"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character_creator.fictional_characters_created", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does don draper gets engaged to [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by <SEP> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.fictional_character.married_to </PATH>"], "scores": [-0.022084491327404976, -0.02535964734852314, -0.034038301557302475], "norm_scores": [0.33502548933029175, 0.33393001556396484, 0.3310444951057434]}}
{"id": "WebQTest-944", "question": "what was van halen first album", "prediction": [["music.artist.album"], ["music.album.artist"], ["music.recording.artist"]], "ground_paths": [["music.album.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was van halen first album [/INST]", "raw_output": {"paths": ["<PATH> music.artist.album </PATH>", "<PATH> music.album.artist </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.021159512922167778, -0.025495314970612526, -0.030721182003617287], "norm_scores": [0.334878534078598, 0.3334296941757202, 0.33169177174568176]}}
{"id": "WebQTest-945", "question": "what political system did spain have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political system did spain have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.004125309642404318, -0.027617499232292175, -0.1445695459842682], "norm_scores": [0.351400762796402, 0.343241810798645, 0.3053574562072754]}}
{"id": "WebQTest-948", "question": "what are the supreme court cases", "prediction": [["law.court.jurisdiction", "government.governmental_jurisdiction.government_bodies"], ["law.court.jurisdiction", "book.written_work.subjects"], ["law.court.jurisdiction", "government.governmental_body.jurisdiction"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the supreme court cases [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> law.court.jurisdiction <SEP> book.written_work.subjects </PATH>", "<PATH> law.court.jurisdiction <SEP> government.governmental_body.jurisdiction </PATH>"], "scores": [-0.022001158446073532, -0.03254835307598114, -0.03775380179286003], "norm_scores": [0.33626118302345276, 0.3327331840991974, 0.33100566267967224]}}
{"id": "WebQTest-949", "question": "what is penn state 's main campus", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"]], "ground_paths": [["location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is penn state 's main campus [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>"], "scores": [-0.003058545757085085, -0.0494876429438591, -0.05214182659983635], "norm_scores": [0.3440287411212921, 0.3284209072589874, 0.32755038142204285]}}
{"id": "WebQTest-952", "question": "what country does alaska belong to", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does alaska belong to [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.0158531554043293, -0.034108858555555344, -0.03875935822725296], "norm_scores": [0.3379218578338623, 0.33180883526802063, 0.33026930689811707]}}
{"id": "WebQTest-953", "question": "where is kansas city", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.hud_county_place.county"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is kansas city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.005861359182745218, -0.05131745710968971, -0.056638602167367935], "norm_scores": [0.3441094756126404, 0.32881778478622437, 0.32707273960113525]}}
{"id": "WebQTest-954", "question": "what was pink floyd 's first album", "prediction": [["music.recording.artist"], ["music.artist.album"], ["music.artist.genre", "music.album.genre"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was pink floyd 's first album [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.artist.genre <SEP> music.album.genre </PATH>"], "scores": [-0.027157587930560112, -0.03332725539803505, -0.043919067829847336], "norm_scores": [0.3358829617500305, 0.3338170349597931, 0.3303000032901764]}}
{"id": "WebQTest-955", "question": "who did granville t. woods married", "prediction": [["people.person.children", "people.person.children"], ["people.person.spouse_s", "people.person.spouse_s"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did granville t. woods married [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.spouse_s <SEP> people.person.spouse_s </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.031542666256427765, -0.03678572550415993, -0.03879283368587494], "norm_scores": [0.3347228169441223, 0.3329724073410034, 0.33230477571487427]}}
{"id": "WebQTest-957", "question": "what language did the sioux indians speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did the sioux indians speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0007184454589150846, -0.07957448810338974, -0.0805438682436943], "norm_scores": [0.35119134187698364, 0.32456153631210327, 0.3242470622062683]}}
{"id": "WebQTest-958", "question": "what are some famous people from el salvador", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some famous people from el salvador [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0027906836476176977, -0.0428219735622406, -0.07579082995653152], "norm_scores": [0.34597766399383545, 0.3324012756347656, 0.3216210603713989]}}
{"id": "WebQTest-959", "question": "who did brett gardner play for in 2008", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did brett gardner play for in 2008 [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.00956597737967968, -0.01674872636795044, -0.031832318753004074], "norm_scores": [0.33660706877708435, 0.334197998046875, 0.32919490337371826]}}
{"id": "WebQTest-960", "question": "who plays dwight in the office", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_series_season.episodes", "tv.tv_director.episodes_directed"], ["tv.tv_series_season.regular_cast", "tv.regular_tv_appearance.actor"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["award.award_winning_work.awards_won", "award.award_honor.award_winner"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays dwight in the office [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.015706151723861694, -0.037288594990968704, -0.03977004811167717], "norm_scores": [0.33842411637306213, 0.3311983644962311, 0.3303775191307068]}}
{"id": "WebQTest-962", "question": "what county is san diego ca", "prediction": [["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["location.location.containedby"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san diego ca [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.016362950205802917, -0.024078000336885452, -0.029331205412745476], "norm_scores": [0.33563467860221863, 0.33305519819259644, 0.3313101828098297]}}
{"id": "WebQTest-963", "question": "who is the miami dolphins quarterback", "prediction": [["american_football.football_player.position_s"], ["american_football.game_rushing_statistics.team", "american_football.game_rushing_statistics.player"], ["american_football.game_passing_statistics.team", "american_football.game_passing_statistics.player"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the miami dolphins quarterback [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.game_rushing_statistics.team <SEP> american_football.game_rushing_statistics.player </PATH>", "<PATH> american_football.game_passing_statistics.team <SEP> american_football.game_passing_statistics.player </PATH>"], "scores": [-0.008104635402560234, -0.03665478155016899, -0.038300562649965286], "norm_scores": [0.3398922383785248, 0.33032548427581787, 0.32978230714797974]}}
{"id": "WebQTest-965", "question": "where did anne frank go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["book.written_work.subjects", "location.location.containedby"]], "ground_paths": [["book.book_subject.works", "book.book_subject.works"], ["book.book_subject.works", "book.written_work.subjects"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did anne frank go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>"], "scores": [-0.02034914679825306, -0.02433982491493225, -0.029169801622629166], "norm_scores": [0.3347576856613159, 0.33342444896698, 0.3318178951740265]}}
{"id": "WebQTest-966", "question": "what basketball player is married to monica", "prediction": [["people.person.nationality", "people.person.nationality"], ["sports.sports_team_coach.sports_coached", "people.person.nationality", "people.person.nationality"], ["sports.sports_team_coach.sports_coached", "people.person.gender", "people.person.gender"]], "ground_paths": [["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what basketball player is married to monica [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.03306610882282257, -0.04120719060301781, -0.04517742618918419], "norm_scores": [0.3355869650840759, 0.33286601305007935, 0.3315470516681671]}}
{"id": "WebQTest-967", "question": "what did hitler use to kill himself", "prediction": [["people.deceased_person.cause_of_death"], ["base.activism.activist.area_of_activism", "base.activism.activist.area_of_activism"], ["base.activism.activist.area_of_activism", "base.activism.activism_issue.activists"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did hitler use to kill himself [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> base.activism.activism_issue.activists </PATH>"], "scores": [-0.008505221456289291, -0.028967004269361496, -0.0470099002122879], "norm_scores": [0.3399079144001007, 0.33302345871925354, 0.32706865668296814]}}
{"id": "WebQTest-968", "question": "what do people speak in germany", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in germany [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013263599947094917, -0.019686361774802208, -0.03689185529947281], "norm_scores": [0.3366723954677582, 0.3345169425010681, 0.3288106620311737]}}
{"id": "WebQTest-969", "question": "when is the last time the denver broncos won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the denver broncos won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.014904814772307873, -0.016221923753619194, -0.024419954046607018], "norm_scores": [0.3345361053943634, 0.3340958058834076, 0.3313680589199066]}}
{"id": "WebQTest-970", "question": "where was caesar when he was stabbed", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was caesar when he was stabbed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.0016858662711456418, -0.06851411610841751, -0.06994791328907013], "norm_scores": [0.348508358001709, 0.3259793221950531, 0.32551226019859314]}}
{"id": "WebQTest-971", "question": "what style of music did bessie smith perform", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.written_work.author"]], "ground_paths": [["book.book_subject.works"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what style of music did bessie smith perform [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.0005431804456748068, -0.07346376031637192, -0.10560715943574905], "norm_scores": [0.3533641993999481, 0.3285137116909027, 0.31812208890914917]}}
{"id": "WebQTest-972", "question": "what team did jackie robinson", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did jackie robinson [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014096581377089024, -0.017899397760629654, -0.029407186433672905], "norm_scores": [0.3354567289352417, 0.3341834545135498, 0.3303597867488861]}}
{"id": "WebQTest-973", "question": "what time zone is chicago in right now", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is chicago in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-8.52717312227469e-06, -0.18577241897583008, -0.1898236870765686], "norm_scores": [0.3762819468975067, 0.3124907314777374, 0.31122732162475586]}}
{"id": "WebQTest-975", "question": "who was the vice president under ronald reagan", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the vice president under ronald reagan [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.0064021749421954155, -0.020086418837308884, -0.07930035144090652], "norm_scores": [0.34292325377464294, 0.33826255798339844, 0.318814218044281]}}
{"id": "WebQTest-976", "question": "where do the chicago white sox play home games", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the chicago white sox play home games [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.0005504053551703691, -0.05989136919379234, -0.11064784973859787], "norm_scores": [0.35234442353248596, 0.33204421401023865, 0.3156113922595978]}}
{"id": "WebQTest-977", "question": "what part did jeff conaway play in grease", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["theater.theater_character.plays_appears_in"], ["film.performance.actor", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part did jeff conaway play in grease [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.02617870829999447, -0.03180158510804176, -0.03546552732586861], "norm_scores": [0.3349916338920593, 0.33311331272125244, 0.33189505338668823]}}
{"id": "WebQTest-978", "question": "what book did benjamin franklin published", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.book_editions_published"]], "ground_paths": [["book.written_work.author"], ["book.author.book_editions_published"], ["people.person.profession", "common.topic.subject_of"], ["book.book_edition.author_editor"], ["book.book_edition.author_editor", "book.book.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did benjamin franklin published [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.book_editions_published </PATH>"], "scores": [-0.011678109876811504, -0.029033109545707703, -0.033709898591041565], "norm_scores": [0.3377233147621155, 0.33191266655921936, 0.33036401867866516]}}
{"id": "WebQTest-979", "question": "what currency to take to barbados", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency to take to barbados [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.010137554258108139, -0.016657629981637, -0.1255475878715515], "norm_scores": [0.34668031334877014, 0.3444272577762604, 0.3088923990726471]}}
{"id": "WebQTest-980", "question": "where did they get the name stanley cup", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["sports.sports_championship_event.championship"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did they get the name stanley cup [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.013217992149293423, -0.022817421704530716, -0.05638853460550308], "norm_scores": [0.33919063210487366, 0.33595019578933716, 0.3248591721057892]}}
{"id": "WebQTest-982", "question": "what is the zip code of syracuse ny", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code of syracuse ny [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00030350894667208195, -0.0778832882642746, -0.13376165926456451], "norm_scores": [0.3570896089076996, 0.33043402433395386, 0.3124763071537018]}}
{"id": "WebQTest-983", "question": "where did tennessee williams go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did tennessee williams go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.013996358960866928, -0.020403577014803886, -0.021434271708130836], "norm_scores": [0.33487340807914734, 0.3327346742153168, 0.3323919177055359]}}
{"id": "WebQTest-984", "question": "what is the name of airport in new york", "prediction": [["aviation.airport.serves"], ["location.location.nearby_airports"], ["location.location.containedby"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of airport in new york [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01010172814130783, -0.02666393853724003, -0.030350062996149063], "norm_scores": [0.3374354839324951, 0.33189284801483154, 0.3306716978549957]}}
{"id": "WebQTest-985", "question": "who was queen victoria 's husband", "prediction": [["people.marriage.spouse", "people.marriage.spouse"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["royalty.kingdom.rulers", "people.person.nationality"], ["base.crime.crime_victim.crime", "base.crime.crime_victim.crime"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen victoria 's husband [/INST]", "raw_output": {"paths": ["<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03668380528688431, -0.037576936185359955, -0.040428802371025085], "norm_scores": [0.33384862542152405, 0.3335506021976471, 0.3326007127761841]}}
{"id": "WebQTest-986", "question": "where did benjamin franklin work", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employer.employees"], ["people.person.profession"]], "ground_paths": [["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "government.government_agency.jurisdiction"], ["people.person.children"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did benjamin franklin work [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.016686787828803062, -0.028450876474380493, -0.04250427708029747], "norm_scores": [0.33751654624938965, 0.3335692286491394, 0.32891422510147095]}}
{"id": "WebQTest-987", "question": "what does barbara bush do for work", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.person.children"]], "ground_paths": [["business.employment_tenure.person", "business.job_title.people_with_this_title"], ["government.politician.government_positions_held", "government.government_office_category.officeholders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does barbara bush do for work [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.children </PATH>"], "scores": [-0.0019768544007092714, -0.06800714880228043, -0.07509718835353851], "norm_scores": [0.3489680886268616, 0.3266699016094208, 0.32436200976371765]}}
{"id": "WebQTest-988", "question": "who is jacob latimore", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["common.topic.notable_types", "common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is jacob latimore [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>"], "scores": [-0.0004376614233478904, -0.07531610131263733, -0.14339697360992432], "norm_scores": [0.3578270971775055, 0.3320121467113495, 0.31016072630882263]}}
{"id": "WebQTest-989", "question": "what happened to dunkirk during ww2", "prediction": [["time.event.locations"], ["military.military_conflict.casualties", "military.casualties.event"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened to dunkirk during ww2 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.event </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.030439376831054688, -0.03141356632113457, -0.03831591084599495], "norm_scores": [0.3343161642551422, 0.3339906334877014, 0.33169323205947876]}}
{"id": "WebQTest-990", "question": "what was scottie pippen good at", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.player"]], "ground_paths": [["sports.pro_sports_played.athlete", "sports.pro_sports_played.sport"], ["common.topic.notable_types", "type.type.domain"], ["basketball.basketball_player.position_s", "sports.sport.positions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was scottie pippen good at [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.player </PATH>"], "scores": [-0.01697932928800583, -0.029937492683529854, -0.032811619341373444], "norm_scores": [0.3365396559238434, 0.3322068452835083, 0.33125343918800354]}}
{"id": "WebQTest-991", "question": "who played donna on west wing", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played donna on west wing [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.02243342623114586, -0.037611883133649826, -0.04249114543199539], "norm_scores": [0.3372592031955719, 0.3321788012981415, 0.33056193590164185]}}
{"id": "WebQTest-993", "question": "who played meg in season 1 of family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played meg in season 1 of family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.02169676683843136, -0.030854932963848114, -0.03756793960928917], "norm_scores": [0.3361189067363739, 0.33305472135543823, 0.33082640171051025]}}
{"id": "WebQTest-994", "question": "when was the last time the dallas cowboys won the superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the dallas cowboys won the superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.006116403732448816, -0.0253001656383276, -0.03822657838463783], "norm_scores": [0.33905214071273804, 0.33260980248451233, 0.32833802700042725]}}
{"id": "WebQTest-996", "question": "where do ireland play rugby union", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_team_location.teams"], ["sports.sports_facility.teams"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do ireland play rugby union [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> sports.sports_facility.teams </PATH>"], "scores": [-0.005030920263379812, -0.04235580191016197, -0.05007833242416382], "norm_scores": [0.3425460755825043, 0.329996258020401, 0.3274576663970947]}}
{"id": "WebQTest-997", "question": "who killed lee harvey oswald shot", "prediction": [["people.deceased_person.cause_of_death"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.person.gender", "people.person.gender"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who killed lee harvey oswald shot [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.010101260617375374, -0.028058061376214027, -0.044111110270023346], "norm_scores": [0.33912497758865356, 0.3330897092819214, 0.32778531312942505]}}
{"id": "WebQTest-998", "question": "when was the last time knicks won the championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time knicks won the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.00820176675915718, -0.016047170385718346, -0.11458190530538559], "norm_scores": [0.3458689749240875, 0.3431661128997803, 0.3109648525714874]}}
{"id": "WebQTest-999", "question": "when did celtics win the championship", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did celtics win the championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.011780815199017525, -0.012403294444084167, -0.14908304810523987], "norm_scores": [0.3483004570007324, 0.3480837047100067, 0.30361583828926086]}}
{"id": "WebQTest-1000", "question": "what countries included in oceania", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries included in oceania [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.006453877780586481, -0.03733111172914505, -0.056309159845113754], "norm_scores": [0.3423529863357544, 0.3319436013698578, 0.32570335268974304]}}
{"id": "WebQTest-1001", "question": "what country is nicki minaj from", "prediction": [["people.person.nationality"], ["music.artist.origin"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country is nicki minaj from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.006133036222308874, -0.04063180834054947, -0.052686650305986404], "norm_scores": [0.3423950672149658, 0.33078429102897644, 0.3268206715583801]}}
{"id": "WebQTest-1002", "question": "who played harry potter dumbledore actor", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played harry potter dumbledore actor [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.016999058425426483, -0.02305716834962368, -0.024206459522247314], "norm_scores": [0.3348088562488556, 0.3327866792678833, 0.3324044346809387]}}
{"id": "WebQTest-1003", "question": "who was the prime minister of pakistan in 1999", "prediction": [["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the prime minister of pakistan in 1999 [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.01010072324424982, -0.0153283029794693, -0.021474575623869896], "norm_scores": [0.3351794183254242, 0.33343183994293213, 0.3313887417316437]}}
{"id": "WebQTest-1004", "question": "what movies did christopher plummer play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["music.recording.artist", "music.recording.releases"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["film.actor.dubbing_performances", "film.film.dubbing_performances"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["film.performance.actor"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["music.artist.album", "film.film.soundtrack"], ["film.producer.films_executive_produced"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did christopher plummer play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.015323981642723083, -0.034876592457294464, -0.03522780165076256], "norm_scores": [0.3377316892147064, 0.331192284822464, 0.3310759961605072]}}
{"id": "WebQTest-1005", "question": "what is capital city of morocco", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is capital city of morocco [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.0061167324893176556, -0.03507406264543533, -0.04184103012084961], "norm_scores": [0.3405572175979614, 0.3308369815349579, 0.3286058008670807]}}
{"id": "WebQTest-1006", "question": "what movies is omar epps in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies is omar epps in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.014205357991158962, -0.030985329300165176, -0.03718527778983116], "norm_scores": [0.33776456117630005, 0.332144170999527, 0.330091267824173]}}
{"id": "WebQTest-1007", "question": "what district does nancy pelosi represent", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["government.government_position_held.office_holder", "government.political_district.representatives"]], "ground_paths": [["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"], ["base.government2.elected_government_positions_held.officeholder", "base.government2.elected_government_positions_held.district_represented"], ["government.politician.government_positions_held", "government.political_district.representatives"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what district does nancy pelosi represent [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.district_represented </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.political_district.representatives </PATH>"], "scores": [-0.022248582914471626, -0.02458730712532997, -0.029691295698285103], "norm_scores": [0.3344203233718872, 0.33363911509513855, 0.33194056153297424]}}
{"id": "WebQTest-1008", "question": "where is walton indiana", "prediction": [["location.location.containedby"], ["location.hud_county_place.county"], ["location.location.contains"]], "ground_paths": [["location.us_county.hud_county_place"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is walton indiana [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0052729081362485886, -0.04620885103940964, -0.05236748233437538], "norm_scores": [0.34318411350250244, 0.32941922545433044, 0.3273966908454895]}}
{"id": "WebQTest-1009", "question": "where did george herbert walker bush go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["base.yalebase.person.residential_college", "education.educational_institution.parent_institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george herbert walker bush go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015938637778162956, -0.016911668702960014, -0.019359556958079338], "norm_scores": [0.333821564912796, 0.33349692821502686, 0.3326815366744995]}}
{"id": "WebQTest-1010", "question": "what did bruce jenner win gold medal for", "prediction": [["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.event"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_event_competition.medalists"]], "ground_paths": [["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bruce jenner win gold medal for [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_event_competition.medalists </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_event_competition.medalists </PATH>"], "scores": [-0.0071867238730192184, -0.019910959526896477, -0.04511624947190285], "norm_scores": [0.33896729350090027, 0.33468151092529297, 0.32635122537612915]}}
{"id": "WebQTest-1011", "question": "where do they speak tibetan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak tibetan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.013473451137542725, -0.03414253145456314, -0.03571495786309242], "norm_scores": [0.3381180465221405, 0.3312011659145355, 0.33068081736564636]}}
{"id": "WebQTest-1012", "question": "what to do in hollywood ca this weekend", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do in hollywood ca this weekend [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.015073596499860287, -0.01531643234193325, -0.028170844539999962], "norm_scores": [0.33481261134147644, 0.3347313106060028, 0.33045607805252075]}}
{"id": "WebQTest-1014", "question": "where did kim kardashian come from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kim kardashian come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.003822998609393835, -0.04001102223992348, -0.04570506513118744], "norm_scores": [0.3420625627040863, 0.32990530133247375, 0.32803210616111755]}}
{"id": "WebQTest-1015", "question": "what town did justin bieber grew up in", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["music.artist.origin"], ["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what town did justin bieber grew up in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.013199969194829464, -0.026457540690898895, -0.03284212574362755], "norm_scores": [0.3369976878166199, 0.33255940675735474, 0.3304429054260254]}}
{"id": "WebQTest-1017", "question": "who plays charlie in the santa clause movies", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film.country", "people.person.nationality"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["film.performance.film", "film.actor.film"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays charlie in the santa clause movies [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.027189217507839203, -0.027594711631536484, -0.027597006410360336], "norm_scores": [0.3334237039089203, 0.3332885205745697, 0.33328777551651]}}
{"id": "WebQTest-1020", "question": "what highschool did r. kelly attend", "prediction": [["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what highschool did r. kelly attend [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.017121832817792892, -0.018185889348387718, -0.020173197612166405], "norm_scores": [0.33379065990448, 0.33343565464019775, 0.33277371525764465]}}
{"id": "WebQTest-1021", "question": "what happened at benghazi", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened at benghazi [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0052803740836679935, -0.043609824031591415, -0.05014687404036522], "norm_scores": [0.342639297246933, 0.32975465059280396, 0.32760605216026306]}}
{"id": "WebQTest-1022", "question": "what country is nike based in", "prediction": [["organization.organization.headquarters", "location.mailing_address.country"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"]], "ground_paths": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["organization.organization.founders", "people.person.nationality"], ["organization.organization.phone_number", "common.phone_number.service_location"], ["organization.organization.headquarters", "location.mailing_address.country"], ["film.film.production_companies", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country is nike based in [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>"], "scores": [-0.0051226201467216015, -0.0388718917965889, -0.04413712024688721], "norm_scores": [0.34146586060523987, 0.33013391494750977, 0.32840025424957275]}}
{"id": "WebQTest-1024", "question": "what kind of money do i bring to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money do i bring to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.006706521846354008, -0.02786475419998169, -0.060170870274305344], "norm_scores": [0.34164631366729736, 0.33449363708496094, 0.3238601088523865]}}
{"id": "WebQTest-1025", "question": "who did steve spurrier play pro football for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["common.topic.notable_types"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did steve spurrier play pro football for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.008481799624860287, -0.021771147847175598, -0.031476300209760666], "norm_scores": [0.33737435936927795, 0.33292052149772644, 0.3297051191329956]}}
{"id": "WebQTest-1027", "question": "where do the blackhawks play", "prediction": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"], ["sports.sports_championship_event.champion"]], "ground_paths": [["sports.sports_team.arena_stadium"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the blackhawks play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>"], "scores": [-0.002666937652975321, -0.03495489060878754, -0.10059804469347], "norm_scores": [0.3478334844112396, 0.3367820084095001, 0.31538456678390503]}}
{"id": "WebQTest-1028", "question": "who plays john connor", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays john connor [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.021534210070967674, -0.021876584738492966, -0.026832686737179756], "norm_scores": [0.333959698677063, 0.3338453769683838, 0.33219489455223083]}}
{"id": "WebQTest-1029", "question": "what is the main language spoken in mexico", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the main language spoken in mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.016896914690732956, -0.026469672098755836, -0.029077941551804543], "norm_scores": [0.3357545733451843, 0.3325558304786682, 0.3316895365715027]}}
{"id": "WebQTest-1030", "question": "what are italians classified as", "prediction": [["people.ethnicity.included_in_group"], ["people.ethnicity.includes_groups"], ["people.person.ethnicity"]], "ground_paths": [["people.ethnicity.includes_groups"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are italians classified as [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.included_in_group </PATH>", "<PATH> people.ethnicity.includes_groups </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.01014693919569254, -0.020003529265522957, -0.06394673883914948], "norm_scores": [0.3403891623020172, 0.33705058693885803, 0.32256022095680237]}}
{"id": "WebQTest-1031", "question": "what has angelina jolie accomplished", "prediction": [["people.person.profession"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has angelina jolie accomplished [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>"], "scores": [-0.015261512249708176, -0.028098151087760925, -0.030228422954678535], "norm_scores": [0.336429625749588, 0.3321385681629181, 0.3314317762851715]}}
{"id": "WebQTest-1032", "question": "who played captain kirk in star trek movie", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played captain kirk in star trek movie [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.024189718067646027, -0.025692682713270187, -0.027442049235105515], "norm_scores": [0.3338618278503418, 0.3333604335784912, 0.332777738571167]}}
{"id": "WebQTest-1033", "question": "who played dana scully in x files", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["film.performance.character", "film.performance.actor"], ["people.person.gender", "people.person.gender"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["fictional_universe.fictional_character.quotations", "people.person.quotations"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played dana scully in x files [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.024928051978349686, -0.030990054830908775, -0.03278589993715286], "norm_scores": [0.3348816931247711, 0.33285775780677795, 0.33226051926612854]}}
{"id": "WebQTest-1035", "question": "what was f. scott fitzgerald", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was f. scott fitzgerald [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.000556644459720701, -0.06872699409723282, -0.09633945673704147], "norm_scores": [0.3517704904079437, 0.3285892605781555, 0.31964024901390076]}}
{"id": "WebQTest-1036", "question": "what is the dominant language spoken in jamaica", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the dominant language spoken in jamaica [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.015113151632249355, -0.021991083398461342, -0.0238290224224329], "norm_scores": [0.33506810665130615, 0.3327714800834656, 0.33216041326522827]}}
{"id": "WebQTest-1037", "question": "who is the coach for the new york knicks", "prediction": [["basketball.basketball_team.head_coach"], ["basketball.basketball_coach.team"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"]], "ground_paths": [["basketball.basketball_team.head_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the coach for the new york knicks [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_team.head_coach </PATH>", "<PATH> basketball.basketball_coach.team </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>"], "scores": [-0.004144189413636923, -0.030160926282405853, -0.06477893143892288], "norm_scores": [0.3429960012435913, 0.33418744802474976, 0.32281652092933655]}}
{"id": "WebQTest-1038", "question": "who was wesley snipes", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was wesley snipes [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.00037510585389100015, -0.08472061157226562, -0.09903129935264587], "norm_scores": [0.3539612889289856, 0.3253306448459625, 0.32070809602737427]}}
{"id": "WebQTest-1039", "question": "where does sherrod brown live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "ground_paths": [["people.person.places_lived", "people.place_lived.location"], ["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does sherrod brown live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.015536680817604065, -0.017255421727895737, -0.03652931749820709], "norm_scores": [0.3358512222766876, 0.33527445793151855, 0.3288743197917938]}}
{"id": "WebQTest-1040", "question": "what kind of language does colombia speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does colombia speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.006638649385422468, -0.021933339536190033, -0.07125626504421234], "norm_scores": [0.342202365398407, 0.3370083272457123, 0.3207893371582031]}}
{"id": "WebQTest-1041", "question": "what currency does england use now", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does england use now [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.003211798146367073, -0.04363996908068657, -0.059734683483839035], "norm_scores": [0.3441839814186096, 0.3305467665195465, 0.32526928186416626]}}
{"id": "WebQTest-1042", "question": "who was the original voice of meg griffin on family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the original voice of meg griffin on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.013890578411519527, -0.02909156307578087, -0.033840060234069824], "norm_scores": [0.33724966645240784, 0.33216190338134766, 0.33058837056159973]}}
{"id": "WebQTest-1043", "question": "what kind of government is mali", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.continent"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is mali [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.008211950771510601, -0.017752425745129585, -0.13813894987106323], "norm_scores": [0.3485943078994751, 0.3452843427658081, 0.3061214089393616]}}
{"id": "WebQTest-1044", "question": "who did cristiano ronaldo play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cristiano ronaldo play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.019637789577245712, -0.022635458037257195, -0.026602933183312416], "norm_scores": [0.33444079756736755, 0.33343973755836487, 0.3321194648742676]}}
{"id": "WebQTest-1045", "question": "what is the zip code for morgantown wv", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for morgantown wv [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-6.073504118830897e-05, -0.10676662623882294, -0.15238723158836365], "norm_scores": [0.3626476228237152, 0.32594403624534607, 0.3114083707332611]}}
{"id": "WebQTest-1046", "question": "what is jacksonville fl timezone", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jacksonville fl timezone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.67100822890643e-06, -0.18991351127624512, -0.19472149014472961], "norm_scores": [0.37734276056289673, 0.3120770752429962, 0.31058019399642944]}}
{"id": "WebQTest-1047", "question": "who does amy stiller play in dodgeball", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["film.film.starring", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does amy stiller play in dodgeball [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.022764896973967552, -0.023814696818590164, -0.026912111788988113], "norm_scores": [0.3339107632637024, 0.33356043696403503, 0.33252882957458496]}}
{"id": "WebQTest-1048", "question": "who were king henry vii children", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were king henry vii children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.011365414597094059, -0.01659287139773369, -0.14162024855613708], "norm_scores": [0.3481096625328064, 0.34629467129707336, 0.30559560656547546]}}
{"id": "WebQTest-1050", "question": "who does mila kunis play on family guy", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does mila kunis play on family guy [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.character </PATH>"], "scores": [-0.02038668841123581, -0.022714195773005486, -0.02403075620532036], "norm_scores": [0.33399710059165955, 0.3332206606864929, 0.33278223872184753]}}
{"id": "WebQTest-1051", "question": "when did the san francisco earthquake occur", "prediction": [["time.event.locations"], ["event.disaster.areas_affected"], ["time.recurring_event.instances"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the san francisco earthquake occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> event.disaster.areas_affected </PATH>", "<PATH> time.recurring_event.instances </PATH>"], "scores": [-0.002972052898257971, -0.04939231276512146, -0.06923794001340866], "norm_scores": [0.3459581732749939, 0.33026576042175293, 0.3237760365009308]}}
{"id": "WebQTest-1052", "question": "who played damien in the omen 2006", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.film", "film.actor.film"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played damien in the omen 2006 [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.017766278237104416, -0.02297908440232277, -0.036040399223566055], "norm_scores": [0.3359433114528656, 0.33419665694236755, 0.32986000180244446]}}
{"id": "WebQTest-1054", "question": "who played denver in four christmases", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.film", "film.actor.film"]], "ground_paths": [["film.film.language", "people.person.languages"], ["film.performance.film", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played denver in four christmases [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.actor.film </PATH>"], "scores": [-0.01993587240576744, -0.030564436689019203, -0.046168457716703415], "norm_scores": [0.33743470907211304, 0.33386725187301636, 0.3286980390548706]}}
{"id": "WebQTest-1055", "question": "what to do with kids in essex", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with kids in essex [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012782776728272438, -0.016611408442258835, -0.03594473749399185], "norm_scores": [0.3363285958766937, 0.33504337072372437, 0.3286280930042267]}}
{"id": "WebQTest-1056", "question": "who did the cleveland cavaliers draft", "prediction": [["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"], ["sports.professional_sports_team.draft_picks", "sports.drafted_athlete.drafted"]], "ground_paths": [["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["sports.sports_team.roster", "sports.pro_athlete.teams"], ["sports.professional_sports_team.draft_picks", "sports.drafted_athlete.drafted"], ["sports.professional_sports_team.draft_picks", "sports.sports_league_draft_pick.player"], ["sports.sports_league_draft_pick.team", "sports.drafted_athlete.drafted"], ["sports.sports_league_draft_pick.team", "sports.sports_league_draft_pick.player"], ["sports.sports_team_roster.team", "sports.pro_athlete.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the cleveland cavaliers draft [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.team <SEP> sports.drafted_athlete.drafted </PATH>", "<PATH> sports.sports_league_draft_pick.team <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.professional_sports_team.draft_picks <SEP> sports.drafted_athlete.drafted </PATH>"], "scores": [-0.02107013575732708, -0.02387368306517601, -0.024621814489364624], "norm_scores": [0.3340398073196411, 0.3331046402454376, 0.3328555226325989]}}
{"id": "WebQTest-1057", "question": "what concentration camp did anne frank died in", "prediction": [["base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["film.film.story_by", "base.fictionaluniverse.deceased_fictional_character.place_of_death"], ["book.written_work.subjects"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what concentration camp did anne frank died in [/INST]", "raw_output": {"paths": ["<PATH> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> film.film.story_by <SEP> base.fictionaluniverse.deceased_fictional_character.place_of_death </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.0068277036771178246, -0.04120767116546631, -0.04490122199058533], "norm_scores": [0.3414313495159149, 0.32989245653152466, 0.3286762237548828]}}
{"id": "WebQTest-1059", "question": "what language does east timor speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does east timor speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.007453894708305597, -0.029620228335261345, -0.048462849110364914], "norm_scores": [0.34037938714027405, 0.3329174518585205, 0.32670313119888306]}}
{"id": "WebQTest-1060", "question": "what is the currency in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.005543556064367294, -0.026564840227365494, -0.144124373793602], "norm_scores": [0.3509029150009155, 0.3436034917831421, 0.30549362301826477]}}
{"id": "WebQTest-1061", "question": "where does greek language come from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"]], "ground_paths": [["base.rosetta.languoid.child"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does greek language come from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.00861596129834652, -0.02503877691924572, -0.05464664474129677], "norm_scores": [0.34028375148773193, 0.3347409665584564, 0.32497528195381165]}}
{"id": "WebQTest-1062", "question": "what is the name of the currency used in italy", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in italy [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.005717774853110313, -0.02307077683508396, -0.13858485221862793], "norm_scores": [0.34984874725341797, 0.3438301682472229, 0.30632105469703674]}}
{"id": "WebQTest-1063", "question": "where is audrey hepburn from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is audrey hepburn from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.001209220034070313, -0.052118007093667984, -0.10013913363218307], "norm_scores": [0.350119024515152, 0.3327409625053406, 0.31713998317718506]}}
{"id": "WebQTest-1065", "question": "who does darth vaders voice", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does darth vaders voice [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.02191135473549366, -0.022001637145876884, -0.035695429891347885], "norm_scores": [0.3348715007305145, 0.334841251373291, 0.33028727769851685]}}
{"id": "WebQTest-1067", "question": "what school did douglas macarthur go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did douglas macarthur go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.014748778194189072, -0.01759297586977482, -0.03074462153017521], "norm_scores": [0.33542510867118835, 0.3344724774360657, 0.3301023840904236]}}
{"id": "WebQTest-1068", "question": "what famous people are from barbados", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["location.location.contains", "location.location.people_born_here"], ["people.person.nationality"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people are from barbados [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.01306149922311306, -0.025990044698119164, -0.041057661175727844], "norm_scores": [0.33788958191871643, 0.33354926109313965, 0.3285611569881439]}}
{"id": "WebQTest-1069", "question": "what is the name of the currency used in china", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in china [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.0063654473051428795, -0.021492522209882736, -0.14542187750339508], "norm_scores": [0.35024237632751465, 0.34498411417007446, 0.3047735393047333]}}
{"id": "WebQTest-1070", "question": "what are the names of michael jackson 's 3 children", "prediction": [["people.person.parents"], ["people.person.children"], ["music.composition.composer", "people.person.parents"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of michael jackson 's 3 children [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.composition.composer <SEP> people.person.parents </PATH>"], "scores": [-0.008688773028552532, -0.017772251740098, -0.09379077702760696], "norm_scores": [0.34371626377105713, 0.34060826897621155, 0.31567540764808655]}}
{"id": "WebQTest-1072", "question": "who is the current governor of arizona 2010", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["location.country.first_level_divisions", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current governor of arizona 2010 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.02001768909394741, -0.020841749384999275, -0.023101400583982468], "norm_scores": [0.33376753330230713, 0.33349260687828064, 0.3327398896217346]}}
{"id": "WebQTest-1074", "question": "who is lamar odom playing for this year", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is lamar odom playing for this year [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.01643575355410576, -0.020243175327777863, -0.03134046122431755], "norm_scores": [0.33541223406791687, 0.33413761854171753, 0.3304500877857208]}}
{"id": "WebQTest-1075", "question": "where were the great pyramids of giza built", "prediction": [["architecture.building.building_complex"], ["location.location.containedby"], ["architecture.building.building_sites"]], "ground_paths": [["architecture.building.building_complex"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where were the great pyramids of giza built [/INST]", "raw_output": {"paths": ["<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_sites </PATH>"], "scores": [-0.00696221087127924, -0.0239575132727623, -0.09332852810621262], "norm_scores": [0.34477925300598145, 0.3389691114425659, 0.31625160574913025]}}
{"id": "WebQTest-1077", "question": "what language do the russian speak", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the russian speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.017337903380393982, -0.021004734560847282, -0.023226192221045494], "norm_scores": [0.33439573645591736, 0.3331718146800995, 0.33243247866630554]}}
{"id": "WebQTest-1078", "question": "what language do australia people speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do australia people speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.00814983993768692, -0.019353481009602547, -0.07741745561361313], "norm_scores": [0.3422388732433319, 0.3384259343147278, 0.3193351924419403]}}
{"id": "WebQTest-1079", "question": "what country was king tut 's tomb found", "prediction": [["people.person.nationality"], ["people.deceased_person.place_of_death"], ["people.deceased_person.place_of_burial"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country was king tut 's tomb found [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.place_of_burial </PATH>"], "scores": [-0.01714099384844303, -0.029728883877396584, -0.029741469770669937], "norm_scores": [0.33613789081573486, 0.3319331705570221, 0.33192893862724304]}}
{"id": "WebQTest-1080", "question": "where did abraham and his descendants live", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality"]], "ground_paths": [["people.deceased_person.place_of_death"], ["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did abraham and his descendants live [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.019539864733815193, -0.033911652863025665, -0.03631707280874252], "norm_scores": [0.3368031084537506, 0.33199724555015564, 0.33119964599609375]}}
{"id": "WebQTest-1081", "question": "where did george washington carver go to collage", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george washington carver go to collage [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.017173711210489273, -0.017905088141560555, -0.018114110454916954], "norm_scores": [0.3335190713405609, 0.3332752585411072, 0.33320561051368713]}}
{"id": "WebQTest-1082", "question": "what are the sports in spain", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the sports in spain [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>"], "scores": [-0.010398510843515396, -0.01542151439934969, -0.13854247331619263], "norm_scores": [0.3478604257106781, 0.3461174964904785, 0.30602210760116577]}}
{"id": "WebQTest-1083", "question": "where president obama was born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["influence.influence_node.influenced_by", "people.person.place_of_birth"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where president obama was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.0032931300811469555, -0.04049631208181381, -0.08177609741687775], "norm_scores": [0.34626060724258423, 0.3336153030395508, 0.3201241195201874]}}
{"id": "WebQTest-1084", "question": "what sports do people play in poland", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["cricket.cricket_team.country"]], "ground_paths": [["sports.sports_team_location.teams"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sports do people play in poland [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> cricket.cricket_team.country </PATH>"], "scores": [-0.010195760987699032, -0.01514152716845274, -0.15921026468276978], "norm_scores": [0.350063681602478, 0.3483366072177887, 0.3015996813774109]}}
{"id": "WebQTest-1085", "question": "who was demi lovato dating", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.artist.genre", "music.artist.genre"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["people.person.profession", "people.person.profession"], ["music.artist.genre", "music.artist.genre"], ["music.composer.compositions", "music.composer.compositions"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["music.composer.compositions", "music.composition.composer"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.recording.featured_artists", "music.album.featured_artists"], ["music.artist.album", "music.composer.compositions"], ["people.person.ethnicity", "people.person.ethnicity"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.nationality", "people.person.nationality"], ["celebrities.celebrity.sexual_relationships", "celebrities.celebrity.sexual_relationships"], ["people.person.languages", "people.person.languages"], ["music.album.artist", "music.album.artist"], ["base.popstra.friendship.participant", "base.popstra.celebrity.friendship"], ["music.recording.artist", "music.composer.compositions"], ["music.artist.label", "music.artist.label"], ["music.recording.artist", "music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was demi lovato dating [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.02832297794520855, -0.030676642432808876, -0.04054287075996399], "norm_scores": [0.33495184779167175, 0.33416441082954407, 0.3308836817741394]}}
{"id": "WebQTest-1087", "question": "what films has scarlett johansson been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["film.performance.actor", "film.performance.character"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.languages", "film.film.language"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["people.person.place_of_birth", "film.film.featured_film_locations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what films has scarlett johansson been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.023501437157392502, -0.023883406072854996, -0.03342261165380478], "norm_scores": [0.3344765603542328, 0.3343488276004791, 0.3311745822429657]}}
{"id": "WebQTest-1088", "question": "who originally voiced meg on family guy", "prediction": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who originally voiced meg on family guy [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.02439359575510025, -0.025562847033143044, -0.025924736633896828], "norm_scores": [0.3336334228515625, 0.33324354887008667, 0.33312299847602844]}}
{"id": "WebQTest-1089", "question": "who played amy squirrel in bad teacher", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.film", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.film.language", "people.person.languages"], ["film.film.starring", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played amy squirrel in bad teacher [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.027076376602053642, -0.03552158176898956, -0.036805327981710434], "norm_scores": [0.3353556990623474, 0.33253544569015503, 0.33210885524749756]}}
{"id": "WebQTest-1090", "question": "where are boeing headquarters", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.place_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are boeing headquarters [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.place_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.005131925456225872, -0.043889183551073074, -0.052740197628736496], "norm_scores": [0.3429953455924988, 0.3299561142921448, 0.32704854011535645]}}
{"id": "WebQTest-1091", "question": "what type of religions are in the united states", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical.subjects"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religions are in the united states [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> book.newspaper.circulation_areas <SEP> book.periodical.subjects </PATH>"], "scores": [-0.008044049143791199, -0.015601391904056072, -0.06569081544876099], "norm_scores": [0.34054675698280334, 0.33798280358314514, 0.3214704692363739]}}
{"id": "WebQTest-1092", "question": "who did viggo mortensen play in lord of the rings", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.performance.actor", "film.performance.character"], ["film.film.story_by", "fictional_universe.fictional_character.character_created_by"], ["music.featured_artist.recordings", "book.book_character.appears_in_book"], ["film.performance.film", "film.performance.character"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["media_common.adaptation.adapted_from", "book.book_character.appears_in_book"], ["film.film_series.films_in_series", "book.book.characters"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["media_common.adapted_work.adaptations", "book.book.characters"], ["film.film.starring", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did viggo mortensen play in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.02191540226340294, -0.022376926615834236, -0.02772211842238903], "norm_scores": [0.3340293765068054, 0.33387526869773865, 0.3320953845977783]}}
{"id": "WebQTest-1094", "question": "who was isaac newton and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was isaac newton and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0006823710864409804, -0.06423261016607285, -0.08127429336309433], "norm_scores": [0.3495284914970398, 0.32800695300102234, 0.3224645256996155]}}
{"id": "WebQTest-1095", "question": "who did william shakespeare marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"]], "ground_paths": [["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["book.book_subject.works", "book.book_subject.works"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.place_of_interment.interred_here", "people.deceased_person.place_of_burial"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did william shakespeare marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.018308259546756744, -0.04303164407610893, -0.04554847627878189], "norm_scores": [0.3391316831111908, 0.3308499753475189, 0.3300183415412903]}}
{"id": "WebQTest-1096", "question": "what movies queen latifah is in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"]], "ground_paths": [["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.film.produced_by"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["film.actor.film", "film.performance.film"], ["film.producer.film", "film.film.sequel"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "film.film.country"], ["film.producer.films_executive_produced"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["music.recording.artist"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["film.film.executive_produced_by"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies queen latifah is in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>"], "scores": [-0.025736117735505104, -0.026191430166363716, -0.03853534162044525], "norm_scores": [0.3348034620285034, 0.33465105295181274, 0.3305455446243286]}}
{"id": "WebQTest-1097", "question": "where did michelle obama graduated from", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.place_of_birth", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did michelle obama graduated from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.016018632799386978, -0.018650894984602928, -0.022105218842625618], "norm_scores": [0.33430245518684387, 0.3334236443042755, 0.3322739005088806]}}
{"id": "WebQTest-1098", "question": "what kind of music did franz liszt compose", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.composition.composer"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of music did franz liszt compose [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.004896256141364574, -0.032856959849596024, -0.0638527199625969], "norm_scores": [0.3430326282978058, 0.3335740566253662, 0.3233932852745056]}}
{"id": "WebQTest-1099", "question": "what country does david luiz play for", "prediction": [["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["people.person.nationality", "sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does david luiz play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.03122560679912567, -0.03160851448774338, -0.033942729234695435], "norm_scores": [0.3336777091026306, 0.33354997634887695, 0.33277231454849243]}}
{"id": "WebQTest-1100", "question": "what document did thomas jefferson wrote", "prediction": [["book.written_work.author"], ["book.author.book_editions_published"], ["book.author.works_written"]], "ground_paths": [["book.book_subject.works"], ["book.written_work.author"], ["book.author.book_editions_published"], ["book.book_edition.author_editor"], [], ["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what document did thomas jefferson wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.book_editions_published </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.010884457267820835, -0.027552366256713867, -0.036624252796173096], "norm_scores": [0.33805954456329346, 0.33247148990631104, 0.3294689953327179]}}
{"id": "WebQTest-1101", "question": "who is martin luther king jr facts", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"], ["base.activism.activist.area_of_activism"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is martin luther king jr facts [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0016244752332568169, -0.06712508946657181, -0.06860816478729248], "norm_scores": [0.3482125699520111, 0.3261353671550751, 0.32565203309059143]}}
{"id": "WebQTest-1102", "question": "where is victoria british columbia", "prediction": [["location.location.containedby"], ["location.province.capital"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["base.aareas.schema.administrative_area.capital"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is victoria british columbia [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.province.capital </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.010108903050422668, -0.019400713965296745, -0.06955249607563019], "norm_scores": [0.34094318747520447, 0.3377898931503296, 0.32126691937446594]}}
{"id": "WebQTest-1103", "question": "how many mary mary sisters", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["music.artist.genre", "music.artist.genre"], ["people.person.parents", "people.person.children"]], "ground_paths": [["music.album.artist", "music.composition.composer"], ["music.artist.origin", "people.person.place_of_birth"], ["music.album.artist", "music.composer.compositions"], ["music.musical_group.member", "music.group_member.membership"], ["music.group_membership.group", "music.group_membership.member"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many mary mary sisters [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.039300233125686646, -0.03968067094683647, -0.04908617585897446], "norm_scores": [0.33446142077445984, 0.33433419466018677, 0.3312043249607086]}}
{"id": "WebQTest-1104", "question": "what do we call the currency of france", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do we call the currency of france [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.008757824078202248, -0.018487611785531044, -0.09837578982114792], "norm_scores": [0.34428176283836365, 0.3409481942653656, 0.31477007269859314]}}
{"id": "WebQTest-1106", "question": "what team does david beckham play for in 2012", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["book.written_work.subjects", "book.written_work.subjects"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_loan.player", "soccer.football_player_loan.lending_team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does david beckham play for in 2012 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.01975010335445404, -0.025610314682126045, -0.029680931940674782], "norm_scores": [0.3350897431373596, 0.3331317901611328, 0.33177849650382996]}}
{"id": "WebQTest-1107", "question": "what movies did nick cannon star in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["music.artist.track", "music.recording.releases"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.tv_producer.programs_produced", "tv.tv_program.tv_producer"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did nick cannon star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.017766909673810005, -0.03343973681330681, -0.035592321306467056], "norm_scores": [0.33706557750701904, 0.3318239748477936, 0.33111047744750977]}}
{"id": "WebQTest-1108", "question": "what radio station is npr on in nyc", "prediction": [["broadcast.radio_station.owner"], ["broadcast.radio_station_owner.radio_stations"], ["broadcast.content.location"]], "ground_paths": [["broadcast.producer.produces", "broadcast.producer.produces"], ["broadcast.producer.location", "broadcast.broadcast.area_served"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["broadcast.radio_network.affiliates", "broadcast.radio_affiliation_duration.station"], ["award.award_winner.awards_won", "award.award_honor.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what radio station is npr on in nyc [/INST]", "raw_output": {"paths": ["<PATH> broadcast.radio_station.owner </PATH>", "<PATH> broadcast.radio_station_owner.radio_stations </PATH>", "<PATH> broadcast.content.location </PATH>"], "scores": [-0.019127534702420235, -0.03594433516263962, -0.044245097786188126], "norm_scores": [0.3380069434642792, 0.3323702812194824, 0.3296228051185608]}}
{"id": "WebQTest-1109", "question": "who did tom landry play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tom landry play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.010716831311583519, -0.012971648015081882, -0.059524331241846085], "norm_scores": [0.3389701545238495, 0.3382067084312439, 0.32282307744026184]}}
{"id": "WebQTest-1110", "question": "where was john jay born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was john jay born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.009960712864995003, -0.016837183386087418, -0.12307770550251007], "norm_scores": [0.3464771509170532, 0.34410277009010315, 0.309420108795166]}}
{"id": "WebQTest-1111", "question": "where is mount fuji located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_range"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mount fuji located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_range </PATH>"], "scores": [-0.011157257482409477, -0.02463659457862377, -0.05031857267022133], "norm_scores": [0.3391892611980438, 0.33464786410331726, 0.3261629045009613]}}
{"id": "WebQTest-1113", "question": "where was toussaint l'ouverture born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was toussaint l'ouverture born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.004195268731564283, -0.029954850673675537, -0.08525734394788742], "norm_scores": [0.3452197313308716, 0.3364405632019043, 0.31833967566490173]}}
{"id": "WebQTest-1114", "question": "what countries makes up the uk", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries makes up the uk [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.018254583701491356, -0.031297650188207626, -0.03151283785700798], "norm_scores": [0.3362620770931244, 0.33190464973449707, 0.33183324337005615]}}
{"id": "WebQTest-1115", "question": "what is lawton ok zip code", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.city.postal_codes"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is lawton ok zip code [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.city.postal_codes </PATH>"], "scores": [-0.00018077447020914406, -0.09857232123613358, -0.1324019432067871], "norm_scores": [0.35939663648605347, 0.3257189989089966, 0.31488436460494995]}}
{"id": "WebQTest-1116", "question": "what is the capital city of spain wiki answers", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital city of spain wiki answers [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.007092954590916634, -0.03239646553993225, -0.04029378667473793], "norm_scores": [0.33986344933509827, 0.33137160539627075, 0.328764945268631]}}
{"id": "WebQTest-1117", "question": "where do they speak german", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak german [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01759699173271656, -0.02175196073949337, -0.023737020790576935], "norm_scores": [0.33447808027267456, 0.33309119939804077, 0.3324306607246399]}}
{"id": "WebQTest-1118", "question": "when did mark mcgwire retired", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["baseball.lifetime_batting_statistics.player", "baseball.lifetime_batting_statistics.ending_season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did mark mcgwire retired [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.013509576208889484, -0.015053908340632915, -0.026043318212032318], "norm_scores": [0.3348960280418396, 0.3343792259693146, 0.33072471618652344]}}
{"id": "WebQTest-1119", "question": "what is colorado technical university", "prediction": [["location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is colorado technical university [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.025254439562559128, -0.032138675451278687, -0.039409056305885315], "norm_scores": [0.33567360043525696, 0.3333706557750702, 0.33095574378967285]}}
{"id": "WebQTest-1120", "question": "what was the currency used in france before the euro", "prediction": [["location.country.currency_formerly_used"], ["finance.currency.countries_formerly_used"], ["finance.currency.countries_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the currency used in france before the euro [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_formerly_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> finance.currency.countries_used </PATH>"], "scores": [-0.006287504453212023, -0.024624789133667946, -0.04784587398171425], "norm_scores": [0.34000617265701294, 0.3338281810283661, 0.32616567611694336]}}
{"id": "WebQTest-1121", "question": "what is kentucky state slogan", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is kentucky state slogan [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.013839976862072945, -0.019689325243234634, -0.023939061909914017], "norm_scores": [0.3351072072982788, 0.33315277099609375, 0.33173999190330505]}}
{"id": "WebQTest-1124", "question": "who played samantha stephens in bewitched", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["tv.tv_program.regular_cast", "tv.tv_actor.starring_roles"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played samantha stephens in bewitched [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.016088303178548813, -0.03172125667333603, -0.03304168954491615], "norm_scores": [0.3369637727737427, 0.3317369818687439, 0.3312992453575134]}}
{"id": "WebQTest-1125", "question": "what area of science did sir isaac newton study", "prediction": [["people.person.profession"], ["education.education.student", "education.education.major_field_of_study"], ["law.invention.inventor"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what area of science did sir isaac newton study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.005639310926198959, -0.032867446541786194, -0.04505672678351402], "norm_scores": [0.34077489376068115, 0.331621378660202, 0.3276037275791168]}}
{"id": "WebQTest-1126", "question": "what county is san diego in", "prediction": [["location.us_county.hud_county_place"], ["location.hud_county_place.county"], ["location.location.containedby"]], "ground_paths": [["location.us_county.county_seat"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is san diego in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.018922507762908936, -0.022697344422340393, -0.025543415918946266], "norm_scores": [0.33448919653892517, 0.3332289457321167, 0.3322818875312805]}}
{"id": "WebQTest-1127", "question": "which country in north america is divided into provinces", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["base.locations.countries.continent"]], "ground_paths": [["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country in north america is divided into provinces [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.01668235845863819, -0.021030042320489883, -0.022506972774863243], "norm_scores": [0.33446449041366577, 0.33301347494125366, 0.3325220048427582]}}
{"id": "WebQTest-1130", "question": "what is real name of santa claus", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character.occupation", "people.person.profession"]], "ground_paths": [["fictional_universe.fictional_character.based_on"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is real name of santa claus [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character.occupation <SEP> people.person.profession </PATH>"], "scores": [-0.005120823159813881, -0.040271081030368805, -0.053726568818092346], "norm_scores": [0.34269848465919495, 0.33086180686950684, 0.3264397084712982]}}
{"id": "WebQTest-1131", "question": "when did kelly slater go pro", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.sports_award.award_winner", "sports.sports_award.season"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did kelly slater go pro [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.sports_award.award_winner <SEP> sports.sports_award.season </PATH>"], "scores": [-0.01701236702501774, -0.030667388811707497, -0.0371515229344368], "norm_scores": [0.3370975852012634, 0.3325257897377014, 0.33037662506103516]}}
{"id": "WebQTest-1132", "question": "what did coretta scott king died from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did coretta scott king died from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.0013606335269287229, -0.045330557972192764, -0.11350324004888535], "norm_scores": [0.35076645016670227, 0.33567747473716736, 0.3135560154914856]}}
{"id": "WebQTest-1133", "question": "what does wh smith stand for", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["organization.organization.founders"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does wh smith stand for [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> organization.organization.founders </PATH>"], "scores": [-0.024361126124858856, -0.042000312358140945, -0.04822373390197754], "norm_scores": [0.3379594087600708, 0.3320503234863281, 0.3299902677536011]}}
{"id": "WebQTest-1135", "question": "who was vp for nixon", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vp for nixon [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.005253670737147331, -0.024143854156136513, -0.11786618083715439], "norm_scores": [0.3478522300720215, 0.34134289622306824, 0.3108048439025879]}}
{"id": "WebQTest-1136", "question": "what religion did adolf hitler follow", "prediction": [["people.person.religion"], ["religion.religion.notable_figures"], ["religion.religion.founding_figures"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion did adolf hitler follow [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.founding_figures </PATH>"], "scores": [-0.0011592904338613153, -0.061025410890579224, -0.08262941241264343], "norm_scores": [0.34920457005500793, 0.32891252636909485, 0.32188287377357483]}}
{"id": "WebQTest-1137", "question": "where is the illinois river on a map", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the illinois river on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01122441329061985, -0.027847062796354294, -0.03366556018590927], "norm_scores": [0.33768683671951294, 0.33212000131607056, 0.3301931619644165]}}
{"id": "WebQTest-1138", "question": "where honduras in the world", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.contains"], ["base.locations.continents.countries_within"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where honduras in the world [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.01657402329146862, -0.019804012030363083, -0.03818197548389435], "norm_scores": [0.3360893726348877, 0.33500558137893677, 0.3289050757884979]}}
{"id": "WebQTest-1142", "question": "who does ronaldinho play for now 2011", "prediction": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does ronaldinho play for now 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>", "<PATH> people.person.nationality <SEP> sports.sports_team_location.teams </PATH>", "<PATH> soccer.football_player_stats.player <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.02479822374880314, -0.025125648826360703, -0.028074221685528755], "norm_scores": [0.33373358845710754, 0.33362433314323425, 0.3326420783996582]}}
{"id": "WebQTest-1143", "question": "what galileo galilei was famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["astronomy.astronomical_discovery.discoverer", "astronomy.star_system_body.star_system"], ["influence.influence_node.influenced", "base.argumentmaps.innovator.original_ideas"], ["symbols.name_source.namesakes", "astronomy.star_system_body.star_system"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["book.written_work.author", "book.book_subject.works"], ["astronomy.astronomer.astronomical_objects_discovered", "astronomy.star_system_body.star_system"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what galileo galilei was famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.001319430535659194, -0.06395099312067032, -0.07483211904764175], "norm_scores": [0.34862470626831055, 0.32745954394340515, 0.3239157199859619]}}
{"id": "WebQTest-1144", "question": "what are the major trading partners of germany", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major trading partners of germany [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_games.participating_countries </PATH>", "<PATH> location.statistical_region.places_exported_to <SEP> location.statistical_region.places_imported_from </PATH>"], "scores": [-0.028776362538337708, -0.03923165425658226, -0.041426245123147964], "norm_scores": [0.3359053432941437, 0.33241167664527893, 0.331682950258255]}}
{"id": "WebQTest-1145", "question": "where did romney graduated college", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.religion", "religion.religion.organizations"], ["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did romney graduated college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.0184285007417202, -0.018803376704454422, -0.023310665041208267], "norm_scores": [0.33391714096069336, 0.33379197120666504, 0.3322908878326416]}}
{"id": "WebQTest-1146", "question": "who did johnny depp play in corpse bride", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.performance.character"], ["film.actor.film", "film.performance.character"]], "ground_paths": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did johnny depp play in corpse bride [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>"], "scores": [-0.017575711011886597, -0.025783078745007515, -0.032347194850444794], "norm_scores": [0.3358902037143707, 0.3331447243690491, 0.3309650719165802]}}
{"id": "WebQTest-1147", "question": "what is the zip code for jonesboro ar", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for jonesboro ar [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-7.249582995427772e-05, -0.10644890367984772, -0.1529141664505005], "norm_scores": [0.3626668155193329, 0.3260686993598938, 0.3112644553184509]}}
{"id": "WebQTest-1148", "question": "what college did se hinton go", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did se hinton go [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.017704907804727554, -0.019559163600206375, -0.023087812587618828], "norm_scores": [0.33413758873939514, 0.3335185945034027, 0.33234381675720215]}}
{"id": "WebQTest-1149", "question": "when was taylor swift fearless tour", "prediction": [["music.concert_tour.artist"], ["time.event.locations"], ["music.artist.concert_tours", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was taylor swift fearless tour [/INST]", "raw_output": {"paths": ["<PATH> music.concert_tour.artist </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> music.artist.concert_tours <SEP> time.event.locations </PATH>"], "scores": [-0.004375155083835125, -0.04936192184686661, -0.06097090244293213], "norm_scores": [0.3447103500366211, 0.32954660058021545, 0.32574301958084106]}}
{"id": "WebQTest-1151", "question": "where did charles darwin and his wife live", "prediction": [["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"], ["book.written_work.subjects", "location.location.contains"]], "ground_paths": [["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["people.person.children", "people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"], ["book.written_work.subjects", "location.location.contains"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.places_lived", "people.place_lived.location"], ["book.written_work.subjects", "location.location.containedby"], ["people.deceased_person.place_of_burial", "travel.travel_destination.tourist_attractions"], ["people.person.nationality", "location.country.capital"], ["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["book.written_work.subjects", "location.country.capital"], ["book.written_work.subjects"], ["influence.influence_node.influenced", "people.person.place_of_birth"], ["book.book_subject.works", "book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did charles darwin and his wife live [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced <SEP> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> book.written_work.subjects <SEP> location.location.contains </PATH>"], "scores": [-0.03025588020682335, -0.03477204218506813, -0.03996317461133003], "norm_scores": [0.33491483330726624, 0.3334057033061981, 0.33167943358421326]}}
{"id": "WebQTest-1152", "question": "who is the voice of kitt", "prediction": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the voice of kitt [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.031551461666822433, -0.03946709632873535, -0.04245344176888466], "norm_scores": [0.3354272246360779, 0.3327825665473938, 0.3317902684211731]}}
{"id": "WebQTest-1153", "question": "what is eminem 's real full name", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.person.nationality"], ["music.artist.album", "music.composer.compositions"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is eminem 's real full name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> music.artist.album <SEP> music.composer.compositions </PATH>"], "scores": [-0.0338791124522686, -0.03949117287993431, -0.04169313982129097], "norm_scores": [0.33482667803764343, 0.3329528570175171, 0.33222052454948425]}}
{"id": "WebQTest-1154", "question": "who is amy grant 's first husband", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.children"]], "ground_paths": [["music.artist.origin", "music.artist.origin"], ["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.parents"], ["music.artist.genre", "music.artist.genre"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is amy grant 's first husband [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.spouse_s <SEP> people.marriage.spouse </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.025079771876335144, -0.03338807076215744, -0.04293767362833023], "norm_scores": [0.33624446392059326, 0.3334624171257019, 0.3302931487560272]}}
{"id": "WebQTest-1155", "question": "what are the seven nations of central america", "prediction": [["location.location.contains"], ["location.location.containedby"], ["common.topic.notable_types"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the seven nations of central america [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.00915611907839775, -0.02532118186354637, -0.043813712894916534], "norm_scores": [0.33899441361427307, 0.3335586190223694, 0.32744696736335754]}}
{"id": "WebQTest-1156", "question": "what is jamie spears baby name", "prediction": [["people.person.children"], ["people.person.parents"], ["base.popstra.celebrity.canoodled", "base.popstra.celebrity.canoodled"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is jamie spears baby name [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> base.popstra.celebrity.canoodled <SEP> base.popstra.celebrity.canoodled </PATH>"], "scores": [-0.00877007283270359, -0.02085067704319954, -0.1279265433549881], "norm_scores": [0.3477461338043213, 0.34357044100761414, 0.30868345499038696]}}
{"id": "WebQTest-1160", "question": "who is the mother of prince michael jackson", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the mother of prince michael jackson [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.007464790716767311, -0.03165006265044212, -0.04128873720765114], "norm_scores": [0.33980703353881836, 0.3316872715950012, 0.32850563526153564]}}
{"id": "WebQTest-1161", "question": "what is the clemson mascot name", "prediction": [["education.school_mascot.school"], ["education.educational_institution.mascot"], []], "ground_paths": [["education.school_mascot.school"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the clemson mascot name [/INST]", "raw_output": {"paths": ["<PATH> education.school_mascot.school </PATH>", "<PATH> education.educational_institution.mascot </PATH>", "<PATH> </PATH>"], "scores": [-0.012509968131780624, -0.01398874819278717, -0.0682862177491188], "norm_scores": [0.33964240550994873, 0.33914053440093994, 0.32121706008911133]}}
{"id": "WebQTest-1163", "question": "what rainforest is in south america", "prediction": [["location.location.containedby"], ["common.topic.notable_types"], ["location.location.partially_containedby"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what rainforest is in south america [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.0035417003091424704, -0.04059414565563202, -0.06194441765546799], "norm_scores": [0.3440096080303192, 0.3314964771270752, 0.324493944644928]}}
{"id": "WebQTest-1164", "question": "what type of art does claude monet do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art does claude monet do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.005949626676738262, -0.0277108047157526, -0.03419847786426544], "norm_scores": [0.3389118015766144, 0.3316163420677185, 0.3294718861579895]}}
{"id": "WebQTest-1166", "question": "when did we start war with iraq", "prediction": [["base.culturalevent.event.entity_involved"], ["time.event.locations"], ["time.event.includes_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did we start war with iraq [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> time.event.includes_event </PATH>"], "scores": [-0.025714633986353874, -0.027864130213856697, -0.031818240880966187], "norm_scores": [0.3342505395412445, 0.33353284001350403, 0.33221662044525146]}}
{"id": "WebQTest-1168", "question": "what guitar did kurt cobain", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitarist.guitars_played"]], "ground_paths": [["music.guitarist.guitars_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar did kurt cobain [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitarist.guitars_played </PATH>"], "scores": [-0.011929565109312534, -0.03322489559650421, -0.036464106291532516], "norm_scores": [0.3384445607662201, 0.33131346106529236, 0.33024200797080994]}}
{"id": "WebQTest-1169", "question": "who did jennifer lawrence play in x men", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did jennifer lawrence play in x men [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.021130956709384918, -0.027486544102430344, -0.03170451521873474], "norm_scores": [0.3352164924144745, 0.33309274911880493, 0.3316907286643982]}}
{"id": "WebQTest-1170", "question": "who is the governor of virginia 2011", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of virginia 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.016907542943954468, -0.019558312371373177, -0.0240464024245739], "norm_scores": [0.33442139625549316, 0.3335360884666443, 0.33204248547554016]}}
{"id": "WebQTest-1171", "question": "who did darth vaders voice", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did darth vaders voice [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.022200485691428185, -0.023514537140727043, -0.036158181726932526], "norm_scores": [0.33502790331840515, 0.334587961435318, 0.33038416504859924]}}
{"id": "WebQTest-1173", "question": "where is walton county ga", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.us_county.county_seat"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is walton county ga [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.us_county.county_seat </PATH>"], "scores": [-0.00911558885127306, -0.031299442052841187, -0.04608389735221863], "norm_scores": [0.3399317264556885, 0.3324737548828125, 0.32759448885917664]}}
{"id": "WebQTest-1174", "question": "what cancer did jackie kennedy die of", "prediction": [["people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what cancer did jackie kennedy die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.013542402535676956, -0.015180796384811401, -0.03618571534752846], "norm_scores": [0.3360244333744049, 0.3354743719100952, 0.32850122451782227]}}
{"id": "WebQTest-1175", "question": "what does lauren conrad major in", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["education.education.student", "education.education.degree"]], "ground_paths": [["people.person.education", "education.field_of_study.students_majoring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does lauren conrad major in [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>"], "scores": [-0.00855552963912487, -0.02147698402404785, -0.04779136925935745], "norm_scores": [0.339134156703949, 0.3347802460193634, 0.3260855972766876]}}
{"id": "WebQTest-1176", "question": "what time zone is pennsylvania in right now", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is pennsylvania in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.6402564142481424e-05, -0.12333250790834427, -0.18297095596790314], "norm_scores": [0.3680814206600189, 0.3253781497478485, 0.30654042959213257]}}
{"id": "WebQTest-1177", "question": "where did mario lopez go to school", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did mario lopez go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.012143783271312714, -0.02496681921184063, -0.026096006855368614], "norm_scores": [0.33631494641304016, 0.33202990889549255, 0.3316551744937897]}}
{"id": "WebQTest-1178", "question": "where is tupac from originally", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tupac from originally [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0043100630864501, -0.03428920730948448, -0.07074857503175735], "norm_scores": [0.34409359097480774, 0.3339310586452484, 0.32197538018226624]}}
{"id": "WebQTest-1179", "question": "what role did george lucas play in star wars", "prediction": [["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["film.writer.film"], ["film.film.produced_by"]], "ground_paths": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.film_character.portrayed_in_films"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what role did george lucas play in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.writer.film </PATH>", "<PATH> film.film.produced_by </PATH>"], "scores": [-0.029277680441737175, -0.038649801164865494, -0.04310428351163864], "norm_scores": [0.33591538667678833, 0.3327818512916565, 0.3313027620315552]}}
{"id": "WebQTest-1180", "question": "what sarah dessen books are movies", "prediction": [["film.film_story_contributor.film_story_credits"], ["book.author.works_written"], ["film.film.story_by"]], "ground_paths": [["film.film.story_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sarah dessen books are movies [/INST]", "raw_output": {"paths": ["<PATH> film.film_story_contributor.film_story_credits </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> film.film.story_by </PATH>"], "scores": [-0.02472207136452198, -0.032302405685186386, -0.03396030515432358], "norm_scores": [0.3352046012878418, 0.33267325162887573, 0.33212217688560486]}}
{"id": "WebQTest-1181", "question": "what is the current time in kauai hawaii", "prediction": [["location.location.time_zones"], ["geography.island.island_group", "location.location.time_zones"], ["location.location.containedby", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the current time in kauai hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> geography.island.island_group <SEP> location.location.time_zones </PATH>", "<PATH> location.location.containedby <SEP> location.location.time_zones </PATH>"], "scores": [-0.00023149950720835477, -0.08288171142339706, -0.09378831833600998], "norm_scores": [0.35318723320961, 0.32517001032829285, 0.32164275646209717]}}
{"id": "WebQTest-1183", "question": "where was farrah fawcett buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was farrah fawcett buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.002466769190505147, -0.03848141059279442, -0.06963825970888138], "norm_scores": [0.34486788511276245, 0.33266860246658325, 0.3224635124206543]}}
{"id": "WebQTest-1185", "question": "what inspired steinbeck", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired steinbeck [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.010016275569796562, -0.01515283528715372, -0.1197240874171257], "norm_scores": [0.3459043502807617, 0.3441321551799774, 0.30996349453926086]}}
{"id": "WebQTest-1186", "question": "what kind of government does cuba have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does cuba have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011572649702429771, -0.012184047140181065, -0.14709827303886414], "norm_scores": [0.3481111526489258, 0.3478983938694, 0.3039904534816742]}}
{"id": "WebQTest-1187", "question": "where are english speakers distributed", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are english speakers distributed [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013556934893131256, -0.024672070518136024, -0.03235580399632454], "norm_scores": [0.33666372299194336, 0.3329423666000366, 0.3303939402103424]}}
{"id": "WebQTest-1189", "question": "what is the capital of italy today", "prediction": [["location.country.capital"], ["base.biblioness.bibs_location.country"], ["location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the capital of italy today [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01265067607164383, -0.026597442105412483, -0.02859107032418251], "norm_scores": [0.33666226267814636, 0.3319994807243347, 0.3313382863998413]}}
{"id": "WebQTest-1191", "question": "what language do people speak in the bahamas", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in the bahamas [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.004808969330042601, -0.03297029808163643, -0.04284140467643738], "norm_scores": [0.3407255709171295, 0.3312641382217407, 0.32801032066345215]}}
{"id": "WebQTest-1192", "question": "where did george michael go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did george michael go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.016605855897068977, -0.01837081089615822, -0.02510681375861168], "norm_scores": [0.33447369933128357, 0.3338839113712311, 0.33164238929748535]}}
{"id": "WebQTest-1194", "question": "what club does cristiano ronaldo play for in 2010", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what club does cristiano ronaldo play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.02027405984699726, -0.021491272374987602, -0.026472683995962143], "norm_scores": [0.3341571092605591, 0.3337506353855133, 0.33209219574928284]}}
{"id": "WebQTest-1195", "question": "what time zone is in the bahamas", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is in the bahamas [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-8.118619007291272e-06, -0.15885360538959503, -0.18637175858020782], "norm_scores": [0.3727032244205475, 0.3179636001586914, 0.3093331456184387]}}
{"id": "WebQTest-1197", "question": "what language do people speak in brazil", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people speak in brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0005926894373260438, -0.07371734827756882, -0.08517743647098541], "norm_scores": [0.3510769307613373, 0.32632070779800415, 0.32260239124298096]}}
{"id": "WebQTest-1198", "question": "who influenced samuel taylor coleridge", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who influenced samuel taylor coleridge [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.006907871924340725, -0.018787197768688202, -0.09018151462078094], "norm_scores": [0.3438446521759033, 0.339784175157547, 0.31637123227119446]}}
{"id": "WebQTest-1199", "question": "what does kim kardashian work as", "prediction": [["people.person.profession"], ["freebase.valuenotation.is_reviewed", "common.topic.notable_types"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does kim kardashian work as [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> common.topic.notable_types </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.000943697348702699, -0.07516998052597046, -0.08908144384622574], "norm_scores": [0.35160553455352783, 0.32645222544670105, 0.3219422698020935]}}
{"id": "WebQTest-1200", "question": "where did the mississippi river end", "prediction": [["geography.river.mouth"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the mississippi river end [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.0013081054203212261, -0.06232630833983421, -0.09109015762805939], "norm_scores": [0.35027047991752625, 0.3295366168022156, 0.3201928734779358]}}
{"id": "WebQTest-1201", "question": "who played carlton lassiter on psych", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played carlton lassiter on psych [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.01983049511909485, -0.02683042734861374, -0.030634187161922455], "norm_scores": [0.3353140652179718, 0.3329750597476959, 0.33171093463897705]}}
{"id": "WebQTest-1203", "question": "who is president of israel", "prediction": [["people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is president of israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.018927477300167084, -0.01996186003088951, -0.024655796587467194], "norm_scores": [0.33408457040786743, 0.33373916149139404, 0.3321762681007385]}}
{"id": "WebQTest-1204", "question": "where did the anasazi indians live", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.contains"], ["location.location.containedby"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the anasazi indians live [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.000707508937921375, -0.08375868201255798, -0.09515052288770676], "norm_scores": [0.353333979845047, 0.3251746892929077, 0.3214913606643677]}}
{"id": "WebQTest-1205", "question": "who plays harley quinn", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["cvg.game_performance.character", "cvg.game_performance.voice_actor"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays harley quinn [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.01627223938703537, -0.0313078872859478, -0.03932609781622887], "norm_scores": [0.33757707476615906, 0.3325393497943878, 0.3298836350440979]}}
{"id": "WebQTest-1206", "question": "what was francis bacon contributions", "prediction": [["people.person.profession"], ["law.invention.inventor"], ["law.inventor.inventions"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was francis bacon contributions [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.inventions </PATH>"], "scores": [-0.005756111815571785, -0.04382465034723282, -0.04537402093410492], "norm_scores": [0.3420202434062958, 0.3292447626590729, 0.32873502373695374]}}
{"id": "WebQTest-1209", "question": "when does ziva start on ncis", "prediction": [["tv.tv_series_season.series"], ["tv.tv_program.seasons"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.seasons"]], "ground_paths": [["tv.tv_program.seasons"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when does ziva start on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_series_season.series </PATH>", "<PATH> tv.tv_program.seasons </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.seasons </PATH>"], "scores": [-0.01235709898173809, -0.03660209849476814, -0.04507991671562195], "norm_scores": [0.33969083428382874, 0.3315540552139282, 0.32875511050224304]}}
{"id": "WebQTest-1210", "question": "what is the name of the national anthem of canada ( 2 words )", "prediction": [["music.composition.language"], ["music.composition.language", "music.composition.language"], ["location.location.containedby", "music.composition.language"]], "ground_paths": [["location.country.official_language", "music.composition.language"], ["location.country.national_anthem", "government.national_anthem.national_anthem_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the national anthem of canada ( 2 words ) [/INST]", "raw_output": {"paths": ["<PATH> music.composition.language </PATH>", "<PATH> music.composition.language <SEP> music.composition.language </PATH>", "<PATH> location.location.containedby <SEP> music.composition.language </PATH>"], "scores": [-0.010624832473695278, -0.020502371713519096, -0.059181466698646545], "norm_scores": [0.3398153781890869, 0.3364753723144531, 0.32370927929878235]}}
{"id": "WebQTest-1212", "question": "what did mark zuckerberg study", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.profession", "base.skills.skill.performed_by"], ["people.person.education", "education.field_of_study.students_majoring"], ["education.education.student", "education.education.major_field_of_study"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did mark zuckerberg study [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.010379712097346783, -0.019136913120746613, -0.03265516832470894], "norm_scores": [0.3367851674556732, 0.3338487446308136, 0.3293660879135132]}}
{"id": "WebQTest-1213", "question": "which country was michael jackson born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country was michael jackson born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.01309273112565279, -0.01728365570306778, -0.04902498424053192], "norm_scores": [0.33777809143066406, 0.3363654315471649, 0.32585641741752625]}}
{"id": "WebQTest-1215", "question": "who was stephen r covey", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was stephen r covey [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0011144396848976612, -0.057775914669036865, -0.1001468226313591], "norm_scores": [0.3508000075817108, 0.33147579431533813, 0.3177242577075958]}}
{"id": "WebQTest-1216", "question": "what country was stalin in charge of", "prediction": [["people.person.nationality"], ["government.government_position_held.office_holder", "government.governmental_jurisdiction.governing_officials"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country was stalin in charge of [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.governmental_jurisdiction.governing_officials </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>"], "scores": [-0.01456468366086483, -0.02269677259027958, -0.04980574548244476], "norm_scores": [0.3381493091583252, 0.33541062474250793, 0.32644009590148926]}}
{"id": "WebQTest-1217", "question": "what is monta ellis career high points", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"]], "ground_paths": [["award.award_honor.award_winner", "award.award_honor.award"], ["sports.sports_award_winner.awards", "sports.sports_award.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is monta ellis career high points [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>"], "scores": [-0.011440226808190346, -0.01706530712544918, -0.04140632599592209], "norm_scores": [0.33728301525115967, 0.335391104221344, 0.3273259103298187]}}
{"id": "WebQTest-1219", "question": "what nationality is rihanna", "prediction": [["people.person.nationality"], ["people.ethnicity.people"], ["music.artist.origin"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what nationality is rihanna [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.011890081688761711, -0.02907615154981613, -0.04092751070857048], "norm_scores": [0.3384849727153778, 0.3327174186706543, 0.3287975788116455]}}
{"id": "WebQTest-1220", "question": "what has shannen doherty played in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.performance.actor", "film.film.starring"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.languages", "tv.tv_program.languages"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what has shannen doherty played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.016314588487148285, -0.031084047630429268, -0.033161815255880356], "norm_scores": [0.33685538172721863, 0.33191677927970886, 0.3312278389930725]}}
{"id": "WebQTest-1221", "question": "what college did joakim noah", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.school"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did joakim noah [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01827278733253479, -0.02203582040965557, -0.024835532531142235], "norm_scores": [0.334481418132782, 0.3332251310348511, 0.33229348063468933]}}
{"id": "WebQTest-1223", "question": "who plays billy elliot", "prediction": [["film.film.country", "people.person.nationality"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["media_common.netflix_title.netflix_genres", "people.person.nationality"], ["award.award_winning_work.awards_won", "award.award_winner.awards_won"], ["film.performance.film", "film.performance.actor"], ["film.film.language", "people.person.languages"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["award.award_honor.honored_for", "award.award_winner.awards_won"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays billy elliot [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.030210336670279503, -0.033685844391584396, -0.034789081662893295], "norm_scores": [0.3342288136482239, 0.33306920528411865, 0.33270198106765747]}}
{"id": "WebQTest-1225", "question": "what kourtney kardashian boyfriends name", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.marriage.spouse", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kourtney kardashian boyfriends name [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.01936034858226776, -0.023745158687233925, -0.025165284052491188], "norm_scores": [0.33446642756462097, 0.33300307393074036, 0.33253049850463867]}}
{"id": "WebQTest-1226", "question": "what form of government does australia use", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["type.type.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does australia use [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> type.type.form_of_government </PATH>"], "scores": [-0.0077008032239973545, -0.017901895567774773, -0.15827837586402893], "norm_scores": [0.3508695960044861, 0.34730854630470276, 0.30182188749313354]}}
{"id": "WebQTest-1227", "question": "what team did howie long", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.drafted_athlete.drafted", "sports.professional_sports_team.draft_picks"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did howie long [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.professional_sports_team.draft_picks </PATH>"], "scores": [-0.012304841540753841, -0.012606756761670113, -0.05012545362114906], "norm_scores": [0.33754318952560425, 0.3374413251876831, 0.32501551508903503]}}
{"id": "WebQTest-1228", "question": "where did the greek alphabet come from", "prediction": [["language.human_language.language_family"], ["language.language_family.languages"], ["language.language_writing_system.parent_writing_systems"]], "ground_paths": [["language.language_writing_system.child_writing_systems"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the greek alphabet come from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> language.language_family.languages </PATH>", "<PATH> language.language_writing_system.parent_writing_systems </PATH>"], "scores": [-0.020642317831516266, -0.02284187264740467, -0.0718592032790184], "norm_scores": [0.3392273485660553, 0.3384820222854614, 0.3222905993461609]}}
{"id": "WebQTest-1229", "question": "what timezone is elizabethtown kentucky", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is elizabethtown kentucky [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.706023774924688e-06, -0.1757909506559372, -0.1802559196949005], "norm_scores": [0.37399032711982727, 0.31370362639427185, 0.3123060464859009]}}
{"id": "WebQTest-1230", "question": "what to see in christchurch new zealand", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["location.location.contains"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to see in christchurch new zealand [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012341990135610104, -0.012539480812847614, -0.04796373099088669], "norm_scores": [0.337289959192276, 0.3372233808040619, 0.32548660039901733]}}
{"id": "WebQTest-1231", "question": "where is chris paul from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chris paul from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.01048816554248333, -0.017209403216838837, -0.07062078267335892], "norm_scores": [0.34072238206863403, 0.3384399712085724, 0.32083770632743835]}}
{"id": "WebQTest-1232", "question": "what year was albert pujols rookie year", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.season"], ["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["sports.pro_athlete.teams", "sports.sports_league_season.teams"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.season"], ["sports.sports_award.award_winner", "sports.sports_league_season.awards"], ["baseball.lifetime_batting_statistics.player", "baseball.lifetime_batting_statistics.starting_season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was albert pujols rookie year [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.season </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.season </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_league_season.teams </PATH>"], "scores": [-0.012094808742403984, -0.013827309012413025, -0.05105763301253319], "norm_scores": [0.3378314971923828, 0.33724671602249146, 0.32492178678512573]}}
{"id": "WebQTest-1233", "question": "what are the colors of the ny giants", "prediction": [["sports.sports_team.colors"], ["sports.professional_sports_team.owner_s", "sports.sports_team.colors"], ["sports.professional_sports_team.colors"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the colors of the ny giants [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.owner_s <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.professional_sports_team.colors </PATH>"], "scores": [-4.141022509429604e-05, -0.10799745470285416, -0.1412121057510376], "norm_scores": [0.3615318238735199, 0.32453522086143494, 0.31393295526504517]}}
{"id": "WebQTest-1234", "question": "what business did andrew carnegie dominate", "prediction": [["people.person.profession"], ["book.written_work.subjects"], ["book.written_work.author"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what business did andrew carnegie dominate [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.010743689723312855, -0.04332292452454567, -0.04815353825688362], "norm_scores": [0.3411540687084198, 0.3302186131477356, 0.3286273181438446]}}
{"id": "WebQTest-1235", "question": "what was sir isaac newton 's inventions", "prediction": [["law.invention.inventor"], ["law.inventor.inventions"], ["symbols.namesake.named_after"]], "ground_paths": [["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was sir isaac newton 's inventions [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.inventions </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0041680652648210526, -0.032184455543756485, -0.06163691356778145], "norm_scores": [0.3428739309310913, 0.3334011435508728, 0.3237248659133911]}}
{"id": "WebQTest-1237", "question": "who did shaq first play for", "prediction": [["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_award.team"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did shaq first play for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.team </PATH>", "<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>"], "scores": [-0.019146468490362167, -0.020710250362753868, -0.02158687263727188], "norm_scores": [0.33377835154533386, 0.3332568109035492, 0.33296477794647217]}}
{"id": "WebQTest-1238", "question": "who was the artist of mickey mouse", "prediction": [["fictional_universe.fictional_character.character_created_by"], ["fictional_universe.fictional_character_creator.fictional_characters_created"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character_creator.fictional_characters_created"], ["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the artist of mickey mouse [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.0009547439985908568, -0.0696333795785904, -0.07154479622840881], "norm_scores": [0.3489828407764435, 0.3258196711540222, 0.3251974880695343]}}
{"id": "WebQTest-1239", "question": "what is the zip code for concord", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for concord [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.00013934692833572626, -0.10219082236289978, -0.1387712061405182], "norm_scores": [0.3605511784553528, 0.32557159662246704, 0.31387728452682495]}}
{"id": "WebQTest-1240", "question": "what football team is michael vick on", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what football team is michael vick on [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.football_player.rushing <SEP> american_football.player_rushing_statistics.team </PATH>"], "scores": [-0.01955941691994667, -0.02300405129790306, -0.037693943828344345], "norm_scores": [0.33572930097579956, 0.334574818611145, 0.3296958804130554]}}
{"id": "WebQTest-1241", "question": "where does the columbia river start", "prediction": [["geography.lake.outflow"], ["geography.river.origin"], []], "ground_paths": [["geography.lake.outflow"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the columbia river start [/INST]", "raw_output": {"paths": ["<PATH> geography.lake.outflow </PATH>", "<PATH> geography.river.origin </PATH>", "<PATH> </PATH>"], "scores": [-0.00710786459967494, -0.02929317206144333, -0.06992721557617188], "norm_scores": [0.34279772639274597, 0.33527639508247375, 0.3219258189201355]}}
{"id": "WebQTest-1242", "question": "what are all the names of the harry potter books", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["book.book_character.appears_in_book"], ["fictional_universe.fictional_universe.works_set_here"]], "ground_paths": [["book.book_character.appears_in_book"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes"], ["fictional_universe.fictional_universe.works_set_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the names of the harry potter books [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes </PATH>", "<PATH> book.book_character.appears_in_book </PATH>", "<PATH> fictional_universe.fictional_universe.works_set_here </PATH>"], "scores": [-0.008166047744452953, -0.02604825049638748, -0.03123074397444725], "norm_scores": [0.3378976583480835, 0.3319090008735657, 0.33019334077835083]}}
{"id": "WebQTest-1244", "question": "who was sir walter raleigh and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was sir walter raleigh and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.000888531154487282, -0.05909508839249611, -0.0766233280301094], "norm_scores": [0.348369300365448, 0.32867076992988586, 0.3229599595069885]}}
{"id": "WebQTest-1245", "question": "what countries are in the euro dollar", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are in the euro dollar [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>"], "scores": [-0.011452162638306618, -0.018759798258543015, -0.04869842901825905], "norm_scores": [0.3382769227027893, 0.3358139395713806, 0.3259091377258301]}}
{"id": "WebQTest-1246", "question": "what currency can you use in barbados", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency can you use in barbados [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.010915053077042103, -0.015043775551021099, -0.13643746078014374], "norm_scores": [0.347473680973053, 0.3460420072078705, 0.30648428201675415]}}
{"id": "WebQTest-1247", "question": "who is angelina jolie", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.person.nationality"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolie [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0018915203399956226, -0.053047746419906616, -0.09330953657627106], "norm_scores": [0.3493124842643738, 0.33189234137535095, 0.31879517436027527]}}
{"id": "WebQTest-1248", "question": "where is nineveh city", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is nineveh city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.0007736075785942376, -0.0737241581082344, -0.10187368094921112], "norm_scores": [0.352921724319458, 0.3280925452709198, 0.3189857006072998]}}
{"id": "WebQTest-1249", "question": "what is the state flower of new mexico", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state flower of new mexico [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.013462621718645096, -0.017244389280676842, -0.022607406601309776], "norm_scores": [0.3347703516483307, 0.3335067331790924, 0.3317229151725769]}}
{"id": "WebQTest-1250", "question": "where is the national ffa foundation located today", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the national ffa foundation located today [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.007748678792268038, -0.034884531050920486, -0.041598472744226456], "norm_scores": [0.3401423394680023, 0.33103638887405396, 0.3288212716579437]}}
{"id": "WebQTest-1251", "question": "which countries speak germanic languages", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak germanic languages [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01486298255622387, -0.03141098842024803, -0.03553479164838791], "norm_scores": [0.33748114109039307, 0.33194243907928467, 0.3305763900279999]}}
{"id": "WebQTest-1252", "question": "who plays eowyn in lord of the rings", "prediction": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.film.starring", "film.performance.actor"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_honor.award_winner"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_honor.honored_for", "award.award_winner.awards_won"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays eowyn in lord of the rings [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.033224258571863174, -0.03394147381186485, -0.03501880168914795], "norm_scores": [0.33361244201660156, 0.3333732485771179, 0.3330143094062805]}}
{"id": "WebQTest-1254", "question": "where is augusta masters 2012", "prediction": [["time.event.locations"], ["time.recurring_event.instances"], ["location.location.events"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is augusta masters 2012 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.recurring_event.instances </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.0021302886307239532, -0.05204338580369949, -0.06824086606502533], "norm_scores": [0.346339613199234, 0.3294770419597626, 0.3241833448410034]}}
{"id": "WebQTest-1255", "question": "what type of cancer did eva peron die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["medicine.disease.notable_people_with_this_condition"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of cancer did eva peron die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>"], "scores": [-0.008284303359687328, -0.01959814317524433, -0.038360998034477234], "norm_scores": [0.33793818950653076, 0.33413636684417725, 0.327925443649292]}}
{"id": "WebQTest-1256", "question": "who did terrell owens play for in 2010", "prediction": [["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"]], "ground_paths": [["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did terrell owens play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.017834380269050598, -0.022522425279021263, -0.02252708002924919], "norm_scores": [0.3343764543533325, 0.3328125774860382, 0.33281099796295166]}}
{"id": "WebQTest-1257", "question": "where is denmark south carolina", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [["location.us_county.hud_county_place"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is denmark south carolina [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.01332161296159029, -0.021355073899030685, -0.05127684772014618], "norm_scores": [0.338437557220459, 0.33572959899902344, 0.3258328139781952]}}
{"id": "WebQTest-1259", "question": "what books did ernest hemingway write", "prediction": [["book.written_work.author"], ["book.book_edition.author_editor"], ["book.author.works_written"]], "ground_paths": [["book.book_edition.author_editor"], ["book.written_work.author"], ["film.film_story_contributor.film_story_credits"], ["film.film.story_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did ernest hemingway write [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.016762714833021164, -0.02254795841872692, -0.027224496006965637], "norm_scores": [0.3351403772830963, 0.3332071006298065, 0.3316524922847748]}}
{"id": "WebQTest-1260", "question": "what year did baltimore ravens win super bowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did baltimore ravens win super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.012903975322842598, -0.013679261319339275, -0.039198629558086395], "norm_scores": [0.3363296687602997, 0.3360690176486969, 0.32760125398635864]}}
{"id": "WebQTest-1261", "question": "where did the battle of passchendaele happen", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the battle of passchendaele happen [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.013045470230281353, -0.013296489603817463, -0.09126710891723633], "norm_scores": [0.341937780380249, 0.34185197949409485, 0.31621021032333374]}}
{"id": "WebQTest-1262", "question": "what part of the country is ohio considered", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is ohio considered [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.012631655670702457, -0.01559910736978054, -0.08161611109972], "norm_scores": [0.3412528932094574, 0.3402417302131653, 0.3185053765773773]}}
{"id": "WebQTest-1263", "question": "what team is cristiano ronaldo play for", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.world_cup_squad.current_world_cup_squad"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["soccer.football_player.transfers", "soccer.football_player_transfer.purchasing_team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is cristiano ronaldo play for [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.goals_scored <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>"], "scores": [-0.021002883091568947, -0.02448546141386032, -0.025753285735845566], "norm_scores": [0.33424869179725647, 0.33308666944503784, 0.3326646387577057]}}
{"id": "WebQTest-1264", "question": "what standard time is michigan", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what standard time is michigan [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-1.5504827388213016e-05, -0.14237111806869507, -0.1887407749891281], "norm_scores": [0.37101250886917114, 0.3217839002609253, 0.3072035610675812]}}
{"id": "WebQTest-1265", "question": "what was john tyler", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was john tyler [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.0032382516656070948, -0.04226282611489296, -0.04925315082073212], "norm_scores": [0.34284675121307373, 0.3297250270843506, 0.3274281620979309]}}
{"id": "WebQTest-1266", "question": "what did romo do", "prediction": [["american_football.football_player.position_s"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.position"], ["sports.sports_team_roster.player", "sports.sports_team_roster.position"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did romo do [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.position </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.position </PATH>"], "scores": [-0.014566098339855671, -0.038148827850818634, -0.05036778375506401], "norm_scores": [0.3399597406387329, 0.3320363759994507, 0.3280039131641388]}}
{"id": "WebQTest-1267", "question": "what currency can you use in jamaica", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency can you use in jamaica [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.000652461894787848, -0.061991479247808456, -0.16356416046619415], "norm_scores": [0.35840100049972534, 0.33707770705223083, 0.30452123284339905]}}
{"id": "WebQTest-1268", "question": "where did jay moriarty die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jay moriarty die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.00026173293008469045, -0.09912160038948059, -0.10748457908630371], "norm_scores": [0.3566085696220398, 0.3230408728122711, 0.3203505575656891]}}
{"id": "WebQTest-1269", "question": "where was the boston tea party 1773", "prediction": [["time.event.locations"], ["location.location.events"], ["event.disaster.areas_affected"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the boston tea party 1773 [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> event.disaster.areas_affected </PATH>"], "scores": [-0.006559782661497593, -0.025312872603535652, -0.06486601382493973], "norm_scores": [0.34190574288368225, 0.33555370569229126, 0.3225405514240265]}}
{"id": "WebQTest-1270", "question": "what drugs lindsay lohan abuse", "prediction": [["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"], ["people.person.substance_abuse", "people.substance_abuse.substance"]], "ground_paths": [["celebrities.celebrity.substance_abuse_problems", "celebrities.abused_substance.abused_by"], ["base.popstra.rehab_stay.patient", "base.popstra.rehab_stay.substance_abused"], ["base.popstra.substance_abuse.abuser", "base.popstra.substance_abuse.substance"], ["base.popstra.celebrity.substance_abuse", "base.popstra.substance_abuse.substance"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drugs lindsay lohan abuse [/INST]", "raw_output": {"paths": ["<PATH> base.popstra.substance_abuse.abuser <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> base.popstra.celebrity.substance_abuse <SEP> base.popstra.substance_abuse.substance </PATH>", "<PATH> people.person.substance_abuse <SEP> people.substance_abuse.substance </PATH>"], "scores": [-0.006887000985443592, -0.02502097561955452, -0.052791777998209], "norm_scores": [0.34046465158462524, 0.33434632420539856, 0.3251890242099762]}}
{"id": "WebQTest-1271", "question": "when did carolina panthers go to superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["tv.tv_actor.guest_roles", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did carolina panthers go to superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.009695742279291153, -0.01805742271244526, -0.038093965500593185], "norm_scores": [0.33741897344589233, 0.33460932970046997, 0.3279716670513153]}}
{"id": "WebQTest-1272", "question": "when did the burma cyclone happen", "prediction": [["meteorology.tropical_cyclone.affected_areas"], ["meteorology.cyclone_affected_area.cyclones"], ["time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the burma cyclone happen [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas </PATH>", "<PATH> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.00774860056117177, -0.022155581042170525, -0.04988935589790344], "norm_scores": [0.33962416648864746, 0.3347662687301636, 0.3256095349788666]}}
{"id": "WebQTest-1274", "question": "who won the governor election in texas", "prediction": [["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["people.deceased_person.place_of_death"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.appointed_by"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["location.location.containedby", "symbols.name_source.namesakes"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["location.administrative_division.country", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.state", "symbols.namesake.named_after"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.us_state.capital", "people.deceased_person.place_of_death"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["film.film_location.featured_in_films", "film.film.subjects"], ["government.governmental_jurisdiction.governing_officials", "government.political_appointer.appointees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the governor election in texas [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.02441161498427391, -0.024781079962849617, -0.024949533864855766], "norm_scores": [0.33343416452407837, 0.33331099152565, 0.3332548439502716]}}
{"id": "WebQTest-1275", "question": "what does helen flanagan do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does helen flanagan do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0010682581923902035, -0.05809219926595688, -0.09793262183666229], "norm_scores": [0.35060033202171326, 0.331167072057724, 0.31823259592056274]}}
{"id": "WebQTest-1276", "question": "when are the summer and winter olympics held", "prediction": [["time.recurring_event.instances"], ["time.event.instance_of_recurring_event"], ["time.recurring_event.current_frequency"]], "ground_paths": [["time.recurring_event.current_frequency"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when are the summer and winter olympics held [/INST]", "raw_output": {"paths": ["<PATH> time.recurring_event.instances </PATH>", "<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> time.recurring_event.current_frequency </PATH>"], "scores": [-0.008154000155627728, -0.0182841457426548, -0.08546210080385208], "norm_scores": [0.34299132227897644, 0.33953431248664856, 0.3174743354320526]}}
{"id": "WebQTest-1277", "question": "where is tennessee river", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.river.basin_countries"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tennessee river [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.004586751572787762, -0.03477095067501068, -0.07448974251747131], "norm_scores": [0.34450075030326843, 0.3342576324939728, 0.321241557598114]}}
{"id": "WebQTest-1278", "question": "what are some major events in venezuela", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["location.location.events"]], "ground_paths": [["base.culturalevent.event.entity_involved"], ["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some major events in venezuela [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> location.location.events </PATH>"], "scores": [-0.015834324061870575, -0.02217489667236805, -0.025875745341181755], "norm_scores": [0.3351556360721588, 0.33303728699684143, 0.331807017326355]}}
{"id": "WebQTest-1279", "question": "what guitar does corey taylor play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitarist.guitars_played"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what guitar does corey taylor play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitarist.guitars_played </PATH>"], "scores": [-0.007460048887878656, -0.03318798169493675, -0.05218563228845596], "norm_scores": [0.34119677543640137, 0.3325304388999939, 0.32627278566360474]}}
{"id": "WebQTest-1280", "question": "who was president after franklin d. roosevelt", "prediction": [["government.us_president.vice_president"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president after franklin d. roosevelt [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.019310684874653816, -0.02308274619281292, -0.03677153214812279], "norm_scores": [0.3356914818286896, 0.3344276249408722, 0.32988089323043823]}}
{"id": "WebQTest-1281", "question": "where did kate gosselin grew up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kate gosselin grew up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0021353319752961397, -0.04071381688117981, -0.10256976634263992], "norm_scores": [0.3488452732563019, 0.33564361929893494, 0.31551116704940796]}}
{"id": "WebQTest-1282", "question": "under what political system does the united states government operate", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["government.form_of_government.countries", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: under what political system does the united states government operate [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <SEP> location.country.form_of_government </PATH>"], "scores": [-0.004562097601592541, -0.023856760933995247, -0.12552690505981445], "norm_scores": [0.3488020896911621, 0.34213659167289734, 0.3090613782405853]}}
{"id": "WebQTest-1284", "question": "what to do in paris in 1 day", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do in paris in 1 day [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.008449756540358067, -0.020220519974827766, -0.03961513936519623], "norm_scores": [0.33811044692993164, 0.3341539800167084, 0.3277356028556824]}}
{"id": "WebQTest-1285", "question": "what city is mt lassen in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.lake.outflow"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is mt lassen in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.lake.outflow </PATH>"], "scores": [-0.005310533568263054, -0.031030239537358284, -0.11544381082057953], "norm_scores": [0.3483928442001343, 0.3395465612411499, 0.3120606243610382]}}
{"id": "WebQTest-1287", "question": "what is the state motto of ohio", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.location_symbol_relationship.symbol"]], "ground_paths": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state motto of ohio [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.016480447724461555, -0.019246574491262436, -0.02375122345983982], "norm_scores": [0.334448903799057, 0.33352506160736084, 0.33202600479125977]}}
{"id": "WebQTest-1289", "question": "what type of music did mozart composed", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["music.composition.composer"]], "ground_paths": [["music.genre.artists"], ["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did mozart composed [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.0052912174724042416, -0.031320951879024506, -0.06779452413320541], "norm_scores": [0.3432043492794037, 0.33438611030578613, 0.32240957021713257]}}
{"id": "WebQTest-1291", "question": "how many kids were there in the kennedy family", "prediction": [["people.person.parents"], ["people.person.children"], ["people.family.members"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many kids were there in the kennedy family [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.family.members </PATH>"], "scores": [-0.009741056710481644, -0.01686577871441841, -0.086815245449543], "norm_scores": [0.3426157534122467, 0.3401833772659302, 0.3172009289264679]}}
{"id": "WebQTest-1292", "question": "which barcelona airport is closest to the port", "prediction": [["location.location.nearby_airports"], ["aviation.airport.serves"], ["travel.transportation.mode_of_transportation", "travel.transport_terminus.travel_destinations_served"]], "ground_paths": [["location.location.nearby_airports"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which barcelona airport is closest to the port [/INST]", "raw_output": {"paths": ["<PATH> location.location.nearby_airports </PATH>", "<PATH> aviation.airport.serves </PATH>", "<PATH> travel.transportation.mode_of_transportation <SEP> travel.transport_terminus.travel_destinations_served </PATH>"], "scores": [-0.006384835112839937, -0.028298858553171158, -0.049424633383750916], "norm_scores": [0.34057652950286865, 0.3331943154335022, 0.32622915506362915]}}
{"id": "WebQTest-1293", "question": "where has the english language evolved from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where has the english language evolved from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.006244414485991001, -0.02562832459807396, -0.06079799681901932], "norm_scores": [0.3415638208389282, 0.3350067436695099, 0.3234294354915619]}}
{"id": "WebQTest-1294", "question": "what country did hitler invade that started ww2", "prediction": [["base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["base.culturalevent.event.entity_involved", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did hitler invade that started ww2 [/INST]", "raw_output": {"paths": ["<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.culturalevent.event.entity_involved <SEP> time.event.locations </PATH>"], "scores": [-0.028932513669133186, -0.031627871096134186, -0.03702700883150101], "norm_scores": [0.3345324397087097, 0.3336319923400879, 0.33183553814888]}}
{"id": "WebQTest-1295", "question": "what does matt damon play in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["tv.tv_program.program_creator"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["music.recording.featured_artists", "music.recording.releases"], ["film.actor.film", "film.performance.film"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["people.person.languages", "tv.tv_program.languages"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.film.written_by"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.languages", "film.film.language"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does matt damon play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.022547410801053047, -0.023020489141345024, -0.043975986540317535], "norm_scores": [0.3357590436935425, 0.33560025691986084, 0.32864075899124146]}}
{"id": "WebQTest-1296", "question": "what was scottie pippen known for", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.player"], ["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.player"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was scottie pippen known for [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.player </PATH>", "<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.player </PATH>"], "scores": [-0.01631534844636917, -0.026631614193320274, -0.03244366496801376], "norm_scores": [0.3362770974636078, 0.33282580971717834, 0.3308970332145691]}}
{"id": "WebQTest-1297", "question": "what continent does canada belong to", "prediction": [["base.locations.countries.continent"], ["location.location.containedby"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.contains"], ["base.locations.continents.countries_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent does canada belong to [/INST]", "raw_output": {"paths": ["<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.01769665628671646, -0.018553368747234344, -0.02723187394440174], "norm_scores": [0.33448687195777893, 0.3342004716396332, 0.3313126266002655]}}
{"id": "WebQTest-1298", "question": "what does george wilson do for a living", "prediction": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["fictional_universe.character_occupation.characters_with_this_occupation"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does george wilson do for a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> fictional_universe.character_occupation.characters_with_this_occupation </PATH>"], "scores": [-0.004863383714109659, -0.031466398388147354, -0.06623286753892899], "norm_scores": [0.34314459562301636, 0.3341362774372101, 0.32271915674209595]}}
{"id": "WebQTest-1300", "question": "what city does fulham play in", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["location.location.containedby"]], "ground_paths": [["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city does fulham play in [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.023304248228669167, -0.023622725158929825, -0.029065977782011032], "norm_scores": [0.33400845527648926, 0.3339020609855652, 0.33208951354026794]}}
{"id": "WebQTest-1301", "question": "what are fun things to do in sacramento ca", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are fun things to do in sacramento ca [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.015294765122234821, -0.01703668013215065, -0.024621356278657913], "norm_scores": [0.3345626890659332, 0.3339804410934448, 0.33145686984062195]}}
{"id": "WebQTest-1303", "question": "what other movies is josh hutcherson in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["film.producer.films_executive_produced"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies is josh hutcherson in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.017766429111361504, -0.035258449614048004, -0.037378985434770584], "norm_scores": [0.33746859431266785, 0.33161693811416626, 0.3309144675731659]}}
{"id": "WebQTest-1304", "question": "what was hitler the leader of", "prediction": [["organization.organization_founder.organizations_founded"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "ground_paths": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was hitler the leader of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.016868744045495987, -0.03556625917553902, -0.04069224372506142], "norm_scores": [0.3380737602710724, 0.3318113684654236, 0.33011484146118164]}}
{"id": "WebQTest-1305", "question": "what awards did marilyn monroe won", "prediction": [["award.award_nomination.award_nominee", "award.award_nomination.award"], ["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_honor.award_winner", "award.award_honor.award"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nomination.award"], ["award.award_nomination.award_nominee", "award.award_category.nominees"], ["people.person.profession", "award.award_discipline.awards_in_this_discipline"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_nominee.award_nominations", "award.award_category.nominees"], ["award.award_honor.award_winner", "award.award_honor.award"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what awards did marilyn monroe won [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.award </PATH>", "<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>"], "scores": [-0.019324639812111855, -0.02367517352104187, -0.02596341073513031], "norm_scores": [0.33455535769462585, 0.3331030309200287, 0.33234167098999023]}}
{"id": "WebQTest-1306", "question": "what kind of government does iran have after 1979", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does iran have after 1979 [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0043470230884850025, -0.022704139351844788, -0.13886800408363342], "norm_scores": [0.3501468002796173, 0.34377774596214294, 0.30607545375823975]}}
{"id": "WebQTest-1307", "question": "where did harper lee attend high school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did harper lee attend high school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.015815118327736855, -0.021361686289310455, -0.021448789164423943], "norm_scores": [0.3345767557621002, 0.3327261209487915, 0.33269715309143066]}}
{"id": "WebQTest-1308", "question": "what college did jrr tolkien go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"], ["education.education.student", "education.education.institution"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did jrr tolkien go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.014780417084693909, -0.023363757878541946, -0.031379904597997665], "norm_scores": [0.33613547682762146, 0.3332626521587372, 0.33060184121131897]}}
{"id": "WebQTest-1309", "question": "what art did wassily kandinsky do", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["book.author.school_or_movement"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what art did wassily kandinsky do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> book.author.school_or_movement </PATH>"], "scores": [-0.008316119201481342, -0.02697959542274475, -0.046348847448825836], "norm_scores": [0.33965185284614563, 0.3333715498447418, 0.3269765377044678]}}
{"id": "WebQTest-1310", "question": "who won the league cup in 2002", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"]], "ground_paths": [["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["sports.sports_award.award", "sports.sports_award_winner.awards"], ["sports.sports_championship.events", "sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the league cup in 2002 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>"], "scores": [-0.02139439433813095, -0.023174770176410675, -0.025743449106812477], "norm_scores": [0.33401453495025635, 0.33342039585113525, 0.332565039396286]}}
{"id": "WebQTest-1311", "question": "what was antoni gaudi inspired by", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["influence.influence_node.influenced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was antoni gaudi inspired by [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.010277656838297844, -0.013795720413327217, -0.07487200945615768], "norm_scores": [0.34083911776542664, 0.33964210748672485, 0.3195188343524933]}}
{"id": "WebQTest-1312", "question": "in what country do people speak danish", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in what country do people speak danish [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.010402003303170204, -0.019547542557120323, -0.04586053267121315], "norm_scores": [0.33828824758529663, 0.335208535194397, 0.3265032172203064]}}
{"id": "WebQTest-1313", "question": "who were demeter 's siblings", "prediction": [["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"]], "ground_paths": [["fictional_universe.romantic_involvement.partner", "fictional_universe.fictional_character.romantically_involved_with"], ["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["common.topic.notable_types", "fictional_universe.fictional_character.species"], ["fictional_universe.fictional_character.children", "fictional_universe.fictional_character.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were demeter 's siblings [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.fictional_character.siblings </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.siblings <SEP> fictional_universe.sibling_relationship_of_fictional_characters.siblings </PATH>"], "scores": [-0.021830791607499123, -0.024238869547843933, -0.025540148839354515], "norm_scores": [0.3340133726596832, 0.3332099914550781, 0.33277666568756104]}}
{"id": "WebQTest-1314", "question": "where should a family stay in paris", "prediction": [["travel.travel_destination.accommodation"], ["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["travel.travel_destination.accommodation"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where should a family stay in paris [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.accommodation </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.0036721567157655954, -0.03282679617404938, -0.09874816238880157], "norm_scores": [0.3471534848213196, 0.33717846870422363, 0.3156680166721344]}}
{"id": "WebQTest-1316", "question": "what was the name of benjamin franklin wife", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.marriage.spouse", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of benjamin franklin wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.02721976302564144, -0.0340372659265995, -0.042695529758930206], "norm_scores": [0.33581286668777466, 0.3335312306880951, 0.33065593242645264]}}
{"id": "WebQTest-1317", "question": "what the largest city in spain", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.aareas.schema.administrative_area.capital"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the largest city in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.capital </PATH>"], "scores": [-0.020656540989875793, -0.02601313591003418, -0.04713808000087738], "norm_scores": [0.3368677794933319, 0.33506813645362854, 0.32806408405303955]}}
{"id": "WebQTest-1319", "question": "who has pudge rodriguez played for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has pudge rodriguez played for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.011504805646836758, -0.01724836230278015, -0.029898623004555702], "norm_scores": [0.3360161781311035, 0.33409175276756287, 0.32989203929901123]}}
{"id": "WebQTest-1320", "question": "what is the milwaukee brewers stadium called", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], []], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the milwaukee brewers stadium called [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> </PATH>"], "scores": [-0.002700875047594309, -0.031992826610803604, -0.11363887041807175], "norm_scores": [0.3489028513431549, 0.3388310670852661, 0.312266081571579]}}
{"id": "WebQTest-1321", "question": "where did the gallipoli war take place", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the gallipoli war take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.008449315093457699, -0.020868739113211632, -0.09083700180053711], "norm_scores": [0.3438113033771515, 0.33956778049468994, 0.31662091612815857]}}
{"id": "WebQTest-1322", "question": "what other movies has tom hardy played in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.film.starring"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other movies has tom hardy played in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>"], "scores": [-0.02356915920972824, -0.03389662876725197, -0.03798145055770874], "norm_scores": [0.3360873758792877, 0.3326343297958374, 0.33127832412719727]}}
{"id": "WebQTest-1323", "question": "what to visit in atlanta", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in atlanta [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01537525374442339, -0.017754144966602325, -0.02775399759411812], "norm_scores": [0.3349722921848297, 0.3341763913631439, 0.33085134625434875]}}
{"id": "WebQTest-1324", "question": "what college did john nash go to", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "ground_paths": [["book.written_work.author", "education.dissertation.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did john nash go to [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> business.employment_tenure.person <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.016704900190234184, -0.01788334548473358, -0.03539497032761574], "norm_scores": [0.33553606271743774, 0.3351408839225769, 0.3293231129646301]}}
{"id": "WebQTest-1325", "question": "what language do maltese speak", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do maltese speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.016128096729516983, -0.029974479228258133, -0.03146086633205414], "norm_scores": [0.3365832567214966, 0.3319549262523651, 0.3314618468284607]}}
{"id": "WebQTest-1326", "question": "what did thomas edison do for the world", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.inventor.inventions"], ["law.inventor.us_patents"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did thomas edison do for the world [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.013750024139881134, -0.020275525748729706, -0.036122214049100876], "norm_scores": [0.33654487133026123, 0.3343559205532074, 0.32909923791885376]}}
{"id": "WebQTest-1327", "question": "where is shakira from", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is shakira from [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.01183298695832491, -0.018238600343465805, -0.05897848308086395], "norm_scores": [0.33926326036453247, 0.33709701895713806, 0.32363972067832947]}}
{"id": "WebQTest-1328", "question": "what did hayes do", "prediction": [["people.person.profession"], ["government.government_position_held.office_holder", "government.government_office_or_title.office_holders"], ["common.topic.notable_types"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did hayes do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_office_or_title.office_holders </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.006174210458993912, -0.04701748490333557, -0.04737288877367973], "norm_scores": [0.34251055121421814, 0.3288031220436096, 0.32868629693984985]}}
{"id": "WebQTest-1329", "question": "who played bilbo in the fellowship of the ring", "prediction": [["film.performance.character", "film.actor.film"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.film.starring", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played bilbo in the fellowship of the ring [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nomination.award_nominee </PATH>"], "scores": [-0.035629842430353165, -0.039846014231443405, -0.04195825383067131], "norm_scores": [0.3345058560371399, 0.33309850096702576, 0.33239564299583435]}}
{"id": "WebQTest-1330", "question": "what is the most populated state in united states", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.administrative_division.first_level_division_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most populated state in united states [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.008219661191105843, -0.027820207178592682, -0.03541022166609764], "norm_scores": [0.3385508358478546, 0.3319796621799469, 0.3294694721698761]}}
{"id": "WebQTest-1331", "question": "who is my state senator arizona", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["base.aareas.schema.administrative_area.capital", "people.person.place_of_birth"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.capital", "people.deceased_person.place_of_death"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.place_of_birth"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["location.country.first_level_divisions", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is my state senator arizona [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.022840281948447227, -0.027488140389323235, -0.029013492166996002], "norm_scores": [0.33453670144081116, 0.33298540115356445, 0.332477867603302]}}
{"id": "WebQTest-1332", "question": "when does the vietnam war end", "prediction": [["time.event.included_in_event"], ["time.event.includes_event"], ["time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when does the vietnam war end [/INST]", "raw_output": {"paths": ["<PATH> time.event.included_in_event </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.015972303226590157, -0.03162011504173279, -0.03229498490691185], "norm_scores": [0.33689501881599426, 0.33166438341140747, 0.33144062757492065]}}
{"id": "WebQTest-1333", "question": "where does the tennessee river go", "prediction": [["geography.river.mouth"], ["location.location.partially_containedby"], ["location.location.containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the tennessee river go [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012651365250349045, -0.034057412296533585, -0.03714517876505852], "norm_scores": [0.33845239877700806, 0.33128446340560913, 0.3302631378173828]}}
{"id": "WebQTest-1334", "question": "who did tom hanks play in apollo 13", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.gender", "people.person.gender"], ["spaceflight.astronaut.missions"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did tom hanks play in apollo 13 [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.022892894223332405, -0.027947349473834038, -0.03130945935845375], "norm_scores": [0.33483147621154785, 0.3331433832645416, 0.3320251703262329]}}
{"id": "WebQTest-1335", "question": "what language do they in ghana", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they in ghana [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.005666361190378666, -0.035231005400419235, -0.0414712019264698], "norm_scores": [0.34063464403152466, 0.330711305141449, 0.32865405082702637]}}
{"id": "WebQTest-1336", "question": "what is utah 's state capitol", "prediction": [["base.aareas.schema.administrative_area.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["base.biblioness.bibs_location.city"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is utah 's state capitol [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.019390715286135674, -0.02623896859586239, -0.02908434346318245], "norm_scores": [0.335173636674881, 0.33288612961769104, 0.33194029331207275]}}
{"id": "WebQTest-1337", "question": "what all did ben franklin invent", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["law.inventor.us_patents"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all did ben franklin invent [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> law.inventor.us_patents </PATH>"], "scores": [-0.0066435569897294044, -0.025291787460446358, -0.07925422489643097], "norm_scores": [0.3434670567512512, 0.3371213376522064, 0.31941160559654236]}}
{"id": "WebQTest-1339", "question": "what team does jeremy lin play for 2013", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does jeremy lin play for 2013 [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014522230252623558, -0.017828889191150665, -0.02209184318780899], "norm_scores": [0.3345423936843872, 0.333437979221344, 0.3320195972919464]}}
{"id": "WebQTest-1340", "question": "what is steve nash nationality", "prediction": [["people.person.nationality"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.country"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.country"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is steve nash nationality [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.schemastaging.athlete_extra.salary <SEP> base.schemastaging.athlete_salary.country </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.country </PATH>"], "scores": [-0.002132145222276449, -0.05723772943019867, -0.05946870148181915], "norm_scores": [0.34594157338142395, 0.3273939788341522, 0.32666441798210144]}}
{"id": "WebQTest-1341", "question": "who are the u s senators of pennsylvania", "prediction": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_children", "people.deceased_person.place_of_burial"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["location.location.containedby", "people.deceased_person.place_of_death"], ["location.location.containedby", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.containedby", "people.person.place_of_birth"], ["base.aareas.schema.administrative_area.administrative_parent", "people.deceased_person.place_of_death"], ["base.aareas.schema.administrative_area.administrative_children", "people.person.place_of_birth"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["base.aareas.schema.administrative_area.administrative_children", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.place_of_birth"], ["location.location.containedby", "people.deceased_person.place_of_burial"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the u s senators of pennsylvania [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.024065040051937103, -0.027643699198961258, -0.028528304770588875], "norm_scores": [0.33422747254371643, 0.3330335021018982, 0.3327390253543854]}}
{"id": "WebQTest-1342", "question": "what kind of language does france speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language does france speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014427768997848034, -0.021951353177428246, -0.026492705568671227], "norm_scores": [0.33551281690597534, 0.33299803733825684, 0.3314891755580902]}}
{"id": "WebQTest-1343", "question": "what degree did martin luther king earn", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.degree"], ["education.education.student", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degree did martin luther king earn [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.004947103559970856, -0.022971905767917633, -0.05233320593833923], "norm_scores": [0.3406161665916443, 0.33453163504600525, 0.3248521685600281]}}
{"id": "WebQTest-1344", "question": "what two states is st louis in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.time_zones"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what two states is st louis in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.time_zones </PATH>"], "scores": [-0.003588386345654726, -0.05769450217485428, -0.06163189187645912], "norm_scores": [0.34590819478034973, 0.3276897668838501, 0.32640203833580017]}}
{"id": "WebQTest-1348", "question": "what team did peyton manning 's dad play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did peyton manning 's dad play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.00816336553543806, -0.016894487664103508, -0.03772852569818497], "norm_scores": [0.337589830160141, 0.33465513586997986, 0.32775503396987915]}}
{"id": "WebQTest-1350", "question": "what did jesse owens won", "prediction": [["olympics.olympic_athlete.medals_won", "olympics.olympic_medal_honor.event"], ["olympics.olympic_medal_honor.medalist", "olympics.olympic_medal_honor.event"], ["olympics.olympic_athlete.medals_won", "olympics.olympic_event_competition.medalists"]], "ground_paths": [["award.award_honor.award_winner", "award.award_category.winners"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jesse owens won [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> olympics.olympic_medal_honor.medalist <SEP> olympics.olympic_medal_honor.event </PATH>", "<PATH> olympics.olympic_athlete.medals_won <SEP> olympics.olympic_event_competition.medalists </PATH>"], "scores": [-0.019191090017557144, -0.020405111834406853, -0.03775254264473915], "norm_scores": [0.33552584052085876, 0.33511874079704285, 0.3293554186820984]}}
{"id": "WebQTest-1351", "question": "what is the name of the currency used in peru", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in peru [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.00865147914737463, -0.017143489792943, -0.11837051063776016], "norm_scores": [0.3463047444820404, 0.34337639808654785, 0.31031882762908936]}}
{"id": "WebQTest-1353", "question": "what are the four nations of the united kingdom", "prediction": [["base.aareas.schema.administrative_area.administrative_children"], ["location.administrative_division.country"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["base.biblioness.bibs_location.country"], ["location.administrative_division.country"], ["base.aareas.schema.administrative_area.administrative_children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the four nations of the united kingdom [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.010998675599694252, -0.02357194572687149, -0.024221064522862434], "norm_scores": [0.3362056612968445, 0.3320048749446869, 0.331789493560791]}}
{"id": "WebQTest-1354", "question": "who is judy garland father", "prediction": [["people.person.parents"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is judy garland father [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.007811674382537603, -0.02327880635857582, -0.09965570271015167], "norm_scores": [0.3451966941356659, 0.33989855647087097, 0.3149048089981079]}}
{"id": "WebQTest-1355", "question": "what school did robert f kennedy go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did robert f kennedy go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.015307647176086903, -0.02112523652613163, -0.02554972656071186], "norm_scores": [0.3351195752620697, 0.3331756293773651, 0.3317047655582428]}}
{"id": "WebQTest-1356", "question": "what drug killed marilyn monroe", "prediction": [["people.cause_of_death.people"], ["people.deceased_person.cause_of_death"], ["medicine.disease.parent_disease"]], "ground_paths": [["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what drug killed marilyn monroe [/INST]", "raw_output": {"paths": ["<PATH> people.cause_of_death.people </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.parent_disease </PATH>"], "scores": [-0.007087848149240017, -0.023252196609973907, -0.05756913498044014], "norm_scores": [0.3407460153102875, 0.3352823555469513, 0.3239716589450836]}}
{"id": "WebQTest-1357", "question": "where is the fukushima daiichi nuclear power station", "prediction": [["location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the fukushima daiichi nuclear power station [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.005113596562296152, -0.04154553636908531, -0.04547277092933655], "norm_scores": [0.3419191241264343, 0.3296865224838257, 0.3283943235874176]}}
{"id": "WebQTest-1358", "question": "what is los angeles california time zone", "prediction": [["location.location.time_zones"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is los angeles california time zone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.142575729318196e-05, -0.16209879517555237, -0.1841023564338684], "norm_scores": [0.372824490070343, 0.3170376420021057, 0.31013786792755127]}}
{"id": "WebQTest-1359", "question": "what did nicolas cage name his son", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.gender"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did nicolas cage name his son [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.011065280996263027, -0.016084561124444008, -0.11946991831064224], "norm_scores": [0.3457506597042084, 0.3440195918083191, 0.3102297782897949]}}
{"id": "WebQTest-1361", "question": "what currency should take to mexico", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency should take to mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> travel.tour_operator.travel_destinations <SEP> travel.travel_destination.tour_operators </PATH>"], "scores": [-0.007631081156432629, -0.032428987324237823, -0.05000222846865654], "norm_scores": [0.34082919359207153, 0.3324812650680542, 0.32668957114219666]}}
{"id": "WebQTest-1362", "question": "who is the state governor of florida", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["location.administrative_division.first_level_division_of", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the state governor of florida [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.013754787854850292, -0.023100100457668304, -0.025655528530478477], "norm_scores": [0.3356979787349701, 0.33257541060447693, 0.33172664046287537]}}
{"id": "WebQTest-1363", "question": "who was jessica simpson married too", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.marriage.spouse"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["people.person.languages", "people.person.languages"], ["base.popstra.celebrity.infidelity_participant", "base.popstra.celebrity.infidelity_victim"], ["people.marriage.spouse", "people.person.spouse_s"], ["broadcast.artist.content", "broadcast.artist.content"], ["music.artist.album", "music.album.featured_artists"], ["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["music.artist.genre", "music.artist.genre"], ["award.award_nominee.award_nominations", "award.award_nomination.award_nominee"], ["celebrities.romantic_relationship.celebrity", "celebrities.celebrity.sexual_relationships"], ["base.saturdaynightlive.snl_episode.host", "base.saturdaynightlive.snl_host.episodes_hosted"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was jessica simpson married too [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.spouse </PATH>"], "scores": [-0.02354157716035843, -0.03265692666172981, -0.04449690505862236], "norm_scores": [0.3366789221763611, 0.3336239457130432, 0.3296971321105957]}}
{"id": "WebQTest-1364", "question": "what do they speak in scotland", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they speak in scotland [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.014377252198755741, -0.01607876643538475, -0.038233958184719086], "norm_scores": [0.3361654579639435, 0.33559393882751465, 0.3282405436038971]}}
{"id": "WebQTest-1365", "question": "what did john dalton study", "prediction": [["people.person.profession"], ["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did john dalton study [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>"], "scores": [-0.012494313530623913, -0.023829493671655655, -0.03485298529267311], "norm_scores": [0.3370841443538666, 0.33328482508659363, 0.3296310305595398]}}
{"id": "WebQTest-1366", "question": "what songs do zac brown band sing", "prediction": [["music.recording.artist"], ["music.artist.album"], ["music.album.artist"]], "ground_paths": [["music.featured_artist.recordings"], ["music.artist.album"], ["music.album.artist"], ["music.recording.featured_artists"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["music.music_video.artist"], ["music.recording.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs do zac brown band sing [/INST]", "raw_output": {"paths": ["<PATH> music.recording.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.album.artist </PATH>"], "scores": [-0.014578744769096375, -0.029673337936401367, -0.05054875463247299], "norm_scores": [0.33901891112327576, 0.3339399993419647, 0.3270411193370819]}}
{"id": "WebQTest-1367", "question": "where is glastonbury england", "prediction": [["location.location.containedby"], ["location.location.contains"], ["time.event.locations"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is glastonbury england [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.011499958112835884, -0.016960328444838524, -0.09103311598300934], "norm_scores": [0.34268853068351746, 0.34082239866256714, 0.3164891004562378]}}
{"id": "WebQTest-1368", "question": "who formed the american federation of labor", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "people.person.nationality"]], "ground_paths": [["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who formed the american federation of labor [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.geographic_scope <SEP> people.person.nationality </PATH>"], "scores": [-0.0014496619114652276, -0.051156509667634964, -0.07917437702417374], "norm_scores": [0.3476172387599945, 0.3307606875896454, 0.3216221034526825]}}
{"id": "WebQTest-1369", "question": "what to visit in denver", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in denver [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013289211317896843, -0.024294055998325348, -0.02461625263094902], "norm_scores": [0.335819274187088, 0.3321439027786255, 0.33203691244125366]}}
{"id": "WebQTest-1370", "question": "where did thomas hobbes live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death"]], "ground_paths": [["influence.influence_node.influenced", "people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did thomas hobbes live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.026176350191235542, -0.030956776812672615, -0.032557401806116104], "norm_scores": [0.3345745801925659, 0.3329789936542511, 0.332446426153183]}}
{"id": "WebQTest-1373", "question": "where was john f kennedy when he got shot", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was john f kennedy when he got shot [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.004186050035059452, -0.050828833132982254, -0.052166979759931564], "norm_scores": [0.34392860531806946, 0.32825517654418945, 0.3278162479400635]}}
{"id": "WebQTest-1374", "question": "what type of government does italy have 2011", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does italy have 2011 [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009025049395859241, -0.013568676076829433, -0.14194193482398987], "norm_scores": [0.3483101427555084, 0.34673115611076355, 0.3049587309360504]}}
{"id": "WebQTest-1376", "question": "what province is montreal canada located", "prediction": [["location.province.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.state"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what province is montreal canada located [/INST]", "raw_output": {"paths": ["<PATH> location.province.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.state </PATH>"], "scores": [-0.02461160533130169, -0.027763523161411285, -0.03112562745809555], "norm_scores": [0.33440786600112915, 0.33335551619529724, 0.3322366178035736]}}
{"id": "WebQTest-1378", "question": "who does kellan lutz play in prom night", "prediction": [["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["film.actor.film", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does kellan lutz play in prom night [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.023336442187428474, -0.024230623617768288, -0.030263857915997505], "norm_scores": [0.33420196175575256, 0.3339032530784607, 0.33189478516578674]}}
{"id": "WebQTest-1379", "question": "what did franz liszt do to earn a living", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["music.artist.genre"]], "ground_paths": [["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did franz liszt do to earn a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> music.artist.genre </PATH>"], "scores": [-0.001005207421258092, -0.060386452823877335, -0.09603829681873322], "norm_scores": [0.3506692051887512, 0.33045223355293274, 0.31887856125831604]}}
{"id": "WebQTest-1380", "question": "what was nikola tesla known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["law.inventor.inventions"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was nikola tesla known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> law.inventor.inventions </PATH>"], "scores": [-0.0022270160261541605, -0.0498679019510746, -0.06891738623380661], "norm_scores": [0.3461452126502991, 0.33004119992256165, 0.3238135874271393]}}
{"id": "WebQTest-1381", "question": "what is the zip code for princeton nj", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.citytown.postal_codes"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for princeton nj [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0003029091458301991, -0.07935161143541336, -0.11845943331718445], "norm_scores": [0.3555489778518677, 0.32852545380592346, 0.3159255385398865]}}
{"id": "WebQTest-1382", "question": "what other countries border argentina", "prediction": [["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains", "location.location.partially_contains"], ["government.form_of_government.countries", "location.country.form_of_government"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["organization.organization_founder.organizations_founded", "organization.organization_founder.organizations_founded"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["time.event.locations", "time.event.locations"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["film.film.featured_film_locations", "media_common.netflix_title.netflix_genres"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["location.location.contains", "location.location.containedby"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["base.locations.continents.counties_within", "base.locations.counties_parishes_and_boroughs.coninent"], ["location.location.partially_containedby", "location.location.partially_contains"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["location.location.partially_containedby", "location.location.containedby"], ["location.location.events", "location.location.events"], ["location.location.contains", "location.location.contains"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.containedby", "location.location.contains_major_portion_of"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["biology.animal_breed.place_of_origin", "biology.animal_breed.place_of_origin"], ["location.location.containedby", "location.location.containedby"], ["travel.tour_operator.travel_destinations", "travel.travel_destination.tour_operators"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.languages_spoken", "location.country.official_language"], ["time.event.locations", "location.location.events"], ["organization.organization_founder.organizations_founded", "organization.organization.founders"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["location.location.primarily_containedby", "location.location.containedby"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["geography.river.basin_countries", "location.location.partially_contains"], ["organization.organization.founders", "organization.organization.founders"], ["biology.animal_breed.place_of_origin", "biology.breed_origin.breeds_originating_here"], ["location.location.events", "time.event.locations"], ["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["sports.multi_event_tournament.participating_countries", "sports.multi_event_tournament.participating_countries"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"], ["language.human_language.main_country", "language.language_family.geographic_distribution"], ["book.book_subject.works", "book.book_subject.works"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["film.film.featured_film_locations", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other countries border argentina [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby <SEP> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_containedby <SEP> location.location.partially_contains </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.039262086153030396, -0.042544737458229065, -0.04518818110227585], "norm_scores": [0.3343571126461029, 0.33326131105422974, 0.33238154649734497]}}
{"id": "WebQTest-1383", "question": "what language does people in iceland speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does people in iceland speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.016346624121069908, -0.017038637772202492, -0.04070494323968887], "norm_scores": [0.33610689640045166, 0.33587437868118286, 0.32801878452301025]}}
{"id": "WebQTest-1384", "question": "what is the predominant religion in israel", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.quotations", "people.person.religion"]], "ground_paths": [["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["people.person.nationality", "people.person.religion"], ["symbols.namesake.named_after", "religion.religion.notable_figures"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.quotations", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the predominant religion in israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>"], "scores": [-0.00400257483124733, -0.02800814062356949, -0.05128179118037224], "norm_scores": [0.3412851095199585, 0.3331899344921112, 0.3255249261856079]}}
{"id": "WebQTest-1385", "question": "who has coached the carolina panthers", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.historical_coaching_staff", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"]], "ground_paths": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"], ["american_football.football_team.historical_coaching_staff", "american_football.football_coach.coaching_history"], ["sports.sports_team.sport", "sports.sports_team_coach.sports_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has coached the carolina panthers [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_team.historical_coaching_staff <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sports_team_coach.sports_coached </PATH>"], "scores": [-0.0064935097470879555, -0.03281189501285553, -0.0345027931034565], "norm_scores": [0.33939674496650696, 0.3305808901786804, 0.3300223648548126]}}
{"id": "WebQTest-1387", "question": "what time zone is toronto gmt", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.containedby"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time zone is toronto gmt [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.00020610805950127542, -0.07604563236236572, -0.1699153333902359], "norm_scores": [0.3608967959880829, 0.33453866839408875, 0.3045644760131836]}}
{"id": "WebQTest-1388", "question": "what country did toussaint l'ouverture help free", "prediction": [["people.person.nationality"], ["base.culturalevent.event.entity_involved"], ["base.activism.activist.area_of_activism"]], "ground_paths": [["military.military_service.military_person", "military.armed_force.personnel"], ["military.military_service.military_person", "military.military_service.military_force"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did toussaint l'ouverture help free [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>"], "scores": [-0.017964515835046768, -0.026631811633706093, -0.04777191951870918], "norm_scores": [0.3376094400882721, 0.3346959054470062, 0.3276946544647217]}}
{"id": "WebQTest-1389", "question": "what jobs did robert burns have", "prediction": [["people.person.profession"], ["influence.influence_node.influenced", "people.person.profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jobs did robert burns have [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> influence.influence_node.influenced <SEP> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0024529490619897842, -0.05531508848071098, -0.0636875107884407], "norm_scores": [0.3461269438266754, 0.32830512523651123, 0.32556793093681335]}}
{"id": "WebQTest-1390", "question": "who won utah attorney general", "prediction": [["base.aareas.schema.administrative_area.administrative_children", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["base.aareas.schema.administrative_area.administrative_parent", "people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["base.biblioness.bibs_location.city", "people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won utah attorney general [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.01672343723475933, -0.02596832998096943, -0.028010042384266853], "norm_scores": [0.33561837673187256, 0.3325299024581909, 0.33185169100761414]}}
{"id": "WebQTest-1392", "question": "what the language spoken in indonesia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the language spoken in indonesia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.015498040243983269, -0.023929137736558914, -0.025701699778437614], "norm_scores": [0.33540698885917664, 0.3325910270214081, 0.33200201392173767]}}
{"id": "WebQTest-1394", "question": "who coaches the carolina panthers", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["sports.sports_team.sport", "sports.sport.team_coaches"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who coaches the carolina panthers [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> sports.sports_team.sport <SEP> sports.sport.team_coaches </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.006448057480156422, -0.03582696244120598, -0.037519875913858414], "norm_scores": [0.340083509683609, 0.33023756742477417, 0.3296789824962616]}}
{"id": "WebQTest-1395", "question": "what other countries does south africa share borders with", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains", "location.location.partially_contains"], ["film.film.featured_film_locations", "film.film_location.featured_in_films"], ["government.form_of_government.countries", "location.country.form_of_government"], ["base.uncommon.topic.exceptions", "base.uncommon.topic.exceptions"], ["location.country.languages_spoken", "language.human_language.main_country"], ["time.event.locations", "time.event.locations"], ["location.location.containedby", "base.locations.continents.countries_within"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["film.film.featured_film_locations", "film.film.subjects"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["location.location.partially_containedby", "geography.river.basin_countries"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.country.languages_spoken", "location.country.languages_spoken"], ["location.location.containedby", "base.locations.countries.continent"], ["location.country.official_language", "location.country.official_language"], ["location.imports_and_exports.exported_to", "location.imports_and_exports.imported_from"], ["geography.river.basin_countries", "location.location.partially_containedby"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.country.official_language", "language.human_language.countries_spoken_in"], ["location.location.containedby", "location.location.containedby"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.country.official_language", "language.human_language.main_country"], ["time.event.locations", "location.location.events"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.multi_event_tournament.participating_countries"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["geography.river.basin_countries", "geography.river.basin_countries"], ["organization.organization.founders", "organization.organization.founders"], ["location.adjoining_relationship.adjoins", "location.location.adjoin_s"], ["location.statistical_region.places_imported_from", "location.statistical_region.places_exported_to"], ["location.country.form_of_government", "location.country.form_of_government"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.currency_used", "location.country.currency_used"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.location.adjoin_s", "location.adjoining_relationship.adjoins"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what other countries does south africa share borders with [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.032295625656843185, -0.03363692760467529, -0.04331080615520477], "norm_scores": [0.33470508456230164, 0.33425647020339966, 0.3310384750366211]}}
{"id": "WebQTest-1396", "question": "who played darth vader at the end of return of the jedi", "prediction": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played darth vader at the end of return of the jedi [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>"], "scores": [-0.022297019138932228, -0.025899948552250862, -0.031549759209156036], "norm_scores": [0.3347623944282532, 0.33355844020843506, 0.33167919516563416]}}
{"id": "WebQTest-1398", "question": "what movies did joan crawford play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.film.starring"]], "ground_paths": [["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["people.deceased_person.place_of_death", "film.film.featured_film_locations"], ["common.topic.notable_types", "common.topic.notable_types"], ["film.actor.film", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["film.person_or_entity_appearing_in_film.films", "film.personal_film_appearance.film"], ["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"], ["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did joan crawford play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>"], "scores": [-0.024031007662415504, -0.034470513463020325, -0.03800076246261597], "norm_scores": [0.3360505998134613, 0.332560658454895, 0.3313886821269989]}}
{"id": "WebQTest-1399", "question": "what products does the united states export", "prediction": [["location.statistical_region.major_exports", "location.imports_exports_by_industry.industry"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what products does the united states export [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.major_exports <SEP> location.imports_exports_by_industry.industry </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>"], "scores": [-0.023023108020424843, -0.026370679959654808, -0.037188880145549774], "norm_scores": [0.33527883887290955, 0.334158331155777, 0.33056285977363586]}}
{"id": "WebQTest-1402", "question": "what is the kentucky state bird", "prediction": [["government.governmental_jurisdiction.official_symbols", "location.symbol_of_administrative_division.official_symbol_of"], ["location.offical_symbol_variety.symbols_of_this_kind", "location.symbol_of_administrative_division.official_symbol_of"], ["location.location_symbol_relationship.administrative_division", "location.location_symbol_relationship.symbol"]], "ground_paths": [["location.location_symbol_relationship.administrative_division", "location.symbol_of_administrative_division.official_symbol_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the kentucky state bird [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.official_symbols <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.offical_symbol_variety.symbols_of_this_kind <SEP> location.symbol_of_administrative_division.official_symbol_of </PATH>", "<PATH> location.location_symbol_relationship.administrative_division <SEP> location.location_symbol_relationship.symbol </PATH>"], "scores": [-0.013696297071874142, -0.022612715139985085, -0.02882646583020687], "norm_scores": [0.3360094428062439, 0.3330267369747162, 0.3309638202190399]}}
{"id": "WebQTest-1403", "question": "who was canadian prime minister in 1993", "prediction": [["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "ground_paths": [["government.government_office_or_title.office_holders", "government.politician.government_positions_held"], ["government.government_position_held.office_position_or_title", "government.government_position_held.office_holder"], ["base.firsts.first_achievement.achievement", "base.firsts.first_achievement.first"], ["government.governmental_jurisdiction.government_positions", "people.person.nationality"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was canadian prime minister in 1993 [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_positions <SEP> people.person.nationality </PATH>", "<PATH> government.government_office_or_title.office_holders <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.office_position_or_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.007056404370814562, -0.01741519197821617, -0.029369201511144638], "norm_scores": [0.3369693458080292, 0.3334967792034149, 0.3295338749885559]}}
{"id": "WebQTest-1405", "question": "what there to do in salt lake city utah", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what there to do in salt lake city utah [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01463992614299059, -0.01487522292882204, -0.03003818541765213], "norm_scores": [0.3350662589073181, 0.33498743176460266, 0.3299463391304016]}}
{"id": "WebQTest-1408", "question": "who is the current ohio state senator", "prediction": [["government.political_district.representatives", "government.government_position_held.office_holder"], ["location.country.first_level_divisions", "people.person.nationality"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current ohio state senator [/INST]", "raw_output": {"paths": ["<PATH> government.political_district.representatives <SEP> government.government_position_held.office_holder </PATH>", "<PATH> location.country.first_level_divisions <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.02340313047170639, -0.026991916820406914, -0.027837499976158142], "norm_scores": [0.3342253565788269, 0.33302807807922363, 0.33274656534194946]}}
{"id": "WebQTest-1409", "question": "what are the major languages spoken in the united kingdom", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the major languages spoken in the united kingdom [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.015436383895576, -0.022378183901309967, -0.023937279358506203], "norm_scores": [0.3350512981414795, 0.33273351192474365, 0.33221516013145447]}}
{"id": "WebQTest-1410", "question": "what did st. nicholas do in his life", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"], ["base.saints.saint.venerated_in"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did st. nicholas do in his life [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.saints.saint.venerated_in </PATH>"], "scores": [-0.016853487119078636, -0.03705538064241409, -0.03804127871990204], "norm_scores": [0.33794790506362915, 0.33118921518325806, 0.3308628797531128]}}
{"id": "WebQTest-1414", "question": "what happened to pope john paul ii", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.deceased_people"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what happened to pope john paul ii [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.deceased_people </PATH>"], "scores": [-0.0006466670893132687, -0.06532228738069534, -0.11545191705226898], "norm_scores": [0.3534928858280182, 0.33135414123535156, 0.31515294313430786]}}
{"id": "WebQTest-1416", "question": "what was franklin pierce famous for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"]], "ground_paths": [["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was franklin pierce famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_office_or_title.office_holders </PATH>"], "scores": [-0.011205941438674927, -0.032094791531562805, -0.03800323233008385], "norm_scores": [0.33865171670913696, 0.33165106177330017, 0.32969725131988525]}}
{"id": "WebQTest-1417", "question": "when was the last time the boston bruins went to the stanley cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.runner_up"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the boston bruins went to the stanley cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.011904596351087093, -0.01438601128757, -0.028735104948282242], "norm_scores": [0.3354767858982086, 0.3346453607082367, 0.3298777937889099]}}
{"id": "WebQTest-1418", "question": "who is meredith gray married to in real life", "prediction": [["fictional_universe.fictional_character.married_to", "fictional_universe.marriage_of_fictional_characters.spouses"], ["fictional_universe.fictional_character.married_to", "fictional_universe.fictional_character.married_to"], ["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is meredith gray married to in real life [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.marriage_of_fictional_characters.spouses </PATH>", "<PATH> fictional_universe.fictional_character.married_to <SEP> fictional_universe.fictional_character.married_to </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.021064316853880882, -0.026199236512184143, -0.033846188336610794], "norm_scores": [0.3353254199028015, 0.3336079716682434, 0.3310666084289551]}}
{"id": "WebQTest-1419", "question": "what inspired michelangelo buonarroti", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired michelangelo buonarroti [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.005500911269336939, -0.020212886855006218, -0.09722893685102463], "norm_scores": [0.3450954556465149, 0.34005555510520935, 0.31484895944595337]}}
{"id": "WebQTest-1420", "question": "what does monsanto own", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization.child"], ["business.business_operation.industry"]], "ground_paths": [["business.business_operation.industry"], ["business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does monsanto own [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>", "<PATH> business.business_operation.industry </PATH>"], "scores": [-0.03359198942780495, -0.04036577418446541, -0.040797606110572815], "norm_scores": [0.33488839864730835, 0.3326275944709778, 0.3324839770793915]}}
{"id": "WebQTest-1421", "question": "who did woody harrelson play on cheers", "prediction": [["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["people.person.gender", "fictional_universe.fictional_character.gender"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.character"], ["people.person.profession", "fictional_universe.fictional_character.occupation"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did woody harrelson play on cheers [/INST]", "raw_output": {"paths": ["<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>"], "scores": [-0.024976957589387894, -0.025904184207320213, -0.026048066094517708], "norm_scores": [0.3335554003715515, 0.33324626088142395, 0.33319833874702454]}}
{"id": "WebQTest-1422", "question": "what language do they speak in sweden yahoo", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in sweden yahoo [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013500450178980827, -0.018361298367381096, -0.034200578927993774], "norm_scores": [0.3361724615097046, 0.33454233407974243, 0.3292851746082306]}}
{"id": "WebQTest-1423", "question": "what language did they speak in ghana", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did they speak in ghana [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.004499020054936409, -0.0372399240732193, -0.046499039977788925], "norm_scores": [0.34168651700019836, 0.33068057894706726, 0.327632874250412]}}
{"id": "WebQTest-1424", "question": "what form of government do we have in the philippines", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["form.form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government do we have in the philippines [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> form.form_of_government.countries </PATH>"], "scores": [-0.008194567635655403, -0.01474471390247345, -0.1574939638376236], "norm_scores": [0.35028937458992004, 0.34800243377685547, 0.3017081618309021]}}
{"id": "WebQTest-1425", "question": "what colleges did mitt romney attend", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.nationality", "base.biblioness.bibs_location.country"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"], ["people.person.religion", "religion.religion.organizations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what colleges did mitt romney attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01745431497693062, -0.02333705686032772, -0.02525958977639675], "norm_scores": [0.334855854511261, 0.3328917622566223, 0.3322523832321167]}}
{"id": "WebQTest-1426", "question": "where is ibrahim afellay from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is ibrahim afellay from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0048032887279987335, -0.039603836834430695, -0.0475379042327404], "norm_scores": [0.3420015275478363, 0.3303043842315674, 0.3276941180229187]}}
{"id": "WebQTest-1427", "question": "who was the secretary of state when andrew jackson was president", "prediction": [["book.written_work.subjects", "people.person.nationality"], ["government.government_position_held.appointed_by", "government.government_position_held.office_holder"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"]], "ground_paths": [["people.person.profession", "common.topic.notable_types"], ["government.political_appointer.appointees", "government.government_position_held.office_holder"], ["government.us_vice_president.to_president"], ["people.person.gender", "people.person.gender"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["government.government_position_held.appointed_by", "government.politician.government_positions_held"], ["government.government_position_held.appointed_by", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the secretary of state when andrew jackson was president [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.appointed_by <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.02193540893495083, -0.024019518867135048, -0.028430158272385597], "norm_scores": [0.33428671956062317, 0.33359071612358093, 0.3321226239204407]}}
{"id": "WebQTest-1428", "question": "who does ron stoppable 's voice", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does ron stoppable 's voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.021679101511836052, -0.02928765118122101, -0.03846336901187897], "norm_scores": [0.3360467851161957, 0.3334996700286865, 0.3304535448551178]}}
{"id": "WebQTest-1429", "question": "what currency is accepted in new zealand", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency is accepted in new zealand [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.008035746403038502, -0.020353097468614578, -0.11946336925029755], "norm_scores": [0.3469434082508087, 0.3426961898803711, 0.3103603720664978]}}
{"id": "WebQTest-1432", "question": "what places are in the west midlands", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.location.containedby", "common.topic.image"], ["location.location.containedby"], []], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what places are in the west midlands [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.005206895526498556, -0.037915803492069244, -0.049201179295778275], "norm_scores": [0.3419061005115509, 0.33090364933013916, 0.32719025015830994]}}
{"id": "WebQTest-1433", "question": "what to do with kids in asheville nc", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with kids in asheville nc [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.008313439786434174, -0.019725538790225983, -0.038615334779024124], "norm_scores": [0.3379742503166199, 0.3341391682624817, 0.3278866112232208]}}
{"id": "WebQTest-1434", "question": "who is in the american league in baseball", "prediction": [["baseball.baseball_team.league"], ["baseball.baseball_league.teams"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["sports.sports_league_participation.league", "sports.sports_league_participation.team"], ["sports.sports_league.teams", "sports.sports_league_participation.team"], ["sports.sports_league.teams", "sports.sports_team.league"], ["baseball.baseball_league.teams", "sports.sports_team.previously_known_as"], ["sports.sports_league.sport", "sports.sports_team.sport"], ["sports.sports_league.sport", "sports.defunct_sports_team.sport"], ["baseball.baseball_team.league", "sports.sports_team.previously_known_as"], ["baseball.baseball_league.teams", "sports.defunct_sports_team.later_known_as"], ["baseball.baseball_team.league", "sports.defunct_sports_team.later_known_as"], ["baseball.baseball_team.league"], ["sports.sports_league_participation.league", "sports.sports_team.league"], ["baseball.baseball_league.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in the american league in baseball [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.league </PATH>", "<PATH> baseball.baseball_league.teams </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.010785474441945553, -0.02640555240213871, -0.04424671456217766], "norm_scores": [0.33880001306533813, 0.33354905247688293, 0.32765093445777893]}}
{"id": "WebQTest-1435", "question": "who did kurt warner play 4", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"]], "ground_paths": [["american_football.player_rushing_statistics.player", "american_football.player_rushing_statistics.team"], ["american_football.football_player.passing", "american_football.player_passing_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_passing_statistics.player", "american_football.player_passing_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did kurt warner play 4 [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.player_rushing_statistics.player <SEP> american_football.player_rushing_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>"], "scores": [-0.0243129450827837, -0.028953231871128082, -0.033547453582286835], "norm_scores": [0.3348761796951294, 0.3333258628845215, 0.3317979872226715]}}
{"id": "WebQTest-1436", "question": "who is dirk nowitzki wife", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is dirk nowitzki wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03935667499899864, -0.0408586710691452, -0.042240407317876816], "norm_scores": [0.33382076025009155, 0.3333197236061096, 0.33285948634147644]}}
{"id": "WebQTest-1441", "question": "where all did picasso live", "prediction": [["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["influence.influence_node.influenced_by", "people.deceased_person.place_of_death"], ["influence.influence_node.influenced", "people.person.nationality"], ["people.person.nationality", "location.location.containedby"], ["people.person.parents", "people.person.nationality"], ["people.place_lived.person", "people.place_lived.location"], ["influence.influence_node.influenced_by", "people.person.nationality"], ["people.person.places_lived", "people.place_lived.location"], ["film.film_subject.films", "film.film.country"], ["symbols.name_source.namesakes", "location.location.containedby"], ["book.written_work.subjects", "book.written_work.subjects"], ["people.person.place_of_birth"], ["symbols.namesake.named_after", "film.film.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where all did picasso live [/INST]", "raw_output": {"paths": ["<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.02412964217364788, -0.0286380797624588, -0.03072303533554077], "norm_scores": [0.3345679044723511, 0.33306291699409485, 0.33236920833587646]}}
{"id": "WebQTest-1442", "question": "what currency shall i take to croatia", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency shall i take to croatia [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.007852227427065372, -0.021082136780023575, -0.07787898927927017], "norm_scores": [0.34255653619766235, 0.3380543887615204, 0.3193890452384949]}}
{"id": "WebQTest-1443", "question": "what team is raul ibanez on", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is raul ibanez on [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.01575593091547489, -0.019869141280651093, -0.024320637807250023], "norm_scores": [0.334742933511734, 0.3333688974380493, 0.33188819885253906]}}
{"id": "WebQTest-1445", "question": "what tv series did mark harmon star in", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["people.person.languages", "tv.tv_program.languages"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"]], "ground_paths": [["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["people.person.languages", "tv.tv_program.languages"], ["people.person.languages", "film.film.language"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what tv series did mark harmon star in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> people.person.languages <SEP> tv.tv_program.languages </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.tv_program.regular_cast </PATH>"], "scores": [-0.02059043198823929, -0.025235380977392197, -0.031234629452228546], "norm_scores": [0.33503326773643494, 0.33348068594932556, 0.3314860463142395]}}
{"id": "WebQTest-1446", "question": "what year did jimmy the rev sullivan die", "prediction": [["people.deceased_person.place_of_death"], ["people.deceased_person.cause_of_death"], ["people.person.deceased_place_of_death"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did jimmy the rev sullivan die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.person.deceased_place_of_death </PATH>"], "scores": [-0.0007703775772824883, -0.06424712389707565, -0.07902463525533676], "norm_scores": [0.3492564857006073, 0.3277758061885834, 0.3229677081108093]}}
{"id": "WebQTest-1447", "question": "who plays anakin skywalker in star wars 1", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["film.film.starring", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.performance.character", "film.performance.actor"], ["film.film.language", "people.person.languages"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays anakin skywalker in star wars 1 [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.017272593453526497, -0.02209489792585373, -0.023349612951278687], "norm_scores": [0.33454543352127075, 0.33293604850769043, 0.3325185477733612]}}
{"id": "WebQTest-1448", "question": "where does michelle pfeiffer live now", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["people.place_lived.person", "people.place_lived.location"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth", "location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does michelle pfeiffer live now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>"], "scores": [-0.016227437183260918, -0.03217153251171112, -0.032500263303518295], "norm_scores": [0.3369225561618805, 0.33159321546554565, 0.33148422837257385]}}
{"id": "WebQTest-1449", "question": "what was laura ingalls wilder famous for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["book.written_work.author"]], "ground_paths": [["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was laura ingalls wilder famous for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.003334871493279934, -0.046434275805950165, -0.055168770253658295], "norm_scores": [0.3439614474773407, 0.3294518291950226, 0.3265867531299591]}}
{"id": "WebQTest-1452", "question": "where is dwight howard now", "prediction": [["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.team"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is dwight howard now [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.player_statistics <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.019761504605412483, -0.02191500924527645, -0.024012839421629906], "norm_scores": [0.3340452313423157, 0.33332666754722595, 0.3326281011104584]}}
{"id": "WebQTest-1453", "question": "who is nick lachey married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.languages", "people.person.languages"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is nick lachey married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.020807012915611267, -0.035528991371393204, -0.04413377866148949], "norm_scores": [0.33757224678993225, 0.3326388895511627, 0.3297888934612274]}}
{"id": "WebQTest-1454", "question": "where did madonna grew up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did madonna grew up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.007610570173710585, -0.03404724970459938, -0.03908370062708855], "norm_scores": [0.3397977650165558, 0.3309323489665985, 0.3292698264122009]}}
{"id": "WebQTest-1455", "question": "what capital city of brazil", "prediction": [["location.country.capital"], ["location.location.containedby"], ["location.administrative_division.country"]], "ground_paths": [["base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what capital city of brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.003867098595947027, -0.043828945606946945, -0.08055479824542999], "norm_scores": [0.34637969732284546, 0.332810640335083, 0.32080963253974915]}}
{"id": "WebQTest-1456", "question": "what college did john stockton go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth", "location.location.contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did john stockton go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.013244634494185448, -0.02086327224969864, -0.025208711624145508], "norm_scores": [0.3355122208595276, 0.33296576142311096, 0.33152204751968384]}}
{"id": "WebQTest-1457", "question": "what kind of currency does mexico use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does mexico use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.002357610035687685, -0.04585280641913414, -0.0740416944026947], "norm_scores": [0.3462289869785309, 0.33149248361587524, 0.32227852940559387]}}
{"id": "WebQTest-1458", "question": "who owns chrysler corporation 2011", "prediction": [["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization_relationship.child", "organization.organization_relationship.parent"], ["organization.organization.parent", "organization.organization.child"]], "ground_paths": [["organization.organization_relationship.child", "organization.organization_relationship.parent"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization.parent", "organization.organization.child"], ["business.business_operation.industry", "business.business_operation.industry"], ["organization.organization.parent", "organization.organization_relationship.parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who owns chrysler corporation 2011 [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry <SEP> business.business_operation.industry </PATH>", "<PATH> organization.organization_relationship.child <SEP> organization.organization_relationship.parent </PATH>", "<PATH> organization.organization.parent <SEP> organization.organization.child </PATH>"], "scores": [-0.016701731830835342, -0.023501647636294365, -0.027399806305766106], "norm_scores": [0.3352799713611603, 0.3330078125, 0.3317122161388397]}}
{"id": "WebQTest-1459", "question": "what are some books that mark twain wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"]], "ground_paths": [["book.book_subject.works"], ["film.film_story_contributor.film_story_credits"], ["book.written_work.author"], ["film.film.story_by"], ["book.book_edition.author_editor"], ["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some books that mark twain wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.01122031919658184, -0.02566777914762497, -0.03293924406170845], "norm_scores": [0.3373623788356781, 0.3325234055519104, 0.3301142156124115]}}
{"id": "WebQTest-1460", "question": "what was gregor mendel contribution", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["symbols.namesake.named_after"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was gregor mendel contribution [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.00883896928280592, -0.044698841869831085, -0.05627527832984924], "norm_scores": [0.3426480293273926, 0.33057841658592224, 0.3267735540866852]}}
{"id": "WebQTest-1461", "question": "who has ray allen dated", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"]], "ground_paths": [["people.marriage.spouse", "people.person.spouse_s"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who has ray allen dated [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> base.popstra.celebrity.dated <SEP> base.popstra.celebrity.dated </PATH>"], "scores": [-0.019520927220582962, -0.03097517415881157, -0.04358642175793648], "norm_scores": [0.33728715777397156, 0.33344581723213196, 0.32926705479621887]}}
{"id": "WebQTest-1462", "question": "who is running for vice president with barack obama 2012", "prediction": [["government.election_campaign.election", "government.election_campaign.candidate"], ["government.election_campaign.candidate", "government.election_campaign.candidate"], ["government.election_campaign.election", "government.politician.election_campaigns"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is running for vice president with barack obama 2012 [/INST]", "raw_output": {"paths": ["<PATH> government.election_campaign.election <SEP> government.election_campaign.candidate </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.candidate </PATH>", "<PATH> government.election_campaign.election <SEP> government.politician.election_campaigns </PATH>"], "scores": [-0.016473110765218735, -0.029746487736701965, -0.030814478173851967], "norm_scores": [0.3364086449146271, 0.3319728970527649, 0.3316185176372528]}}
{"id": "WebQTest-1463", "question": "what language do people from the uk speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do people from the uk speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016178498044610023, -0.016401777043938637, -0.040296584367752075], "norm_scores": [0.33602747321128845, 0.3359524607658386, 0.3280200660228729]}}
{"id": "WebQTest-1464", "question": "what to do with my kids in toronto", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"], ["film.film_festival.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do with my kids in toronto [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013933837413787842, -0.014157908037304878, -0.033538930118083954], "norm_scores": [0.33552974462509155, 0.3354545533657074, 0.32901570200920105]}}
{"id": "WebQTest-1465", "question": "what kind of government is south korea", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government is south korea [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009636612609028816, -0.014530385844409466, -0.13522587716579437], "norm_scores": [0.34757277369499207, 0.34587597846984863, 0.3065512180328369]}}
{"id": "WebQTest-1467", "question": "who did ben franklin marry", "prediction": [["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.marriage.spouse", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.deceased_person.place_of_death", "people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here", "people.place_of_interment.interred_here"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did ben franklin marry [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.032036300748586655, -0.032618384808301926, -0.0436093807220459], "norm_scores": [0.334681898355484, 0.3344871401786804, 0.3308309316635132]}}
{"id": "WebQTest-1468", "question": "what sri lanka capital", "prediction": [["location.country.capital"], ["location.location.containedby"], ["base.biblioness.bibs_location.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sri lanka capital [/INST]", "raw_output": {"paths": ["<PATH> location.country.capital </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>"], "scores": [-0.009136169217526913, -0.026978125795722008, -0.04344303160905838], "norm_scores": [0.3391450047492981, 0.33314764499664307, 0.32770735025405884]}}
{"id": "WebQTest-1470", "question": "what kind of government does poland have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does poland have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011322496458888054, -0.01286420039832592, -0.14748495817184448], "norm_scores": [0.34829121828079224, 0.34775468707084656, 0.3039540648460388]}}
{"id": "WebQTest-1471", "question": "what famous people came from delaware", "prediction": [["people.person.place_of_birth"], ["location.location.containedby", "people.person.nationality"], ["location.location.containedby", "people.person.place_of_birth"]], "ground_paths": [["location.location.people_born_here"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what famous people came from delaware [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> location.location.containedby <SEP> people.person.place_of_birth </PATH>"], "scores": [-0.01344416942447424, -0.03877827525138855, -0.0469832606613636], "norm_scores": [0.33990466594696045, 0.33140164613723755, 0.3286936283111572]}}
{"id": "WebQTest-1472", "question": "when did romney become governor", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did romney become governor [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>"], "scores": [-0.02190721407532692, -0.030417565256357193, -0.031536273658275604], "norm_scores": [0.3353517949581146, 0.33250996470451355, 0.33213818073272705]}}
{"id": "WebQTest-1473", "question": "what currency does cyprus use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does cyprus use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.0027429910842329264, -0.03958288952708244, -0.11197199672460556], "norm_scores": [0.34960687160491943, 0.3369617760181427, 0.3134313225746155]}}
{"id": "WebQTest-1475", "question": "what type of guitar does johnny depp play", "prediction": [["music.group_member.instruments_played"], ["music.instrument.family"], ["music.guitar.guitarists"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of guitar does johnny depp play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.instrument.family </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.01382804848253727, -0.03182437643408775, -0.0353296622633934], "norm_scores": [0.33773598074913025, 0.33171236515045166, 0.3305516541004181]}}
{"id": "WebQTest-1477", "question": "what episode of how i met your mother is carrie underwood on", "prediction": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"]], "ground_paths": [["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what episode of how i met your mother is carrie underwood on [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> tv.regular_tv_appearance.actor <SEP> tv.regular_tv_appearance.series </PATH>"], "scores": [-0.01777052693068981, -0.02706061489880085, -0.02949194423854351], "norm_scores": [0.3356718420982361, 0.33256787061691284, 0.3317602872848511]}}
{"id": "WebQTest-1478", "question": "who are the 9 justices of the supreme court 2011", "prediction": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"], ["book.written_work.subjects", "book.written_work.author"]], "ground_paths": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the 9 justices of the supreme court 2011 [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects <SEP> book.author.works_written </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.author </PATH>"], "scores": [-0.007695190608501434, -0.032812099903821945, -0.033064983785152435], "norm_scores": [0.33896636962890625, 0.3305585980415344, 0.33047500252723694]}}
{"id": "WebQTest-1479", "question": "who is dustin johnson the golfer", "prediction": [["people.person.profession"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.player"], ["sports.sports_league_draft_pick.player", "sports.drafted_athlete.drafted"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is dustin johnson the golfer [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.player </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.drafted_athlete.drafted </PATH>"], "scores": [-0.0022542073857039213, -0.06021856144070625, -0.06509341299533844], "norm_scores": [0.34688758850097656, 0.3273521363735199, 0.32576024532318115]}}
{"id": "WebQTest-1480", "question": "who did cher have a son with", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["music.artist.genre", "music.artist.genre"]], "ground_paths": [["music.composer.compositions", "music.composer.compositions"], ["base.popstra.celebrity.breakup", "base.popstra.breakup.participant"], ["people.person.spouse_s", "people.person.spouse_s"], ["music.album.artist", "music.album.artist"], ["base.popstra.celebrity.breakup", "base.popstra.celebrity.breakup"], ["award.award_nomination.award_nominee", "award.award_nominee.award_nominations"], ["music.artist.label", "music.artist.label"], ["award.award_nominee.award_nominations", "award.award_nominee.award_nominations"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["music.artist.concert_tours", "music.concert_tour.artist"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["music.artist.genre", "music.artist.genre"], ["people.person.children", "people.person.children"], ["base.americancomedy.double_act.comic", "base.americancomedy.double_act.straight_man"], ["award.award_winner.awards_won", "award.award_honor.award_winner"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cher have a son with [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>"], "scores": [-0.02171414904296398, -0.033546559512615204, -0.039424166083335876], "norm_scores": [0.3366228938102722, 0.33266332745552063, 0.33071377873420715]}}
{"id": "WebQTest-1481", "question": "what city is the university of oregon state in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is the university of oregon state in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.0025601459201425314, -0.05490194633603096, -0.05736694857478142], "norm_scores": [0.3453429043292999, 0.3277319669723511, 0.3269250988960266]}}
{"id": "WebQTest-1482", "question": "who played carmen cortez in spy kids", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.film", "film.performance.actor"]], "ground_paths": [["film.performance.film", "film.performance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played carmen cortez in spy kids [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.performance.film <SEP> film.performance.actor </PATH>"], "scores": [-0.024919142946600914, -0.032803818583488464, -0.03294099494814873], "norm_scores": [0.3351030647754669, 0.33247125148773193, 0.33242565393447876]}}
{"id": "WebQTest-1483", "question": "what is the political system in germany", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the political system in germany [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009792189113795757, -0.0147812869399786, -0.12806746363639832], "norm_scores": [0.34680378437042236, 0.34507787227630615, 0.3081183433532715]}}
{"id": "WebQTest-1484", "question": "what prompted shakespeare to write poetry", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced_by", "influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what prompted shakespeare to write poetry [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced_by <SEP> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.016557874158024788, -0.02589448168873787, -0.02879057452082634], "norm_scores": [0.3357340097427368, 0.3326139450073242, 0.33165207505226135]}}
{"id": "WebQTest-1485", "question": "what is the currency in turkey called", "prediction": [["finance.currency.countries_used"], ["finance.currency.countries_formerly_used"], ["location.country.currency_used"]], "ground_paths": [["location.country.currency_formerly_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency in turkey called [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>", "<PATH> location.country.currency_used </PATH>"], "scores": [-0.01753307320177555, -0.018808847293257713, -0.03197478875517845], "norm_scores": [0.3350771963596344, 0.3346499800682068, 0.3302728831768036]}}
{"id": "WebQTest-1486", "question": "what school did mark zuckerberg attend", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did mark zuckerberg attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.016076726838946342, -0.017484882846474648, -0.022345371544361115], "norm_scores": [0.3341861963272095, 0.33371594548225403, 0.3320978283882141]}}
{"id": "WebQTest-1487", "question": "where does kate middleton live 2012", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.place_of_birth"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does kate middleton live 2012 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.place_of_birth </PATH>"], "scores": [-0.011224188841879368, -0.021011995151638985, -0.03500548377633095], "norm_scores": [0.3370681405067444, 0.3337850868701935, 0.32914677262306213]}}
{"id": "WebQTest-1488", "question": "what type of art does marc chagall do", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.associated_artists"]], "ground_paths": [["book.school_or_movement.associated_authors"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art does marc chagall do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.associated_artists </PATH>"], "scores": [-0.0016534756869077682, -0.037518441677093506, -0.11156570166349411], "norm_scores": [0.34956681728363037, 0.33725178241729736, 0.31318140029907227]}}
{"id": "WebQTest-1489", "question": "what character did billy d williams play in star wars", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"], ["tv.regular_tv_appearance.actor", "tv.tv_character.appeared_in_tv_program"], ["film.performance.actor", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what character did billy d williams play in star wars [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.021615590900182724, -0.022004898637533188, -0.02857985347509384], "norm_scores": [0.33414971828460693, 0.33401966094970703, 0.3318306803703308]}}
{"id": "WebQTest-1490", "question": "what language is spoken in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is spoken in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.007247903384268284, -0.029189495369791985, -0.04790087416768074], "norm_scores": [0.3403143584728241, 0.3329286277294159, 0.32675701379776]}}
{"id": "WebQTest-1493", "question": "what school did wilfred owen go to", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school did wilfred owen go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.015124648809432983, -0.024812744930386543, -0.025111041963100433], "norm_scores": [0.33552297949790955, 0.33228808641433716, 0.3321889638900757]}}
{"id": "WebQTest-1494", "question": "who is kevin jonas married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["people.person.nationality", "people.person.nationality"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is kevin jonas married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.028372086584568024, -0.03469426929950714, -0.039568230509757996], "norm_scores": [0.33528199791908264, 0.33316895365715027, 0.3315490782260895]}}
{"id": "WebQTest-1495", "question": "where does airtran airways fly", "prediction": [["aviation.airline.airports_served", "aviation.airline_airport_presence.airport"], ["aviation.airline_airport_presence.airline", "aviation.airline_airport_presence.airport"], ["aviation.airport.hub_for", "aviation.airport.serves"]], "ground_paths": [["aviation.airline_airport_presence.airline", "aviation.airport.airlines"], ["aviation.airline.airports_served", "aviation.airline_airport_presence.airport"], ["aviation.airport.focus_city_for"], ["aviation.airline.focus_cities"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does airtran airways fly [/INST]", "raw_output": {"paths": ["<PATH> aviation.airline.airports_served <SEP> aviation.airline_airport_presence.airport </PATH>", "<PATH> aviation.airline_airport_presence.airline <SEP> aviation.airline_airport_presence.airport </PATH>", "<PATH> aviation.airport.hub_for <SEP> aviation.airport.serves </PATH>"], "scores": [-0.019378241151571274, -0.024311579763889313, -0.03905436024069786], "norm_scores": [0.33606722950935364, 0.3344133794307709, 0.3295193612575531]}}
{"id": "WebQTest-1496", "question": "where does the city of paris get its name", "prediction": [["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["location.location.containedby"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the city of paris get its name [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01087381411343813, -0.015345833264291286, -0.07505624741315842], "norm_scores": [0.3409046232700348, 0.33938348293304443, 0.31971192359924316]}}
{"id": "WebQTest-1499", "question": "who did sandra bullock marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.infidelity.victim", "base.popstra.celebrity.infidelity_perpetrator"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["base.popstra.infidelity.participant", "base.popstra.infidelity.perpetrator"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did sandra bullock marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.01826191321015358, -0.03786994144320488, -0.03928469866514206], "norm_scores": [0.3378629982471466, 0.331302672624588, 0.3308343291282654]}}
{"id": "WebQTest-1500", "question": "where is olympic national park located", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["travel.travel_destination.tourist_attractions"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is olympic national park located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.010920648463070393, -0.0252551157027483, -0.029000286012887955], "norm_scores": [0.3369441628456116, 0.3321487009525299, 0.33090707659721375]}}
{"id": "WebQTest-1501", "question": "where is the un based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["book.book_subject.works", "base.reviews.review.mentions"], ["organization.organization_founder.organizations_founded", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the un based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.013447596691548824, -0.04192458838224411, -0.04199234023690224], "norm_scores": [0.3396989107131958, 0.3301617503166199, 0.3301393687725067]}}
{"id": "WebQTest-1502", "question": "what city and state is yale located in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city and state is yale located in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.0015992290573194623, -0.060534488409757614, -0.06870628893375397], "norm_scores": [0.34748005867004395, 0.32759302854537964, 0.32492688298225403]}}
{"id": "WebQTest-1503", "question": "where did thomas paine die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did thomas paine die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.0003736462676897645, -0.09544497728347778, -0.10127493739128113], "norm_scores": [0.3554506301879883, 0.3232141435146332, 0.3213352859020233]}}
{"id": "WebQTest-1504", "question": "where was barack hussein obama sr born", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was barack hussein obama sr born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0007504572276957333, -0.06308755278587341, -0.10274669528007507], "norm_scores": [0.35179072618484497, 0.33053064346313477, 0.31767863035202026]}}
{"id": "WebQTest-1506", "question": "where is the home stadium of the new england patriots", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["location.location.containedby"]], "ground_paths": [["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the home stadium of the new england patriots [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.003722952678799629, -0.02642916887998581, -0.10575447231531143], "norm_scores": [0.3471558094024658, 0.3393620550632477, 0.3134821057319641]}}
{"id": "WebQTest-1507", "question": "who was kate chopin", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was kate chopin [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0004757303686346859, -0.07987243682146072, -0.09912075847387314], "norm_scores": [0.3533896505832672, 0.32641664147377014, 0.3201937675476074]}}
{"id": "WebQTest-1508", "question": "when did bill clinton go to college", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did bill clinton go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.015039396472275257, -0.02113243378698826, -0.021229511126875877], "norm_scores": [0.33469951152801514, 0.33266639709472656, 0.3326340913772583]}}
{"id": "WebQTest-1510", "question": "what are staffordshire terriers", "prediction": [["biology.animal_breed.breed_of"], ["biology.breed_of_breed.breed_of"], ["biology.breed_breed.breed_of"]], "ground_paths": [["biology.animal_breed.breed_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are staffordshire terriers [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.breed_of </PATH>", "<PATH> biology.breed_of_breed.breed_of </PATH>", "<PATH> biology.breed_breed.breed_of </PATH>"], "scores": [-0.007146866526454687, -0.04078582674264908, -0.04838576540350914], "norm_scores": [0.34170272946357727, 0.33039939403533936, 0.32789790630340576]}}
{"id": "WebQTest-1513", "question": "what to do downtown sacramento", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do downtown sacramento [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.01654629595577717, -0.02020649053156376, -0.021925458684563637], "norm_scores": [0.33433836698532104, 0.3331168591976166, 0.33254474401474]}}
{"id": "WebQTest-1515", "question": "who is michael j fox parents", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced_by"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is michael j fox parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>"], "scores": [-0.011333382688462734, -0.01656109280884266, -0.14640092849731445], "norm_scores": [0.3486211895942688, 0.3468034267425537, 0.3045753240585327]}}
{"id": "WebQTest-1516", "question": "where did sanjay gupta go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.honorary_degree_recipient.honorary_degrees", "education.honorary_degree.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did sanjay gupta go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.017473701387643814, -0.01976434700191021, -0.027809517458081245], "norm_scores": [0.3347359299659729, 0.3339700400829315, 0.3312939703464508]}}
{"id": "WebQTest-1518", "question": "what countries are involved in the nafta", "prediction": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["common.topic.notable_types"]], "ground_paths": [["organization.organization_founder.organizations_founded", "people.person.nationality"], ["organization.membership_organization.members", "organization.organization_member.member_of"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["organization.organization_membership.organization", "organization.organization_member.member_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are involved in the nafta [/INST]", "raw_output": {"paths": ["<PATH> organization.membership_organization.members <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.011878933757543564, -0.02439418062567711, -0.041708335280418396], "norm_scores": [0.3380464017391205, 0.33384203910827637, 0.32811158895492554]}}
{"id": "WebQTest-1519", "question": "what continent is cayman islands located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what continent is cayman islands located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.008934494107961655, -0.021788224577903748, -0.07724553346633911], "norm_scores": [0.34232527017593384, 0.33795326948165894, 0.319721519947052]}}
{"id": "WebQTest-1520", "question": "what language speak in brazil", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language speak in brazil [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0011021546088159084, -0.0673006996512413, -0.07504379004240036], "norm_scores": [0.34908023476600647, 0.32671990990638733, 0.3241998553276062]}}
{"id": "WebQTest-1522", "question": "who does kroy biermann play football for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does kroy biermann play football for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.0202805045992136, -0.027488045394420624, -0.02768164500594139], "norm_scores": [0.3349584937095642, 0.332552969455719, 0.3324885666370392]}}
{"id": "WebQTest-1523", "question": "when is the last time the pittsburgh steelers won a superbowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when is the last time the pittsburgh steelers won a superbowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.011282104067504406, -0.01503327488899231, -0.034007664769887924], "norm_scores": [0.33627161383628845, 0.3350125551223755, 0.32871580123901367]}}
{"id": "WebQTest-1524", "question": "who invented islamic religion", "prediction": [["religion.religion.notable_figures"], ["religion.religion.founding_figures"], ["religion.belief.belief_of"]], "ground_paths": [["religion.religion.notable_figures"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who invented islamic religion [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.founding_figures </PATH>", "<PATH> religion.belief.belief_of </PATH>"], "scores": [-0.002672301372513175, -0.04214067384600639, -0.0670274943113327], "norm_scores": [0.3449498414993286, 0.3316003680229187, 0.3234497606754303]}}
{"id": "WebQTest-1525", "question": "who were we fighting in the gulf war", "prediction": [["military.military_combatant_group.conflict", "military.military_combatant.military_conflicts"], ["time.event.locations"], ["military.military_combatant_group.conflict", "military.military_combatant_group.combatants"]], "ground_paths": [["military.military_person.participated_in_conflicts", "people.person.nationality"], ["film.film.genre", "film.film.country"], ["time.event.includes_event", "base.culturalevent.event.entity_involved"], ["film.film.subjects", "film.film.country"], ["military.military_conflict.casualties", "military.military_combatant.casualties"], ["time.event.included_in_event", "base.culturalevent.event.entity_involved"], ["film.film_subject.films", "film.film.country"], ["military.casualties.military_conflict", "military.military_combatant.casualties"], ["time.event.locations"], ["book.book_subject.works", "media_common.netflix_title.netflix_genres"], ["base.culturalevent.event.entity_involved", "people.person.place_of_birth"], ["military.military_conflict.combatants", "military.military_combatant_group.combatants"], ["film.film_genre.films_in_this_genre", "film.film.country"], ["film.film_subject.films", "tv.tv_program.country_of_origin"], ["base.culturalevent.event.entity_involved", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were we fighting in the gulf war [/INST]", "raw_output": {"paths": ["<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant.military_conflicts </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> military.military_combatant_group.conflict <SEP> military.military_combatant_group.combatants </PATH>"], "scores": [-0.025022536516189575, -0.026784641668200493, -0.030020806938409805], "norm_scores": [0.3340846002101898, 0.33349645137786865, 0.33241894841194153]}}
{"id": "WebQTest-1526", "question": "how many languages do they speak in switzerland", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many languages do they speak in switzerland [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.015965357422828674, -0.022144664078950882, -0.022554989904165268], "norm_scores": [0.3347536027431488, 0.3326914608478546, 0.33255496621131897]}}
{"id": "WebQTest-1527", "question": "what political party does barack obama represent", "prediction": [["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "ground_paths": [["government.politician.election_campaigns", "government.election_campaign.party"], ["government.election_campaign.candidate", "government.election_campaign.party"], ["base.politicalconventions.political_convention.presidential_nominee", "base.politicalconventions.political_convention.party"], ["government.election.winner", "government.primary_election.party"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party does barack obama represent [/INST]", "raw_output": {"paths": ["<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.018351063132286072, -0.02088516764342785, -0.02417021058499813], "norm_scores": [0.33426183462142944, 0.3334158658981323, 0.332322359085083]}}
{"id": "WebQTest-1528", "question": "what movies does logan lerman play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["tv.tv_guest_role.actor", "tv.tv_guest_role.episodes_appeared_in"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies does logan lerman play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.016372257843613625, -0.037001390010118484, -0.037483274936676025], "norm_scores": [0.33798712491989136, 0.3310861587524414, 0.33092665672302246]}}
{"id": "WebQTest-1529", "question": "where did emperor constantine die", "prediction": [["people.deceased_person.place_of_death"], ["location.deceased_person.place_of_death"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did emperor constantine die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> location.deceased_person.place_of_death </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0006876565748825669, -0.07406260073184967, -0.08228305727243423], "norm_scores": [0.35076677799224854, 0.32595086097717285, 0.323282390832901]}}
{"id": "WebQTest-1531", "question": "where is mali located", "prediction": [["location.location.containedby"], ["base.locations.countries.continent"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mali located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.countries.continent </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.017050981521606445, -0.02275605872273445, -0.03906645253300667], "norm_scores": [0.33641305565834045, 0.3344992399215698, 0.3290877044200897]}}
{"id": "WebQTest-1533", "question": "what planet did luke skywalker came from", "prediction": [["fictional_universe.fictional_setting.characters_that_have_lived_here"], ["fictional_universe.fictional_character.place_of_birth"], ["fictional_universe.fictional_character.places_lived"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what planet did luke skywalker came from [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_setting.characters_that_have_lived_here </PATH>", "<PATH> fictional_universe.fictional_character.place_of_birth </PATH>", "<PATH> fictional_universe.fictional_character.places_lived </PATH>"], "scores": [-0.011888828128576279, -0.01944412849843502, -0.019962603226304054], "norm_scores": [0.3350721597671509, 0.33255013823509216, 0.33237773180007935]}}
{"id": "WebQTest-1534", "question": "who are the coaches for the oakland raiders", "prediction": [["american_football.football_historical_coach_position.team", "american_football.football_historical_coach_position.coach"], ["american_football.football_team.current_head_coach"], ["american_football.football_coach.current_team_head_coached"]], "ground_paths": [["american_football.football_coach.current_team_head_coached"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the coaches for the oakland raiders [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_historical_coach_position.team <SEP> american_football.football_historical_coach_position.coach </PATH>", "<PATH> american_football.football_team.current_head_coach </PATH>", "<PATH> american_football.football_coach.current_team_head_coached </PATH>"], "scores": [-0.015684086829423904, -0.02157500386238098, -0.025079933926463127], "norm_scores": [0.33503368496894836, 0.33306583762168884, 0.3319004774093628]}}
{"id": "WebQTest-1535", "question": "where was the first microsoft headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["base.engineering1.rfid_company.location", "base.biblioness.bibs_location.country"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the first microsoft headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.017799681052565575, -0.03444746881723404, -0.04258311167359352], "norm_scores": [0.337950736284256, 0.33237117528915405, 0.32967811822891235]}}
{"id": "WebQTest-1536", "question": "where did john steinbeck go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "base.biblioness.bibs_location.country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john steinbeck go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01699381321668625, -0.01758219301700592, -0.018425865098834038], "norm_scores": [0.3335578441619873, 0.3333616554737091, 0.333080530166626]}}
{"id": "WebQTest-1537", "question": "what did niels bohr discover about the atomic structure", "prediction": [["symbols.name_source.namesakes"], ["symbols.namesake.named_after"], ["base.nobelprizes.nobel_honor.nobel_prize_winner"]], "ground_paths": [["education.academic.advisees", "book.author.works_written"], ["base.nobelprizes.nobel_honor.nobel_prize_winner", "base.nobelprizes.nobel_honor.subject_area"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did niels bohr discover about the atomic structure [/INST]", "raw_output": {"paths": ["<PATH> symbols.name_source.namesakes </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.nobelprizes.nobel_honor.nobel_prize_winner </PATH>"], "scores": [-0.012884572148323059, -0.02384508028626442, -0.035211075097322464], "norm_scores": [0.33703848719596863, 0.3333645462989807, 0.32959696650505066]}}
{"id": "WebQTest-1538", "question": "what books did louis sachar wrote", "prediction": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "ground_paths": [["book.book_edition.author_editor"], ["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what books did louis sachar wrote [/INST]", "raw_output": {"paths": ["<PATH> book.book_edition.author_editor </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>"], "scores": [-0.012323436327278614, -0.025490330532193184, -0.02920929342508316], "norm_scores": [0.33668047189712524, 0.33227649331092834, 0.3310430645942688]}}
{"id": "WebQTest-1539", "question": "who is ryan reynolds married to 2012", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.spouse_s", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is ryan reynolds married to 2012 [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.01711925119161606, -0.029479527845978737, -0.03234313055872917], "norm_scores": [0.33640503883361816, 0.33227255940437317, 0.33132243156433105]}}
{"id": "WebQTest-1540", "question": "what is newt gingrich 's wife 's name", "prediction": [["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.profession", "common.topic.notable_types"], ["film.film.executive_produced_by", "film.film.executive_produced_by"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.religion", "people.person.religion"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is newt gingrich 's wife 's name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.023307889699935913, -0.036339171230793, -0.03919978439807892], "norm_scores": [0.33655449748039246, 0.3321972191333771, 0.33124828338623047]}}
{"id": "WebQTest-1541", "question": "what type of music did antonio vivaldi play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did antonio vivaldi play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0024114646948873997, -0.03815333917737007, -0.11227013170719147], "norm_scores": [0.3495464622974396, 0.33727365732192993, 0.3131799101829529]}}
{"id": "WebQTest-1542", "question": "what school does james franco go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what school does james franco go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.014680864289402962, -0.01680397242307663, -0.02321680821478367], "norm_scores": [0.33451759815216064, 0.33380812406539917, 0.3316743075847626]}}
{"id": "WebQTest-1543", "question": "what is samuel langhorne clemens well known for", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["fictional_universe.fictional_character.occupation"]], "ground_paths": [["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is samuel langhorne clemens well known for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>"], "scores": [-0.001531295827589929, -0.04566142335534096, -0.090752512216568], "norm_scores": [0.34825336933135986, 0.33321908116340637, 0.31852754950523376]}}
{"id": "WebQTest-1544", "question": "who was president after william henry harrison", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.person.gender", "people.person.gender"]], "ground_paths": [["government.government_office_or_title.jurisdiction", "people.person.nationality"], ["freebase.type_profile.equivalent_topic", "common.topic.notable_types"], ["government.government_position_held.office_position_or_title", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was president after william henry harrison [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.0152918491512537, -0.022163741290569305, -0.043516650795936584], "norm_scores": [0.3372315466403961, 0.33492207527160645, 0.32784631848335266]}}
{"id": "WebQTest-1546", "question": "where did rick santorum attend high school", "prediction": [["people.person.education", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rick santorum attend high school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.015617692843079567, -0.022639505565166473, -0.02708798460662365], "norm_scores": [0.3353906273841858, 0.33304381370544434, 0.33156558871269226]}}
{"id": "WebQTest-1547", "question": "what is the title of george bush 's new book", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"]], "ground_paths": [["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the title of george bush 's new book [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.009274973534047604, -0.024765925481915474, -0.04518921673297882], "norm_scores": [0.339057594537735, 0.33384573459625244, 0.3270966708660126]}}
{"id": "WebQTest-1548", "question": "where did antoine lavoisier die", "prediction": [["people.deceased_person.place_of_death"], ["deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did antoine lavoisier die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.00023539559333585203, -0.10299574583768845, -0.10688607394695282], "norm_scores": [0.35699203610420227, 0.32212933897972107, 0.3208785951137543]}}
{"id": "WebQTest-1550", "question": "what is the name of king george vi wife", "prediction": [["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"], ["people.person.parents", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["film.film.subjects", "film.film_subject.films"], ["people.family.members", "people.family.members"], ["people.deceased_person.place_of_burial", "people.deceased_person.place_of_burial"], ["people.person.religion", "people.person.religion"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of king george vi wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.03636325150728226, -0.03659447655081749, -0.03933378681540489], "norm_scores": [0.33368897438049316, 0.3336118459701538, 0.3326992392539978]}}
{"id": "WebQTest-1551", "question": "what year did seattle seahawks go to the super bowl", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship.events"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did seattle seahawks go to the super bowl [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship.events </PATH>"], "scores": [-0.01749655231833458, -0.022126076743006706, -0.024649111554026604], "norm_scores": [0.33464357256889343, 0.333097904920578, 0.33225855231285095]}}
{"id": "WebQTest-1553", "question": "where does sami yusuf live", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.place_of_birth"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does sami yusuf live [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.0243043415248394, -0.028337867930531502, -0.038238365203142166], "norm_scores": [0.33532997965812683, 0.33398014307022095, 0.3306898772716522]}}
{"id": "WebQTest-1554", "question": "what is the official language of brazil now", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the official language of brazil now [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.0004938131896778941, -0.07457724213600159, -0.09138690680265427], "norm_scores": [0.3519008457660675, 0.326773077249527, 0.32132604718208313]}}
{"id": "WebQTest-1555", "question": "what is the parliament of nepal called", "prediction": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_jurisdiction.governing_officials", "government.governmental_body.members"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.governmental_body"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the parliament of nepal called [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.governmental_body.members </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.governmental_body </PATH>"], "scores": [-0.017760619521141052, -0.01784713752567768, -0.02657310664653778], "norm_scores": [0.33432072401046753, 0.33429181575775146, 0.331387460231781]}}
{"id": "WebQTest-1556", "question": "when 's the last time the mets won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when 's the last time the mets won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.01114988885819912, -0.011468138545751572, -0.07829024642705917], "norm_scores": [0.3407450020313263, 0.34063658118247986, 0.31861838698387146]}}
{"id": "WebQTest-1557", "question": "where do florida panthers play", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do florida panthers play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.007073957473039627, -0.023937862366437912, -0.06463202834129333], "norm_scores": [0.3416065573692322, 0.33589401841163635, 0.32249948382377625]}}
{"id": "WebQTest-1558", "question": "who is john cena married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is john cena married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>"], "scores": [-0.019715318456292152, -0.03407130390405655, -0.03490931913256645], "norm_scores": [0.3366246819496155, 0.33182665705680847, 0.33154869079589844]}}
{"id": "WebQTest-1559", "question": "what to do san jose ca", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["film.film_festival.location"], ["location.neighborhood.neighborhood_of"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to do san jose ca [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.01629161275923252, -0.016566433012485504, -0.02803436666727066], "norm_scores": [0.33466628193855286, 0.3345743417739868, 0.3307593762874603]}}
{"id": "WebQTest-1560", "question": "who founded new york university", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["education.educational_institution.parent_institution", "people.person.parents"]], "ground_paths": [["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded new york university [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> people.person.parents </PATH>"], "scores": [-0.026281222701072693, -0.03117373399436474, -0.03534168004989624], "norm_scores": [0.33488500118255615, 0.33325058221817017, 0.33186447620391846]}}
{"id": "WebQTest-1561", "question": "who created arthur", "prediction": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.program_creator"], ["fictional_universe.fictional_character_creator.fictional_characters_created"]], "ground_paths": [["tv.tv_program.program_creator"], ["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who created arthur [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program.program_creator </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>"], "scores": [-0.00736306793987751, -0.03660829737782478, -0.04691055044531822], "norm_scores": [0.34101730585098267, 0.3311885595321655, 0.3277941048145294]}}
{"id": "WebQTest-1562", "question": "what countries constitute the eu", "prediction": [["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization.geographic_scope", "location.location.containedby"]], "ground_paths": [["organization.membership_organization.members", "organization.organization_member.member_of"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.district_represented"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["organization.organization_membership.organization", "organization.organization_membership.member"], ["organization.membership_organization.members", "organization.organization_membership.member"], ["government.government_position_held.jurisdiction_of_office", "government.political_district.representatives"], ["organization.organization_founder.organizations_founded"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["organization.organization.geographic_scope", "base.locations.countries.continent"], ["organization.organization.geographic_scope", "location.location.primarily_containedby"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.district_represented"], ["organization.organization_membership.organization", "organization.organization_member.member_of"], ["organization.organization.founders"], ["organization.organization.geographic_scope", "base.locations.continents.countries_within"], ["organization.organization.geographic_scope", "location.location.containedby"], ["base.industrystandards.standard_organization.standards", "base.aareas.coding_scheme.geo_scope"], ["government.governmental_jurisdiction.governing_officials", "government.political_district.representatives"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries constitute the eu [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_membership.organization <SEP> organization.organization_member.member_of </PATH>", "<PATH> organization.organization.geographic_scope <SEP> base.locations.continents.countries_within </PATH>", "<PATH> organization.organization.geographic_scope <SEP> location.location.containedby </PATH>"], "scores": [-0.029778221622109413, -0.031979162245988846, -0.03466448560357094], "norm_scores": [0.33412107825279236, 0.33338651061058044, 0.3324924409389496]}}
{"id": "WebQTest-1563", "question": "what are all the countries in north africa", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the countries in north africa [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.005325149744749069, -0.032529350370168686, -0.07032456248998642], "norm_scores": [0.34361541271209717, 0.33439362049102783, 0.3219910264015198]}}
{"id": "WebQTest-1564", "question": "what did bobby darin die of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bobby darin die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.001375391148030758, -0.04533899948000908, -0.1109224334359169], "norm_scores": [0.3504801392555237, 0.33540552854537964, 0.3141143023967743]}}
{"id": "WebQTest-1565", "question": "where does houston dynamo play", "prediction": [["sports.sports_facility.teams"], ["sports.sports_team.arena_stadium"], ["sports.sports_team.location"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does houston dynamo play [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_team.location </PATH>"], "scores": [-0.011535548605024815, -0.013856290839612484, -0.08444899320602417], "norm_scores": [0.3416043221950531, 0.34081247448921204, 0.3175831437110901]}}
{"id": "WebQTest-1566", "question": "what kind of guitar john mayer", "prediction": [["music.guitar.guitarists"], ["music.guitarist.guitars_played"], ["music.instrument.family"]], "ground_paths": [["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar john mayer [/INST]", "raw_output": {"paths": ["<PATH> music.guitar.guitarists </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.instrument.family </PATH>"], "scores": [-0.01359753031283617, -0.01696225069463253, -0.05235448107123375], "norm_scores": [0.3379948139190674, 0.33685946464538574, 0.32514578104019165]}}
{"id": "WebQTest-1567", "question": "what cancer did patrick swayze", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["people.deceased_person.cause_of_death"]], "ground_paths": [["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what cancer did patrick swayze [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.0016489307163283229, -0.04603508114814758, -0.05843186378479004], "norm_scores": [0.3446631133556366, 0.32969942688941956, 0.3256374001502991]}}
{"id": "WebQTest-1568", "question": "what university did obama graduated from", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what university did obama graduated from [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.014881519600749016, -0.015372825786471367, -0.026181360706686974], "norm_scores": [0.3346414864063263, 0.33447712659835815, 0.33088135719299316]}}
{"id": "WebQTest-1569", "question": "what jamaican money called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what jamaican money called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0005193472024984658, -0.07259652018547058, -0.13494351506233215], "norm_scores": [0.35654714703559875, 0.3317525386810303, 0.31170034408569336]}}
{"id": "WebQTest-1570", "question": "where is the arizona cardinals stadium located", "prediction": [["sports.sports_team.arena_stadium"], ["sports.sports_facility.teams"], ["sports.sports_team_location.teams"]], "ground_paths": [["sports.sports_team_location.teams"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the arizona cardinals stadium located [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.arena_stadium </PATH>", "<PATH> sports.sports_facility.teams </PATH>", "<PATH> sports.sports_team_location.teams </PATH>"], "scores": [-0.005799454636871815, -0.026404595002532005, -0.054080743342638016], "norm_scores": [0.34100931882858276, 0.3340546488761902, 0.3249360918998718]}}
{"id": "WebQTest-1571", "question": "what is sir isaac newton theory", "prediction": [["symbols.namesake.named_after"], ["symbols.name_source.namesakes"], ["law.invention.inventor"]], "ground_paths": [["symbols.namesake.named_after"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is sir isaac newton theory [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>", "<PATH> law.invention.inventor </PATH>"], "scores": [-0.007389246020466089, -0.023308226838707924, -0.07089903950691223], "norm_scores": [0.3421526849269867, 0.3367490768432617, 0.32109829783439636]}}
{"id": "WebQTest-1572", "question": "what should i do today in san francisco", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.neighborhood.neighborhood_of"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what should i do today in san francisco [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.015545598231256008, -0.019657770171761513, -0.02132980152964592], "norm_scores": [0.33443379402160645, 0.3330613374710083, 0.33250492811203003]}}
{"id": "WebQTest-1573", "question": "who influenced picasso art", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who influenced picasso art [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.012179560028016567, -0.012858961708843708, -0.11002103239297867], "norm_scores": [0.3441022038459778, 0.3438684940338135, 0.31202933192253113]}}
{"id": "WebQTest-1574", "question": "what is my timezone in pennsylvania", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is my timezone in pennsylvania [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-1.405597595294239e-05, -0.13348646461963654, -0.18869329988956451], "norm_scores": [0.36994510889053345, 0.3237210810184479, 0.3063338100910187]}}
{"id": "WebQTest-1575", "question": "what products and\\/or services does google offer customers", "prediction": [["organization.organization.sectors"], ["organization.organization_founder.organizations_founded"], ["organization.organization.sectors", "organization.organization.sectors"]], "ground_paths": [["business.company_product_relationship.company", "business.company_product_relationship.consumer_product"], ["computer.software.developer"], ["computer.software_developer.software"], ["business.company_product_relationship.company", "business.consumer_product.company"], ["computer.computer_manufacturer_brand.computer_models"], ["business.consumer_company.products", "business.company_product_relationship.consumer_product"], ["business.consumer_company.products"], ["business.consumer_company.products", "business.consumer_product.company"], ["internet.website.owner"], ["projects.project_participation.participant", "projects.project_participation.project"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what products and\\/or services does google offer customers [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.sectors </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.sectors <SEP> organization.organization.sectors </PATH>"], "scores": [-0.04385165125131607, -0.05084707587957382, -0.055315837264060974], "norm_scores": [0.3353869915008545, 0.3330489993095398, 0.3315640091896057]}}
{"id": "WebQTest-1577", "question": "who did cliff lee play for last year", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did cliff lee play for last year [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014614785090088844, -0.015020719729363918, -0.029230287298560143], "norm_scores": [0.33499884605407715, 0.3348628878593445, 0.33013826608657837]}}
{"id": "WebQTest-1580", "question": "what type of democracy is the united states", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of democracy is the united states [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>"], "scores": [-0.008613988757133484, -0.015523320063948631, -0.1612844169139862], "norm_scores": [0.3506892919540405, 0.3482746481895447, 0.3010360896587372]}}
{"id": "WebQTest-1582", "question": "what time in china hong kong", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.countries.continent"]], "ground_paths": [["time.time_zone.locations_in_this_time_zone"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what time in china hong kong [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.00588181521743536, -0.023242173716425896, -0.08574257791042328], "norm_scores": [0.34411153197288513, 0.33818918466567993, 0.31769922375679016]}}
{"id": "WebQTest-1583", "question": "what all movies has taylor lautner been in", "prediction": [["people.person.nationality", "film.film.country"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.nationality", "film.film.country"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what all movies has taylor lautner been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> award.award_nominee.award_nominations <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.02313583716750145, -0.029194602742791176, -0.029558507725596428], "norm_scores": [0.3347215950489044, 0.332699716091156, 0.3325786590576172]}}
{"id": "WebQTest-1584", "question": "what disease did bernie mac die of", "prediction": [["people.deceased_person.cause_of_death"], ["medicine.disease.notable_people_with_this_condition"], ["people.cause_of_death.people"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did bernie mac die of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.cause_of_death.people </PATH>"], "scores": [-0.004597736522555351, -0.03448833152651787, -0.040022965520620346], "norm_scores": [0.34062883257865906, 0.3305979073047638, 0.32877323031425476]}}
{"id": "WebQTest-1585", "question": "who plays spike spiegel", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["base.schemastaging.tv_character_extra.regular_dubbing_performances", "base.schemastaging.tv_star_dubbing_performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays spike spiegel [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.01680423691868782, -0.03461753576993942, -0.035453762859106064], "norm_scores": [0.3373969793319702, 0.3314400613307953, 0.3311629891395569]}}
{"id": "WebQTest-1586", "question": "who was queen isabella 's mother", "prediction": [["people.person.parents"], ["people.person.children"], ["influence.influence_node.influenced"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was queen isabella 's mother [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> influence.influence_node.influenced </PATH>"], "scores": [-0.009227847680449486, -0.019213564693927765, -0.1262224018573761], "norm_scores": [0.34726396203041077, 0.34381353855133057, 0.30892255902290344]}}
{"id": "WebQTest-1587", "question": "who does chris hemsworth have a baby with", "prediction": [["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.children", "people.person.parents"], ["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.children"], ["celebrities.friendship.friend", "celebrities.friendship.friend"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does chris hemsworth have a baby with [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.022342778742313385, -0.02414814382791519, -0.026675410568714142], "norm_scores": [0.33401548862457275, 0.3334130346775055, 0.33257144689559937]}}
{"id": "WebQTest-1589", "question": "where is the mts iceplex in winnipeg", "prediction": [["location.location.containedby"], ["travel.travel_destination.tourist_attractions"], ["location.location.contains"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the mts iceplex in winnipeg [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.01328995916992426, -0.026881488040089607, -0.029971014708280563], "norm_scores": [0.33670511841773987, 0.33215972781181335, 0.331135094165802]}}
{"id": "WebQTest-1590", "question": "what are the most popular religions in israel", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.quotations", "people.person.religion"]], "ground_paths": [["location.country.administrative_divisions", "religion.religion.sacred_sites"], ["film.film_location.featured_in_films", "media_common.netflix_title.netflix_genres"], ["people.person.nationality", "people.person.religion"], ["symbols.namesake.named_after", "religion.religion.notable_figures"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.quotations", "people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the most popular religions in israel [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.quotations <SEP> people.person.religion </PATH>"], "scores": [-0.0029481283854693174, -0.030850498005747795, -0.06040657311677933], "norm_scores": [0.34285974502563477, 0.333425372838974, 0.32371482253074646]}}
{"id": "WebQTest-1591", "question": "what was obama 's former occupation", "prediction": [["people.person.profession"], ["people.person.profession", "base.onephylogeny.type_of_thing.things_of_this_type"], ["people.person.profession", "people.profession.specializations"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was obama 's former occupation [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.person.profession <SEP> base.onephylogeny.type_of_thing.things_of_this_type </PATH>", "<PATH> people.person.profession <SEP> people.profession.specializations </PATH>"], "scores": [-0.007620671764016151, -0.03589213639497757, -0.03975764662027359], "norm_scores": [0.3400784134864807, 0.33059853315353394, 0.32932308316230774]}}
{"id": "WebQTest-1592", "question": "what are all the songs justin bieber wrote", "prediction": [["music.composer.compositions"], ["music.recording.artist"], ["music.composition.composer"]], "ground_paths": [["music.featured_artist.recordings"], ["music.artist.album"], ["music.lyricist.lyrics_written"], ["music.composition.composer"], ["music.album.artist"], ["music.recording.featured_artists"], ["music.composition.lyricist"], ["music.composer.compositions"], ["music.featured_artist.albums"], ["music.recording.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are all the songs justin bieber wrote [/INST]", "raw_output": {"paths": ["<PATH> music.composer.compositions </PATH>", "<PATH> music.recording.artist </PATH>", "<PATH> music.composition.composer </PATH>"], "scores": [-0.01679927483201027, -0.022484291344881058, -0.033991649746894836], "norm_scores": [0.3358764052391052, 0.33397236466407776, 0.3301512598991394]}}
{"id": "WebQTest-1593", "question": "where is the thames river source", "prediction": [["geography.river.origin"], ["geography.river.mouth"], ["geography.river.basin_countries"]], "ground_paths": [["geography.river.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the thames river source [/INST]", "raw_output": {"paths": ["<PATH> geography.river.origin </PATH>", "<PATH> geography.river.mouth </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.004946299362927675, -0.05245119705796242, -0.053299691528081894], "norm_scores": [0.34406793117523193, 0.3281051814556122, 0.3278268873691559]}}
{"id": "WebQTest-1595", "question": "what new movies is robert pattinson in", "prediction": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_winner.awards_won", "award.award_honor.honored_for"], ["music.artist.track", "music.recording.releases"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["music.recording.artist", "music.recording.releases"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["film.producer.films_executive_produced"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what new movies is robert pattinson in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.02752300724387169, -0.030711915343999863, -0.03228629752993584], "norm_scores": [0.3342174291610718, 0.3331533372402191, 0.3326292335987091]}}
{"id": "WebQTest-1596", "question": "what language do they use in pakistan", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they use in pakistan [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01526531670242548, -0.018191592767834663, -0.035044509917497635], "norm_scores": [0.3358530104160309, 0.33487164974212646, 0.32927536964416504]}}
{"id": "WebQTest-1597", "question": "what kind of products does nike sell", "prediction": [["business.business_operation.industry"], ["business.industry.companies"], ["business.industry.parent_industry"]], "ground_paths": [["business.business_operation.industry"], ["business.industry.companies"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of products does nike sell [/INST]", "raw_output": {"paths": ["<PATH> business.business_operation.industry </PATH>", "<PATH> business.industry.companies </PATH>", "<PATH> business.industry.parent_industry </PATH>"], "scores": [-0.012080350890755653, -0.02816462330520153, -0.053698401898145676], "norm_scores": [0.33975693583488464, 0.33433592319488525, 0.3259071111679077]}}
{"id": "WebQTest-1598", "question": "who did lbj marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.profession", "common.topic.notable_types"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.parents", "people.person.parents"], ["book.written_work.subjects", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.marriage.spouse", "people.person.spouse_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did lbj marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.02698439545929432, -0.04098433628678322, -0.045530110597610474], "norm_scores": [0.3369586765766144, 0.3322741687297821, 0.33076712489128113]}}
{"id": "WebQTest-1599", "question": "where did kim jong il die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_interment.interred_here"], ["people.place_of_death.people"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kim jong il die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.place_of_death.people </PATH>"], "scores": [-0.00036296271719038486, -0.0988500788807869, -0.10602720081806183], "norm_scores": [0.35638758540153503, 0.32296106219291687, 0.32065141201019287]}}
{"id": "WebQTest-1603", "question": "what is the sacred text of daoism", "prediction": [["religion.religion.texts"], ["religion.religious_text.religious_text_of"], ["religion.religion.sacred_sites"]], "ground_paths": [["religion.religion.texts"], ["book.written_work.subjects"], ["religion.religion.notable_figures"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the sacred text of daoism [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.texts </PATH>", "<PATH> religion.religious_text.religious_text_of </PATH>", "<PATH> religion.religion.sacred_sites </PATH>"], "scores": [-0.009522907435894012, -0.014360549859702587, -0.11777965724468231], "norm_scores": [0.34571316838264465, 0.3440447747707367, 0.31024202704429626]}}
{"id": "WebQTest-1604", "question": "what countries speak english natively", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries speak english natively [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.016535526141524315, -0.02079155296087265, -0.02599947340786457], "norm_scores": [0.334858775138855, 0.33343660831451416, 0.33170461654663086]}}
{"id": "WebQTest-1607", "question": "who won fedex cup 2012", "prediction": [["award.competition.winner", "people.person.profession"], ["award.competition.winner", "people.person.gender", "people.person.gender"], ["award.competitor.competitions", "people.person.profession"]], "ground_paths": [["sports.sports_award.award", "sports.sports_award.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won fedex cup 2012 [/INST]", "raw_output": {"paths": ["<PATH> award.competition.winner <SEP> people.person.profession </PATH>", "<PATH> award.competition.winner <SEP> people.person.gender <SEP> people.person.gender </PATH>", "<PATH> award.competitor.competitions <SEP> people.person.profession </PATH>"], "scores": [-0.01763594150543213, -0.027476852759718895, -0.03056074120104313], "norm_scores": [0.33586737513542175, 0.3325783610343933, 0.3315542936325073]}}
{"id": "WebQTest-1608", "question": "what language is mainly spoken in egypt", "prediction": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is mainly spoken in egypt [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.main_country </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.023488828912377357, -0.024030758067965508, -0.02581246942281723], "norm_scores": [0.33365172147750854, 0.3334709405899048, 0.33287733793258667]}}
{"id": "WebQTest-1609", "question": "who played alf on tv show", "prediction": [["tv.tv_program_creator.programs_created"], ["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played alf on tv show [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program_creator.programs_created </PATH>", "<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.017568601295351982, -0.02078024111688137, -0.04868059605360031], "norm_scores": [0.3371361792087555, 0.3360551595687866, 0.3268086910247803]}}
{"id": "WebQTest-1610", "question": "who was gerald ford vp", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_president.vice_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was gerald ford vp [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.00501413457095623, -0.026032593101263046, -0.0911531001329422], "norm_scores": [0.3452242612838745, 0.3380439281463623, 0.3167317807674408]}}
{"id": "WebQTest-1611", "question": "what is armenian currency", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is armenian currency [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.0067584337666630745, -0.025785326957702637, -0.07715480029582977], "norm_scores": [0.3432678282260895, 0.33679822087287903, 0.3199339807033539]}}
{"id": "WebQTest-1612", "question": "what type of art did raphael sanzio create", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art did raphael sanzio create [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.004148130305111408, -0.023943791165947914, -0.06022090092301369], "norm_scores": [0.3417787551879883, 0.335079550743103, 0.3231416940689087]}}
{"id": "WebQTest-1613", "question": "what language do they speak in malta", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in malta [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01790209300816059, -0.022208964452147484, -0.026829693466424942], "norm_scores": [0.33480486273765564, 0.3333660066127777, 0.33182916045188904]}}
{"id": "WebQTest-1614", "question": "what religion was king louis xiv", "prediction": [["people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"], ["religion.religion.notable_figures"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion was king louis xiv [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>"], "scores": [-0.0017289702082052827, -0.05343811586499214, -0.06539048254489899], "norm_scores": [0.34626907110214233, 0.32881882786750793, 0.32491207122802734]}}
{"id": "WebQTest-1615", "question": "what football team does mike wright play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what football team does mike wright play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.015373163856565952, -0.016723811626434326, -0.030083267018198967], "norm_scores": [0.3351152539253235, 0.3346629738807678, 0.3302217721939087]}}
{"id": "WebQTest-1616", "question": "what did shannon hoon die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did shannon hoon die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0021959212608635426, -0.03807642683386803, -0.12707041203975677], "norm_scores": [0.351202130317688, 0.33882421255111694, 0.30997368693351746]}}
{"id": "WebQTest-1617", "question": "what did nicholas copernicus discover", "prediction": [["symbols.namesake.named_after"], ["base.argumentmaps.original_idea.innovator"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["symbols.namesake.named_after"], ["base.argumentmaps.innovator.original_ideas"], ["base.argumentmaps.original_idea.innovator"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did nicholas copernicus discover [/INST]", "raw_output": {"paths": ["<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.original_idea.innovator </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.015527833253145218, -0.023192189633846283, -0.03568590059876442], "norm_scores": [0.3364274799823761, 0.333858847618103, 0.3297136723995209]}}
{"id": "WebQTest-1618", "question": "what teams has manny ramirez played for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_coach.teams_coached", "sports.sports_team.coaches"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what teams has manny ramirez played for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.01875470206141472, -0.021584518253803253, -0.022928040474653244], "norm_scores": [0.33411186933517456, 0.3331677317619324, 0.33272039890289307]}}
{"id": "WebQTest-1619", "question": "where do the appalachian mountains run", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["geography.mountain_range.mountains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the appalachian mountains run [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> geography.mountain_range.mountains </PATH>"], "scores": [-0.008366871625185013, -0.0202505923807621, -0.08367051929235458], "norm_scores": [0.3429768681526184, 0.33892518281936646, 0.3180979788303375]}}
{"id": "WebQTest-1620", "question": "when was president wilson in office", "prediction": [["book.written_work.subjects", "base.culturalevent.event.entity_involved"], ["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "location.location.events"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was president wilson in office [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> base.culturalevent.event.entity_involved </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> location.location.events </PATH>"], "scores": [-0.031184270977973938, -0.03373822942376137, -0.03423419967293739], "norm_scores": [0.3339563012123108, 0.3331044614315033, 0.3329392969608307]}}
{"id": "WebQTest-1621", "question": "what political party is jerry brown", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"], ["government.election_campaign.candidate", "government.election_campaign.party"]], "ground_paths": [["government.election_campaign.candidate", "government.election_campaign.party"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"], ["people.person.nationality", "organization.organization.geographic_scope"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party is jerry brown [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.politician.party <SEP> government.political_party_tenure.party </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.party </PATH>"], "scores": [-0.01290561817586422, -0.019498560577630997, -0.027611497789621353], "norm_scores": [0.33570224046707153, 0.33349624276161194, 0.3308015465736389]}}
{"id": "WebQTest-1622", "question": "what ocean is around hawaii", "prediction": [["location.location.contains", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"], ["location.location.containedby", "location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what ocean is around hawaii [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.contains </PATH>"], "scores": [-0.026566971093416214, -0.02949567325413227, -0.03906013071537018], "norm_scores": [0.33504652976989746, 0.3340667188167572, 0.33088675141334534]}}
{"id": "WebQTest-1623", "question": "what is the most common language spoken in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most common language spoken in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0074119907803833485, -0.027771180495619774, -0.04537852853536606], "norm_scores": [0.339836448431015, 0.33298760652542114, 0.32717591524124146]}}
{"id": "WebQTest-1624", "question": "what language tunisia", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language tunisia [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01637701690196991, -0.017467046156525612, -0.04670514538884163], "norm_scores": [0.3368094563484192, 0.3364425301551819, 0.3267480134963989]}}
{"id": "WebQTest-1625", "question": "what are some inventions that leonardo da vinci invented", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some inventions that leonardo da vinci invented [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.006792873609811068, -0.019607167690992355, -0.07422646880149841], "norm_scores": [0.3422246277332306, 0.33786725997924805, 0.31990811228752136]}}
{"id": "WebQTest-1626", "question": "who wrote st trinians", "prediction": [["film.film.written_by"], ["fictional_universe.fictional_universe.locations"], ["film.film_story_contributor.film_story_credits"]], "ground_paths": [["fictional_universe.fictional_universe.created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who wrote st trinians [/INST]", "raw_output": {"paths": ["<PATH> film.film.written_by </PATH>", "<PATH> fictional_universe.fictional_universe.locations </PATH>", "<PATH> film.film_story_contributor.film_story_credits </PATH>"], "scores": [-0.022473089396953583, -0.03495410457253456, -0.044397614896297455], "norm_scores": [0.33716458082199097, 0.3329825699329376, 0.3298528492450714]}}
{"id": "WebQTest-1627", "question": "what are malaysian coins called", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are malaysian coins called [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.006318589206784964, -0.028098253533244133, -0.06299002468585968], "norm_scores": [0.34207209944725037, 0.33470243215560913, 0.3232254683971405]}}
{"id": "WebQTest-1628", "question": "what type government does mexico have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type government does mexico have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.006037293002009392, -0.02227769047021866, -0.14312058687210083], "norm_scores": [0.350165992975235, 0.3445250988006592, 0.30530890822410583]}}
{"id": "WebQTest-1629", "question": "what currency does dominican republic take", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does dominican republic take [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-0.000495626125484705, -0.07121043652296066, -0.134587362408638], "norm_scores": [0.35634902119636536, 0.33202022314071655, 0.3116307258605957]}}
{"id": "WebQTest-1630", "question": "what inspired michael jackson to become a singer", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inspired michael jackson to become a singer [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.006132304668426514, -0.020052997395396233, -0.09095664322376251], "norm_scores": [0.3442519009113312, 0.33949288725852966, 0.3162551522254944]}}
{"id": "WebQTest-1631", "question": "who is the texas state senator", "prediction": [["location.administrative_division.country", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.events", "military.military_person.participated_in_conflicts"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["location.us_state.capital", "people.deceased_person.place_of_death"], ["government.government_position_held.district_represented", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["time.event.locations", "base.culturalevent.event.entity_involved"], ["government.governmental_jurisdiction.governing_officials", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"], ["base.biblioness.bibs_location.state", "symbols.namesake.named_after"], ["base.biblioness.bibs_location.state", "people.person.place_of_birth"], ["location.administrative_division.country", "people.person.nationality"], ["base.biblioness.bibs_location.state", "people.deceased_person.place_of_death"], ["government.political_district.representatives", "government.government_position_held.office_holder"], ["government.political_district.elections", "government.election.winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the texas state senator [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.country <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.016897810623049736, -0.02520248107612133, -0.028288481757044792], "norm_scores": [0.3355250060558319, 0.332750141620636, 0.3317248523235321]}}
{"id": "WebQTest-1632", "question": "what to visit in london city", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["organization.organization.geographic_scope"], ["transportation.bridge.locale"], ["travel.travel_destination.tourist_attractions"], ["metropolitan_transit.transit_system.area_served"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what to visit in london city [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.012045033276081085, -0.02057819627225399, -0.031448014080524445], "norm_scores": [0.3364412486553192, 0.3335825502872467, 0.3299762010574341]}}
{"id": "WebQTest-1633", "question": "what genre of art is the mona lisa", "prediction": [["visual_art.artwork.art_form"], ["visual_art.visual_artist.art_forms"], ["visual_art.art_subject.artwork_on_the_subject"]], "ground_paths": [["visual_art.artwork.art_genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what genre of art is the mona lisa [/INST]", "raw_output": {"paths": ["<PATH> visual_art.artwork.art_form </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_subject.artwork_on_the_subject </PATH>"], "scores": [-0.018163591623306274, -0.028784435242414474, -0.038889672607183456], "norm_scores": [0.336822509765625, 0.3332641124725342, 0.32991334795951843]}}
{"id": "WebQTest-1634", "question": "who does peter griffin voice", "prediction": [["fictional_universe.fictional_character.character_created_by"], ["film.performance.character", "film.actor.film"], ["fictional_universe.fictional_character_creator.fictional_characters_created"]], "ground_paths": [["fictional_universe.fictional_character.character_created_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does peter griffin voice [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.character_created_by </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> fictional_universe.fictional_character_creator.fictional_characters_created </PATH>"], "scores": [-0.017927804961800575, -0.032054971903562546, -0.03284987807273865], "norm_scores": [0.33656877279281616, 0.3318474590778351, 0.33158376812934875]}}
{"id": "WebQTest-1635", "question": "when did tony romo got drafted", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did tony romo got drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft_pick.draft </PATH>"], "scores": [-0.012782552279531956, -0.014999275095760822, -0.0261964350938797], "norm_scores": [0.33506882190704346, 0.33432692289352417, 0.3306042551994324]}}
{"id": "WebQTest-1637", "question": "what are the catholic holy days", "prediction": [["religion.religion.holidays"], ["time.holiday.featured_in_religions"], ["time.recurring_event.holidays"]], "ground_paths": [["time.holiday.featured_in_religions"], ["religion.religion.holidays"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the catholic holy days [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.holidays </PATH>", "<PATH> time.holiday.featured_in_religions </PATH>", "<PATH> time.recurring_event.holidays </PATH>"], "scores": [-0.011771447025239468, -0.014179793186485767, -0.09324594587087631], "norm_scores": [0.34254190325737, 0.341717928647995, 0.3157401382923126]}}
{"id": "WebQTest-1638", "question": "what language is most commonly spoken in belgium", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language is most commonly spoken in belgium [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01143789291381836, -0.01953384093940258, -0.03437352553009987], "norm_scores": [0.3367840051651001, 0.3340684473514557, 0.3291475474834442]}}
{"id": "WebQTest-1639", "question": "where did frank baum die", "prediction": [["people.deceased_person.place_of_death"], ["people.place_of_death.people_deceased_here"], ["people.place_of_death.people", "people.place_of_death.location"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did frank baum die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>", "<PATH> people.place_of_death.people <SEP> people.place_of_death.location </PATH>"], "scores": [-0.0003386183234397322, -0.10155227035284042, -0.1062389463186264], "norm_scores": [0.3567282557487488, 0.3223895728588104, 0.3208821713924408]}}
{"id": "WebQTest-1640", "question": "where is tunis on a map", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.continents.countries_within"]], "ground_paths": [["base.locations.countries.continent"], ["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.country"], ["location.location.containedby"], []], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is tunis on a map [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.continents.countries_within </PATH>"], "scores": [-0.013792848214507103, -0.03483666107058525, -0.03886142373085022], "norm_scores": [0.3384760022163391, 0.33142760396003723, 0.33009636402130127]}}
{"id": "WebQTest-1641", "question": "who is nicolas cage married too", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is nicolas cage married too [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>"], "scores": [-0.017240768298506737, -0.032343074679374695, -0.043680474162101746], "norm_scores": [0.3379613757133484, 0.3328957259654999, 0.32914286851882935]}}
{"id": "WebQTest-1643", "question": "who does the voice for chucky in child 's play", "prediction": [["film.performance.film", "film.performance.actor"], ["film.film.country", "people.person.nationality"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["film.performance.film", "film.performance.actor"], ["film.performance.film", "film.actor.film"], ["film.film.starring", "film.actor.film"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does the voice for chucky in child 's play [/INST]", "raw_output": {"paths": ["<PATH> film.performance.film <SEP> film.performance.actor </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.02933424524962902, -0.02984640933573246, -0.03117344342172146], "norm_scores": [0.3335946202278137, 0.33342379331588745, 0.3329816162586212]}}
{"id": "WebQTest-1644", "question": "what did mary wollstonecraft fight for", "prediction": [["people.person.profession"], ["base.activism.activist.area_of_activism"], ["base.activism.activism_issue.activists"]], "ground_paths": [["base.activism.activist.area_of_activism"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did mary wollstonecraft fight for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> base.activism.activist.area_of_activism </PATH>", "<PATH> base.activism.activism_issue.activists </PATH>"], "scores": [-0.01825747638940811, -0.022323865443468094, -0.04245195537805557], "norm_scores": [0.33646947145462036, 0.3351040184497833, 0.32842645049095154]}}
{"id": "WebQTest-1646", "question": "what year did the phillies begin", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_team.previously_known"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the phillies begin [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.previously_known </PATH>"], "scores": [-0.0036957862321287394, -0.03619933873414993, -0.058938439935445786], "norm_scores": [0.34313857555389404, 0.3321646749973297, 0.32469677925109863]}}
{"id": "WebQTest-1648", "question": "what language does iceland speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does iceland speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01456103753298521, -0.018421201035380363, -0.0467095710337162], "norm_scores": [0.3373238742351532, 0.3360242545604706, 0.3266519010066986]}}
{"id": "WebQTest-1649", "question": "what was dr seuss real name", "prediction": [["people.person.profession"], ["book.book.interior_illustrations_by", "people.person.nationality"], ["book.book_edition.author_editor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was dr seuss real name [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> book.book.interior_illustrations_by <SEP> people.person.nationality </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.025599755346775055, -0.027151087298989296, -0.038842011243104935], "norm_scores": [0.33497530221939087, 0.33445602655410767, 0.33056870102882385]}}
{"id": "WebQTest-1650", "question": "what did gloria steinem founded", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.founder.organizations_founded"]], "ground_paths": [["organization.organization_founder.organizations_founded"], ["organization.organization.founders"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did gloria steinem founded [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.founder.organizations_founded </PATH>"], "scores": [-0.0055919308215379715, -0.024082757532596588, -0.10754237323999405], "norm_scores": [0.346650093793869, 0.34029915928840637, 0.3130508065223694]}}
{"id": "WebQTest-1652", "question": "what language they speak in scotland", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language they speak in scotland [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.015960711985826492, -0.01754709891974926, -0.03041382133960724], "norm_scores": [0.3351132869720459, 0.3345820903778076, 0.33030468225479126]}}
{"id": "WebQTest-1653", "question": "what boarding school did mark zuckerberg go to", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what boarding school did mark zuckerberg go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.013955125585198402, -0.02054816670715809, -0.025287887081503868], "norm_scores": [0.3353274464607239, 0.3331238925457001, 0.3315487205982208]}}
{"id": "WebQTest-1654", "question": "what are bigos", "prediction": [["food.dish.cuisine"], ["food.diet_follower.follows_diet"], ["food.diet_and_soup.diet"]], "ground_paths": [["food.type_of_dish.dishes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are bigos [/INST]", "raw_output": {"paths": ["<PATH> food.dish.cuisine </PATH>", "<PATH> food.diet_follower.follows_diet </PATH>", "<PATH> food.diet_and_soup.diet </PATH>"], "scores": [-0.03191304951906204, -0.06590995192527771, -0.09107890725135803], "norm_scores": [0.34374597668647766, 0.33225610852241516, 0.32399794459342957]}}
{"id": "WebQTest-1655", "question": "where are ike and tina turner 's children", "prediction": [["people.person.children"], ["people.person.parents"], ["music.artist.genre", "music.artist.genre"]], "ground_paths": [["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are ike and tina turner 's children [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> music.artist.genre <SEP> music.artist.genre </PATH>"], "scores": [-0.008405529893934727, -0.019619889557361603, -0.0836121067404747], "norm_scores": [0.3428885042667389, 0.33906468749046326, 0.31804683804512024]}}
{"id": "WebQTest-1656", "question": "what are some of leonardo da vinci most famous inventions", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are some of leonardo da vinci most famous inventions [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.0068094730377197266, -0.01962396688759327, -0.07517323642969131], "norm_scores": [0.34232649207115173, 0.33796772360801697, 0.3197058141231537]}}
{"id": "WebQTest-1657", "question": "when did shaq come into the nba", "prediction": [["sports.sports_award_winner.awards", "sports.sports_award.season"], ["sports.sports_award_winner.awards", "sports.sports_league_season.awards"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"]], "ground_paths": [["sports.drafted_athlete.drafted", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did shaq come into the nba [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_league_season.awards </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>"], "scores": [-0.015520294196903706, -0.021547144278883934, -0.02199414186179638], "norm_scores": [0.33472374081611633, 0.3327125012874603, 0.3325637876987457]}}
{"id": "WebQTest-1658", "question": "where do they speak turkish", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak turkish [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01345853041857481, -0.02283446304500103, -0.028082629665732384], "norm_scores": [0.3360045254230499, 0.33286890387535095, 0.33112654089927673]}}
{"id": "WebQTest-1659", "question": "what is the zip code for walnut creek", "prediction": [["location.citytown.postal_codes"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.citytown.postal_codes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the zip code for walnut creek [/INST]", "raw_output": {"paths": ["<PATH> location.citytown.postal_codes </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-7.234699296532199e-05, -0.10787398368120193, -0.16249066591262817], "norm_scores": [0.36391544342041016, 0.3267253339290619, 0.30935922265052795]}}
{"id": "WebQTest-1661", "question": "who was the voice of darth vader in the original star wars movie", "prediction": [["film.performance.character", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the voice of darth vader in the original star wars movie [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.01832062005996704, -0.02562844194471836, -0.03092223033308983], "norm_scores": [0.3355483412742615, 0.3331051468849182, 0.33134642243385315]}}
{"id": "WebQTest-1662", "question": "where is english spoken around the world", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is english spoken around the world [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013521007262170315, -0.0246114581823349, -0.027434486895799637], "norm_scores": [0.33611708879470825, 0.3324100077152252, 0.33147290349006653]}}
{"id": "WebQTest-1663", "question": "what music did louis armstrong play", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what music did louis armstrong play [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.0017629647627472878, -0.04910828545689583, -0.09162357449531555], "norm_scores": [0.34869736433029175, 0.332572877407074, 0.31872978806495667]}}
{"id": "WebQTest-1664", "question": "what form of currency does germany use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of currency does germany use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.005844567436724901, -0.025287123396992683, -0.11957298964262009], "norm_scores": [0.3480384945869446, 0.34133708477020264, 0.31062445044517517]}}
{"id": "WebQTest-1665", "question": "where is the latin language from", "prediction": [["language.human_language.language_family"], ["base.rosetta.languoid.parent"], ["base.rosetta.languoid.child"]], "ground_paths": [["language.human_language.language_family"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the latin language from [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.language_family </PATH>", "<PATH> base.rosetta.languoid.parent </PATH>", "<PATH> base.rosetta.languoid.child </PATH>"], "scores": [-0.0019504957599565387, -0.05311039462685585, -0.06700535118579865], "norm_scores": [0.34636321663856506, 0.3290889263153076, 0.3245478868484497]}}
{"id": "WebQTest-1666", "question": "what are the top tourist attractions in rome", "prediction": [["location.location.containedby"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"]], "ground_paths": [["base.arthist.int_zm_nyek.helynev"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the top tourist attractions in rome [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>"], "scores": [-0.01793585531413555, -0.018436580896377563, -0.018728571012616158], "norm_scores": [0.33347705006599426, 0.3333101272583008, 0.33321282267570496]}}
{"id": "WebQTest-1667", "question": "who were the children of king solomon", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.parent"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who were the children of king solomon [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.01370335090905428, -0.01397783774882555, -0.13655345141887665], "norm_scores": [0.3467259705066681, 0.34663084149360657, 0.30664315819740295]}}
{"id": "WebQTest-1668", "question": "what did andy irons die from", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did andy irons die from [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0019710881169885397, -0.04006817564368248, -0.1241099163889885], "norm_scores": [0.35116735100746155, 0.3380405306816101, 0.31079205870628357]}}
{"id": "WebQTest-1669", "question": "where is auburn university located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is auburn university located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.0025937880855053663, -0.043521374464035034, -0.12620630860328674], "norm_scores": [0.35166433453559875, 0.33756211400032043, 0.3107735812664032]}}
{"id": "WebQTest-1670", "question": "who is tatyana ali parents", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is tatyana ali parents [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.011808022856712341, -0.016570962965488434, -0.15472404658794403], "norm_scores": [0.34939682483673096, 0.3477366268634796, 0.30286654829978943]}}
{"id": "WebQTest-1671", "question": "where did leif ericson grow up", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did leif ericson grow up [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0015830170596018434, -0.05039510130882263, -0.0844576507806778], "norm_scores": [0.3480892479419708, 0.33150628209114075, 0.3204044997692108]}}
{"id": "WebQTest-1672", "question": "what language do the maasai speak", "prediction": [["people.ethnicity.languages_spoken"], ["location.country.languages_spoken"], ["language.human_language.main_country"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do the maasai speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0029194108210504055, -0.04851796478033066, -0.06374120712280273], "norm_scores": [0.34525421261787415, 0.3298646807670593, 0.32488107681274414]}}
{"id": "WebQTest-1673", "question": "what political party was hitler in", "prediction": [["base.activism.activist.area_of_activism", "government.political_party.ideology"], ["organization.organization_founder.organizations_founded"], ["base.activism.activism_issue.supporting_political_parties"]], "ground_paths": [["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["organization.organization_founder.organizations_founded"], ["base.activism.activist.area_of_activism", "government.political_party.ideology"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what political party was hitler in [/INST]", "raw_output": {"paths": ["<PATH> base.activism.activist.area_of_activism <SEP> government.political_party.ideology </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> base.activism.activism_issue.supporting_political_parties </PATH>"], "scores": [-0.017228195443749428, -0.021004796028137207, -0.02954835444688797], "norm_scores": [0.3351221978664398, 0.3338589668273926, 0.3310187757015228]}}
{"id": "WebQTest-1674", "question": "who is the leader of north korea today", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["government.government_position_held.basic_title", "government.politician.government_positions_held"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the leader of north korea today [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.004552467726171017, -0.028452729806303978, -0.06816405802965164], "norm_scores": [0.3430823087692261, 0.334979772567749, 0.3219379484653473]}}
{"id": "WebQTest-1675", "question": "where do the appalachian mountains run through", "prediction": [["location.location.partially_containedby"], ["location.location.partially_contains"], ["location.location.containedby"]], "ground_paths": [["location.location.partially_containedby"], ["location.location.partially_contains"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do the appalachian mountains run through [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.007956042885780334, -0.019608383998274803, -0.0977073535323143], "norm_scores": [0.34452182054519653, 0.3405306339263916, 0.31494754552841187]}}
{"id": "WebQTest-1676", "question": "what money does spain use", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money does spain use [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.006051645614206791, -0.026295071467757225, -0.1383385956287384], "norm_scores": [0.3501339554786682, 0.3431173264980316, 0.30674871802330017]}}
{"id": "WebQTest-1677", "question": "where was terry waite born", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was terry waite born [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.00993750337511301, -0.016784073784947395, -0.14059528708457947], "norm_scores": [0.3483477234840393, 0.34597086906433105, 0.30568140745162964]}}
{"id": "WebQTest-1678", "question": "what country did the battle of san juan hill take place", "prediction": [["location.location.events"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.casualties", "military.casualties.combatant"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did the battle of san juan hill take place [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.casualties <SEP> military.casualties.combatant </PATH>"], "scores": [-0.006759857293218374, -0.02631847932934761, -0.05437786877155304], "norm_scores": [0.3408166170120239, 0.3342154622077942, 0.3249679505825043]}}
{"id": "WebQTest-1679", "question": "what kind of government does the united states have currently", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.planet"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does the united states have currently [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.planet </PATH>"], "scores": [-0.0076735857874155045, -0.01653963513672352, -0.15725021064281464], "norm_scores": [0.350600928068161, 0.34750622510910034, 0.30189281702041626]}}
{"id": "WebQTest-1680", "question": "when was the last time the orioles won the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.championship"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the orioles won the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.009070079773664474, -0.013563894666731358, -0.09835680574178696], "norm_scores": [0.343630850315094, 0.3420901298522949, 0.31427904963493347]}}
{"id": "WebQTest-1682", "question": "who did ben stiller date", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["base.popstra.celebrity.dated", "base.popstra.celebrity.dated"], ["people.person.languages", "people.person.languages"], ["base.popstra.friendship.participant", "base.popstra.celebrity.friendship"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["people.person.ethnicity", "people.person.ethnicity"], ["base.popstra.dated.participant", "base.popstra.dated.participant"], ["base.popstra.celebrity.dated", "base.popstra.dated.participant"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["common.topic.notable_types", "common.topic.notable_types"], ["base.popstra.dated.participant", "base.popstra.celebrity.dated"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["celebrities.romantic_relationship.celebrity", "celebrities.romantic_relationship.celebrity"], ["people.person.parents", "people.person.parents"], ["celebrities.celebrity.sexual_relationships", "celebrities.romantic_relationship.celebrity"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did ben stiller date [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.015075210481882095, -0.030745482072234154, -0.0339072160422802], "norm_scores": [0.3371776044368744, 0.3319351375102997, 0.3308872878551483]}}
{"id": "WebQTest-1683", "question": "what is the most common language in norway", "prediction": [["location.country.official_language"], ["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the most common language in norway [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.01552190538495779, -0.021274670958518982, -0.03342113271355629], "norm_scores": [0.33596235513687134, 0.3340352177619934, 0.33000239729881287]}}
{"id": "WebQTest-1684", "question": "what influenced william shakespeare to start writing", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what influenced william shakespeare to start writing [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.009861690923571587, -0.013309317640960217, -0.10381853580474854], "norm_scores": [0.3440113961696625, 0.3428274095058441, 0.313161164522171]}}
{"id": "WebQTest-1685", "question": "where did benjamin franklin went to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did benjamin franklin went to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01302084606140852, -0.02382521703839302, -0.024082886055111885], "norm_scores": [0.33576735854148865, 0.3321591317653656, 0.3320735692977905]}}
{"id": "WebQTest-1686", "question": "who is the governor of arizona 2009", "prediction": [["location.location.containedby", "people.person.nationality"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"]], "ground_paths": [["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.office_holder"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"], ["government.government_position_held.jurisdiction_of_office", "government.government_position_held.appointed_by"], ["location.country.first_level_divisions", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the governor of arizona 2009 [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby <SEP> people.person.nationality </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.020839765667915344, -0.021089794114232063, -0.02586148865520954], "norm_scores": [0.3339186906814575, 0.3338352143764496, 0.3322460651397705]}}
{"id": "WebQTest-1687", "question": "what movies has spike lee made", "prediction": [["film.film.directed_by"], ["film.director.film"], ["film.producer.film"]], "ground_paths": [["film.film.directed_by"], ["film.film.produced_by"], ["film.film.executive_produced_by"], ["film.director.film"], ["film.writer.film"], ["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has spike lee made [/INST]", "raw_output": {"paths": ["<PATH> film.film.directed_by </PATH>", "<PATH> film.director.film </PATH>", "<PATH> film.producer.film </PATH>"], "scores": [-0.026374151930212975, -0.03439728915691376, -0.03806385397911072], "norm_scores": [0.33552688360214233, 0.33284565806388855, 0.3316275179386139]}}
{"id": "WebQTest-1688", "question": "what did bernie mac died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did bernie mac died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.0015568699454888701, -0.04408973455429077, -0.1264723688364029], "norm_scores": [0.35199734568595886, 0.3373398184776306, 0.31066280603408813]}}
{"id": "WebQTest-1690", "question": "where is electrolux based", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization.headquarters", "location.mailing_address.country"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.founders", "people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is electrolux based [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.country </PATH>"], "scores": [-0.004734965972602367, -0.03522930294275284, -0.054808247834444046], "norm_scores": [0.3423337936401367, 0.3320521116256714, 0.3256140947341919]}}
{"id": "WebQTest-1691", "question": "what is houston timezone", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is houston timezone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.397876056027599e-06, -0.19547507166862488, -0.19731374084949493], "norm_scores": [0.3783017694950104, 0.3111349046230316, 0.3105633556842804]}}
{"id": "WebQTest-1694", "question": "what was shakespeare 's wife called", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"], ["people.person.parents", "people.person.children"]], "ground_paths": [["freebase.valuenotation.has_value", "freebase.valuenotation.has_value"], ["people.person.place_of_birth", "people.deceased_person.place_of_death"], ["book.book_subject.works", "book.book_subject.works"], ["people.marriage.spouse", "people.person.spouse_s"], ["people.person.parents", "people.person.parents"], ["people.person.nationality", "people.person.nationality"], ["people.place_of_interment.interred_here", "people.deceased_person.place_of_burial"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was shakespeare 's wife called [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.parents <SEP> people.person.children </PATH>"], "scores": [-0.025834407657384872, -0.03755440562963486, -0.03966907411813736], "norm_scores": [0.33617863059043884, 0.3322616219520569, 0.3315597176551819]}}
{"id": "WebQTest-1695", "question": "when did truman become president", "prediction": [["government.us_president.vice_president"], ["government.us_vice_president.to_president"], ["government.government_position_held.office_holder", "government.government_position_held.office_holder"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did truman become president [/INST]", "raw_output": {"paths": ["<PATH> government.us_president.vice_president </PATH>", "<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.007047276943922043, -0.028393138200044632, -0.06166667118668556], "norm_scores": [0.34179550409317017, 0.33457690477371216, 0.3236275315284729]}}
{"id": "WebQTest-1696", "question": "what did ferdinand magellan do", "prediction": [["people.person.profession"], ["people.profession.people_with_this_profession"], ["common.topic.notable_types"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did ferdinand magellan do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0015485500916838646, -0.0617334321141243, -0.0671013593673706], "norm_scores": [0.34744662046432495, 0.32715240120887756, 0.3254009783267975]}}
{"id": "WebQTest-1698", "question": "where do burmese cats originate from", "prediction": [["biology.animal_breed.place_of_origin"], ["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.breed_of"]], "ground_paths": [["biology.breed_origin.breeds_originating_here"], ["biology.animal_breed.place_of_origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do burmese cats originate from [/INST]", "raw_output": {"paths": ["<PATH> biology.animal_breed.place_of_origin </PATH>", "<PATH> biology.breed_origin.breeds_originating_here </PATH>", "<PATH> biology.animal_breed.breed_of </PATH>"], "scores": [-0.0037647292483597994, -0.02747354842722416, -0.06424976140260696], "norm_scores": [0.3427148163318634, 0.3346850275993347, 0.3226001560688019]}}
{"id": "WebQTest-1699", "question": "who played kirk", "prediction": [["film.film_character.portrayed_in_films", "film.actor.film"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["fictional_universe.fictional_character.quotations", "media_common.quotation.author"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played kirk [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.023708252236247063, -0.031513962894678116, -0.033872075378894806], "norm_scores": [0.3353327512741089, 0.3327254354953766, 0.33194175362586975]}}
{"id": "WebQTest-1700", "question": "where is mt st helens located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.mountain.mountain_type"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mt st helens located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.mountain.mountain_type </PATH>"], "scores": [-0.008456284180283546, -0.021293971687555313, -0.0919506773352623], "norm_scores": [0.343980610370636, 0.33959290385246277, 0.31642648577690125]}}
{"id": "WebQTest-1701", "question": "what county is st paul minnesota in", "prediction": [["location.us_county.county_seat"], ["location.location.containedby"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is st paul minnesota in [/INST]", "raw_output": {"paths": ["<PATH> location.us_county.county_seat </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.014070452190935612, -0.019095944240689278, -0.0358252227306366], "norm_scores": [0.33630773425102234, 0.3346218466758728, 0.32907044887542725]}}
{"id": "WebQTest-1702", "question": "who was king henry viii son", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.gender"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was king henry viii son [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.gender </PATH>"], "scores": [-0.013593866489827633, -0.015779491513967514, -0.06610093265771866], "norm_scores": [0.3393667936325073, 0.33862587809562683, 0.3220073878765106]}}
{"id": "WebQTest-1705", "question": "where was franz ferdinand from", "prediction": [["people.person.place_of_birth"], ["people.deceased_person.place_of_death"], ["people.person.nationality"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was franz ferdinand from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.0009204939706251025, -0.07319393754005432, -0.07792505621910095], "norm_scores": [0.3501201868057251, 0.3257085978984833, 0.324171245098114]}}
{"id": "WebQTest-1706", "question": "what was benedict arnold most famous for", "prediction": [["base.honouriam.dishonoured_person.dishonor_bestowed"], ["people.person.profession"], ["base.honouriam.dishonoured_person.dishonor_bestowed", "base.honouri.dishonor.dishonor_bestowed"]], "ground_paths": [["base.honouriam.dishonoured_person.dishonor_bestowed"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was benedict arnold most famous for [/INST]", "raw_output": {"paths": ["<PATH> base.honouriam.dishonoured_person.dishonor_bestowed </PATH>", "<PATH> people.person.profession </PATH>", "<PATH> base.honouriam.dishonoured_person.dishonor_bestowed <SEP> base.honouri.dishonor.dishonor_bestowed </PATH>"], "scores": [-0.021741706877946854, -0.024721743538975716, -0.05965423211455345], "norm_scores": [0.3378582000732422, 0.33685287833213806, 0.32528892159461975]}}
{"id": "WebQTest-1707", "question": "who plays lex luthor on smallville", "prediction": [["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"], ["award.award_nominated_work.award_nominations", "award.award_nomination.award_nominee"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["tv.tv_program.languages", "people.person.languages"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays lex luthor on smallville [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nominee.award_nominations </PATH>", "<PATH> award.award_nominated_work.award_nominations <SEP> award.award_nomination.award_nominee </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.021953871473670006, -0.023814188316464424, -0.027300452813506126], "norm_scores": [0.3341342508792877, 0.3335132598876953, 0.33235251903533936]}}
{"id": "WebQTest-1708", "question": "what are dallas cowboys colors", "prediction": [["sports.sports_team.colors"], ["sports.sports_team.colors", "sports.sports_team.colors"], ["sports.sports_team.color"]], "ground_paths": [["sports.sports_team.colors"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are dallas cowboys colors [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.colors <SEP> sports.sports_team.colors </PATH>", "<PATH> sports.sports_team.color </PATH>"], "scores": [-8.008436270756647e-05, -0.11628928780555725, -0.1205667182803154], "norm_scores": [0.3601296544075012, 0.3206194341182709, 0.3192509412765503]}}
{"id": "WebQTest-1710", "question": "what is the currency of the czech republic", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the currency of the czech republic [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.002181852934882045, -0.04071618989109993, -0.11813844740390778], "norm_scores": [0.3505435585975647, 0.33729252219200134, 0.3121638894081116]}}
{"id": "WebQTest-1711", "question": "when was the last time the la kings won a playoff series", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the la kings won a playoff series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.00626428984105587, -0.018317269161343575, -0.06525362282991409], "norm_scores": [0.341211199760437, 0.3371232748031616, 0.32166555523872375]}}
{"id": "WebQTest-1712", "question": "where did spencer pratt go to school", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.place_of_birth", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did spencer pratt go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.01590603217482567, -0.026476092636585236, -0.03011891059577465], "norm_scores": [0.3360922932624817, 0.3325584828853607, 0.33134925365448]}}
{"id": "WebQTest-1713", "question": "what disease did abraham lincoln had", "prediction": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"], ["people.deceased_person.cause_of_death"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.notable_people_with_this_condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what disease did abraham lincoln had [/INST]", "raw_output": {"paths": ["<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> people.deceased_person.cause_of_death </PATH>"], "scores": [-0.003080962458625436, -0.036775972694158554, -0.04661054164171219], "norm_scores": [0.34196561574935913, 0.33063504099845886, 0.327399343252182]}}
{"id": "WebQTest-1714", "question": "who is vancouver canucks coach", "prediction": [["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team_coach_tenure.team", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"], ["ice_hockey.hockey_coach.current_team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is vancouver canucks coach [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team_coach_tenure.team <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.01206361223012209, -0.02134844847023487, -0.02962920069694519], "norm_scores": [0.3363214433193207, 0.333213210105896, 0.33046531677246094]}}
{"id": "WebQTest-1715", "question": "where did cs lewis wrote", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.book_edition.author_editor"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did cs lewis wrote [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.02086377702653408, -0.02830454334616661, -0.03544964641332626], "norm_scores": [0.33578377962112427, 0.3332945704460144, 0.33092162013053894]}}
{"id": "WebQTest-1716", "question": "what is carlos pena jr nationality", "prediction": [["people.person.nationality"], ["people.person.place_of_birth"], ["people.person.ethnicity"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is carlos pena jr nationality [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.001126221613958478, -0.06738055497407913, -0.08216923475265503], "norm_scores": [0.3498893082141876, 0.32745885848999023, 0.32265183329582214]}}
{"id": "WebQTest-1719", "question": "who was married to jane wyman", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.profession", "people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was married to jane wyman [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.02038140408694744, -0.0401456393301487, -0.04377548024058342], "norm_scores": [0.3381454348564148, 0.33152785897254944, 0.330326646566391]}}
{"id": "WebQTest-1721", "question": "what language does peru speak", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language does peru speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010897863656282425, -0.0307196993380785, -0.03813359513878822], "norm_scores": [0.33858075737953186, 0.3319355547428131, 0.32948368787765503]}}
{"id": "WebQTest-1723", "question": "where is english spoken in the world map", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is english spoken in the world map [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013479159213602543, -0.023133154958486557, -0.028191348537802696], "norm_scores": [0.33604544401168823, 0.33281686902046204, 0.33113768696784973]}}
{"id": "WebQTest-1724", "question": "when did arsenal won the league", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["soccer.football_team.matches_played", "soccer.football_player_match_participation.season"]], "ground_paths": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did arsenal won the league [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> soccer.football_team.matches_played <SEP> soccer.football_player_match_participation.season </PATH>"], "scores": [-0.011057416908442974, -0.013756551779806614, -0.08884471654891968], "norm_scores": [0.3421768248081207, 0.34125450253486633, 0.3165687322616577]}}
{"id": "WebQTest-1725", "question": "what timezone is phoenix az in right now", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is phoenix az in right now [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.160727131529711e-06, -0.17785006761550903, -0.18580499291419983], "norm_scores": [0.37488022446632385, 0.313803106546402, 0.31131672859191895]}}
{"id": "WebQTest-1726", "question": "where is south portland", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.location.contains"]], "ground_paths": [["location.hud_county_place.county"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is south portland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.007300015073269606, -0.04102960228919983, -0.050126589834690094], "norm_scores": [0.341890811920166, 0.33055126667022705, 0.32755789160728455]}}
{"id": "WebQTest-1727", "question": "what countries speak french as a first language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["base.rosetta.local_name.locale"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries speak french as a first language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.011462156660854816, -0.02488989569246769, -0.0272993016988039], "norm_scores": [0.3365927040576935, 0.33210325241088867, 0.33130401372909546]}}
{"id": "WebQTest-1728", "question": "who was ptolemy and what did he do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was ptolemy and what did he do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.00170434161555022, -0.0519331619143486, -0.07120880484580994], "norm_scores": [0.34675654768943787, 0.3297695517539978, 0.3234739303588867]}}
{"id": "WebQTest-1729", "question": "what was the name of henry viii first wife", "prediction": [["people.person.children", "people.person.children"], ["people.person.children", "people.person.parents"], ["people.person.parents", "people.person.parents"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["people.family_member.family", "people.family.members"], ["book.written_work.subjects", "theater.play.characters"], ["people.person.religion", "people.person.religion"], ["people.person.parents", "people.person.parents"], ["people.person.children", "people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the name of henry viii first wife [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.children </PATH>", "<PATH> people.person.children <SEP> people.person.parents </PATH>", "<PATH> people.person.parents <SEP> people.person.parents </PATH>"], "scores": [-0.03476116061210632, -0.036264266818761826, -0.0377812422811985], "norm_scores": [0.33383604884147644, 0.33333462476730347, 0.3328293561935425]}}
{"id": "WebQTest-1730", "question": "where does the parana river flow", "prediction": [["location.location.containedby"], ["geography.river.basin_countries"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does the parana river flow [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.019864028319716454, -0.022601211443543434, -0.027098020538687706], "norm_scores": [0.334441602230072, 0.33352744579315186, 0.3320309817790985]}}
{"id": "WebQTest-1731", "question": "what is president nixon 's first name", "prediction": [["people.person.nationality", "people.person.nationality"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.gender", "people.person.gender"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is president nixon 's first name [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> common.topic.notable_types <SEP> common.topic.notable_types </PATH>", "<PATH> people.person.gender <SEP> people.person.gender </PATH>"], "scores": [-0.023106297478079796, -0.03540055826306343, -0.042344097048044205], "norm_scores": [0.33684471249580383, 0.3327288031578064, 0.33042648434638977]}}
{"id": "WebQTest-1732", "question": "where did theodor schwann get his education", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.deceased_person.place_of_death", "location.location.contains"], ["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did theodor schwann get his education [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.016528695821762085, -0.019243158400058746, -0.02198043465614319], "norm_scores": [0.3342410922050476, 0.3333350419998169, 0.3324238657951355]}}
{"id": "WebQTest-1733", "question": "who or what influenced frank lloyd wright", "prediction": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"], ["people.person.profession"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who or what influenced frank lloyd wright [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced </PATH>", "<PATH> people.person.profession </PATH>"], "scores": [-0.009555122815072536, -0.014021497219800949, -0.09403425455093384], "norm_scores": [0.3431079685688019, 0.3415789306163788, 0.31531304121017456]}}
{"id": "WebQTest-1734", "question": "what year did houston rockets win their first championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did houston rockets win their first championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.006467052269726992, -0.01809637062251568, -0.12522470951080322], "norm_scores": [0.3476494252681732, 0.3436298966407776, 0.3087206780910492]}}
{"id": "WebQTest-1735", "question": "where do most of the people live in russia", "prediction": [["location.location.containedby"], ["location.administrative_division.country"], ["location.location.partially_containedby"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do most of the people live in russia [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.administrative_division.country </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.021946653723716736, -0.02642342634499073, -0.04186014086008072], "norm_scores": [0.336042195558548, 0.3345411717891693, 0.3294166326522827]}}
{"id": "WebQTest-1736", "question": "where is holy roman empire located", "prediction": [["location.location.contains"], ["location.location.containedby"], ["fictional_universe.fictional_setting.languages", "language.human_language.main_country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is holy roman empire located [/INST]", "raw_output": {"paths": ["<PATH> location.location.contains </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> fictional_universe.fictional_setting.languages <SEP> language.human_language.main_country </PATH>"], "scores": [-0.018122786656022072, -0.0205129012465477, -0.044121403247117996], "norm_scores": [0.336479514837265, 0.33567625284194946, 0.3278442323207855]}}
{"id": "WebQTest-1737", "question": "what movies has liam hemsworth played in", "prediction": [["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.performance.film"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["people.person.place_of_birth", "film.film.featured_film_locations"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has liam hemsworth played in [/INST]", "raw_output": {"paths": ["<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.0269634947180748, -0.030880611389875412, -0.03656225651502609], "norm_scores": [0.33483588695526123, 0.3335268497467041, 0.33163726329803467]}}
{"id": "WebQTest-1738", "question": "who plays nicholas newman on the young and the restless", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.tv_program.country_of_origin", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays nicholas newman on the young and the restless [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.021750347688794136, -0.026114894077181816, -0.02839561551809311], "norm_scores": [0.3345576226711273, 0.3331005871295929, 0.3323417603969574]}}
{"id": "WebQTest-1739", "question": "who is the voice of eric cartman on south park", "prediction": [["film.performance.special_performance_type", "film.performance.actor"], ["film.performance.special_performance_type", "film.actor.film"], ["tv.regular_tv_appearance.special_performance_type", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program_creator.programs_created"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the voice of eric cartman on south park [/INST]", "raw_output": {"paths": ["<PATH> film.performance.special_performance_type <SEP> film.performance.actor </PATH>", "<PATH> film.performance.special_performance_type <SEP> film.actor.film </PATH>", "<PATH> tv.regular_tv_appearance.special_performance_type <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.018039455637335777, -0.027788391336798668, -0.029643472284078598], "norm_scores": [0.33570995926856995, 0.33245304226875305, 0.33183690905570984]}}
{"id": "WebQTest-1740", "question": "who is angelina jolies brother", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.nationality", "people.person.place_of_birth"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.sibling_s", "people.sibling_relationship.sibling"], ["people.person.place_of_birth", "people.person.place_of_birth"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.parents", "people.person.parents"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["film.producer.films_executive_produced", "film.producer.films_executive_produced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is angelina jolies brother [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.nationality <SEP> people.person.place_of_birth </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.022215720266103745, -0.033167533576488495, -0.039474885910749435], "norm_scores": [0.33647412061691284, 0.3328092396259308, 0.33071669936180115]}}
{"id": "WebQTest-1741", "question": "where is the headquarters of bank of america", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["location.location.containedby"]], "ground_paths": [["common.image.appears_in_topic_gallery", "location.location.containedby"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the headquarters of bank of america [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.004973794799298048, -0.03777604177594185, -0.05234326422214508], "norm_scores": [0.34229403734207153, 0.3312481641769409, 0.32645779848098755]}}
{"id": "WebQTest-1742", "question": "what type of government japan have", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government japan have [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.010510708205401897, -0.014276997186243534, -0.1296599954366684], "norm_scores": [0.3467507064342499, 0.34544721245765686, 0.3078020513057709]}}
{"id": "WebQTest-1744", "question": "what team does josh hamilton play for", "prediction": [["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does josh hamilton play for [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.014125673100352287, -0.01623760350048542, -0.03080587089061737], "norm_scores": [0.3354186713695526, 0.33471107482910156, 0.3298702538013458]}}
{"id": "WebQTest-1746", "question": "where is burundi country", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.river.basin_countries"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is burundi country [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.river.basin_countries </PATH>"], "scores": [-0.003469649236649275, -0.0504915677011013, -0.06656569987535477], "norm_scores": [0.34567153453826904, 0.32979363203048706, 0.3245348334312439]}}
{"id": "WebQTest-1748", "question": "through which countries of the sahel does the niger river flow", "prediction": [["location.location.partially_containedby"], ["geography.river.basin_countries"], ["location.location.partially_contains"]], "ground_paths": [["location.location.partially_containedby"], ["geography.river.basin_countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: through which countries of the sahel does the niger river flow [/INST]", "raw_output": {"paths": ["<PATH> location.location.partially_containedby </PATH>", "<PATH> geography.river.basin_countries </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.017582925036549568, -0.018373675644397736, -0.03112027980387211], "norm_scores": [0.3349227011203766, 0.3346579670906067, 0.33041927218437195]}}
{"id": "WebQTest-1750", "question": "what city does paul ryan live in", "prediction": [["people.person.place_of_birth"], ["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city does paul ryan live in [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.021012764424085617, -0.02108062244951725, -0.038455214351415634], "norm_scores": [0.3352733254432678, 0.335250586271286, 0.3294760584831238]}}
{"id": "WebQTest-1751", "question": "what city is acadia university in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city is acadia university in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.006965718697756529, -0.04300941899418831, -0.0503990612924099], "norm_scores": [0.342220276594162, 0.3301050364971161, 0.3276746869087219]}}
{"id": "WebQTest-1752", "question": "what did f. scott fitzgerald do", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did f. scott fitzgerald do [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0006000566645525396, -0.07860756665468216, -0.08046175539493561], "norm_scores": [0.3510987460613251, 0.32475143671035767, 0.32414981722831726]}}
{"id": "WebQTest-1753", "question": "what countries does tomtom western europe include", "prediction": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["common.topic.notable_types"], ["location.location.containedby"]], "ground_paths": [["base.schemastaging.organization_extra.phone_number", "base.schemastaging.phone_sandbox.service_location"], ["organization.organization.phone_number", "common.phone_number.service_location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries does tomtom western europe include [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.organization_extra.phone_number <SEP> base.schemastaging.phone_sandbox.service_location </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.008831674233078957, -0.022236067801713943, -0.04540196806192398], "norm_scores": [0.33889397978782654, 0.3343816101551056, 0.32672441005706787]}}
{"id": "WebQTest-1754", "question": "what is the national language of iran", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the national language of iran [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.017946142703294754, -0.021823624148964882, -0.028363073244690895], "norm_scores": [0.33492231369018555, 0.3336261808872223, 0.33145153522491455]}}
{"id": "WebQTest-1755", "question": "where did the second battle of ypres occur", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["time.event.locations"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the second battle of ypres occur [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.008789436891674995, -0.021204469725489616, -0.06808643788099289], "norm_scores": [0.34128662943840027, 0.33707574009895325, 0.32163766026496887]}}
{"id": "WebQTest-1756", "question": "what languages do people speak in spain", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what languages do people speak in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013664312660694122, -0.023300116881728172, -0.028072863817214966], "norm_scores": [0.3360096216201782, 0.3327874541282654, 0.3312029242515564]}}
{"id": "WebQTest-1757", "question": "what did michael crabtree do", "prediction": [["american_football.football_player.games", "american_football.football_player_game_statistics.position_s"], ["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.role"], ["american_football.football_player.receiving", "american_football.football_player_receiving_statistics.position"]], "ground_paths": [["people.person.profession"], ["american_football.football_player.position_s"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did michael crabtree do [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.games <SEP> american_football.football_player_game_statistics.position_s </PATH>", "<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.role </PATH>", "<PATH> american_football.football_player.receiving <SEP> american_football.football_player_receiving_statistics.position </PATH>"], "scores": [-0.029897838830947876, -0.03509053960442543, -0.03912297263741493], "norm_scores": [0.3349367678165436, 0.333202064037323, 0.33186113834381104]}}
{"id": "WebQTest-1758", "question": "where do they speak afrikaans", "prediction": [["location.country.languages_spoken"], ["language.human_language.main_country"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where do they speak afrikaans [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.004654319025576115, -0.04379790648818016, -0.04955413565039635], "norm_scores": [0.34273502230644226, 0.32957831025123596, 0.3276866376399994]}}
{"id": "WebQTest-1759", "question": "what 3 states border florida", "prediction": [["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.partially_contains", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"]], "ground_paths": [["location.location.partially_contains", "location.location.partially_contains"], ["location.administrative_division.first_level_division_of", "location.administrative_division.first_level_division_of"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.partially_containedby", "location.location.partially_contains"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.cyclone_affected_area.cyclones"], ["location.location.containedby", "location.location.contains"], ["location.location.events", "time.event.locations"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.contains", "location.location.contains"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization_scope.organizations_with_this_scope"], ["location.location.time_zones", "location.location.time_zones"], ["location.administrative_division.first_level_division_of", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["meteorology.tropical_cyclone.affected_areas", "meteorology.tropical_cyclone.affected_areas"], ["location.location.partially_contains", "location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 3 states border florida [/INST]", "raw_output": {"paths": ["<PATH> meteorology.tropical_cyclone.affected_areas <SEP> meteorology.cyclone_affected_area.cyclones </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>", "<PATH> organization.organization_scope.organizations_with_this_scope <SEP> organization.organization.geographic_scope </PATH>"], "scores": [-0.026956012472510338, -0.03545951470732689, -0.03776688128709793], "norm_scores": [0.335482656955719, 0.33264198899269104, 0.3318753242492676]}}
{"id": "WebQTest-1760", "question": "who plays ziva david on ncis", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_character.appeared_in_tv_episodes", "tv.tv_guest_role.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["tv.tv_guest_role.character", "tv.tv_guest_role.actor"], ["tv.tv_guest_role.character", "tv.tv_actor.guest_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays ziva david on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.021499114111065865, -0.030307257547974586, -0.03261702135205269], "norm_scores": [0.33555084466934204, 0.33260825276374817, 0.3318409025669098]}}
{"id": "WebQTest-1761", "question": "who ruled after king henry viii died", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.religion", "people.person.religion"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who ruled after king henry viii died [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.religion <SEP> people.person.religion </PATH>"], "scores": [-0.02032582089304924, -0.03835747018456459, -0.04461963474750519], "norm_scores": [0.33805152773857117, 0.33201053738594055, 0.3299379050731659]}}
{"id": "WebQTest-1763", "question": "who are the st louis cardinals coaches", "prediction": [["baseball.baseball_team.current_coaches"], ["sports.sports_team.coaches", "sports.sports_team_coach_tenure.coach"], ["sports.sports_team.coaches", "sports.sports_team_coach.teams_coached"]], "ground_paths": [["baseball.baseball_team.current_coaches", "baseball.baseball_coach.current_team_coaching"], ["baseball.current_coaching_tenure.baseball_team", "baseball.baseball_coach.current_team_coaching"], ["baseball.baseball_team.current_coaches", "baseball.current_coaching_tenure.baseball_coach"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the st louis cardinals coaches [/INST]", "raw_output": {"paths": ["<PATH> baseball.baseball_team.current_coaches </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach_tenure.coach </PATH>", "<PATH> sports.sports_team.coaches <SEP> sports.sports_team_coach.teams_coached </PATH>"], "scores": [-0.016569754108786583, -0.032345402985811234, -0.04274908825755119], "norm_scores": [0.33800816535949707, 0.3327177166938782, 0.32927414774894714]}}
{"id": "WebQTest-1764", "question": "where is daud ibrahim", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth", "location.location.containedby"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is daud ibrahim [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.016009226441383362, -0.021961141377687454, -0.042222850024700165], "norm_scores": [0.3369053602218628, 0.33490607142448425, 0.32818859815597534]}}
{"id": "WebQTest-1765", "question": "what region is turkey considered", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.locations.countries.continent"]], "ground_paths": [["base.locations.countries.continent"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what region is turkey considered [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-0.012308306992053986, -0.02881571091711521, -0.048340510576963425], "norm_scores": [0.3391857147216797, 0.33363258838653564, 0.3271816670894623]}}
{"id": "WebQTest-1766", "question": "what language do russians speak", "prediction": [["language.human_language.countries_spoken_in"], ["language.human_language.main_country"], ["location.country.languages_spoken"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["people.ethnicity.geographic_distribution", "language.human_language.main_country"], ["people.ethnicity.geographic_distribution", "location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["people.ethnicity.geographic_distribution", "language.human_language.countries_spoken_in"], ["people.ethnicity.geographic_distribution", "location.country.official_language"], ["people.ethnicity.languages_spoken"], ["people.ethnicity.included_in_group", "people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do russians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.014577066525816917, -0.020393742248415947, -0.028486769646406174], "norm_scores": [0.3355269134044647, 0.33358094096183777, 0.3308921754360199]}}
{"id": "WebQTest-1767", "question": "where was martin luther king jr buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was martin luther king jr buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.00344562204554677, -0.030560681596398354, -0.0855417400598526], "norm_scores": [0.34549087285995483, 0.3362487256526947, 0.31826046109199524]}}
{"id": "WebQTest-1768", "question": "what are the official languages of argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the official languages of argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.008485127240419388, -0.02451285347342491, -0.047159720212221146], "norm_scores": [0.3394244313240051, 0.33402758836746216, 0.32654792070388794]}}
{"id": "WebQTest-1769", "question": "where is the city wales", "prediction": [["location.location.containedby"], ["base.biblioness.bibs_location.country"], ["location.administrative_division.country"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_children"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the city wales [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.biblioness.bibs_location.country </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.018725689500570297, -0.02131195180118084, -0.03529580309987068], "norm_scores": [0.33545976877212524, 0.3345932960510254, 0.32994693517684937]}}
{"id": "WebQTest-1770", "question": "what country does rafael nadal play for", "prediction": [["people.person.nationality"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country does rafael nadal play for [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.01666979491710663, -0.02149702049791813, -0.030870622023940086], "norm_scores": [0.3354484736919403, 0.33383309841156006, 0.3307184875011444]}}
{"id": "WebQTest-1771", "question": "what is the islamic belief", "prediction": [["religion.belief.belief_of"], ["religion.religion.beliefs"], ["belief.belief.belief_of"]], "ground_paths": [["religion.religion.beliefs"], ["religion.belief.belief_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the islamic belief [/INST]", "raw_output": {"paths": ["<PATH> religion.belief.belief_of </PATH>", "<PATH> religion.religion.beliefs </PATH>", "<PATH> belief.belief.belief_of </PATH>"], "scores": [-0.008029847405850887, -0.01939920149743557, -0.11209552735090256], "norm_scores": [0.346037358045578, 0.34212544560432434, 0.31183716654777527]}}
{"id": "WebQTest-1772", "question": "who is the prime minister of spain 2011", "prediction": [["people.person.nationality"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["government.government_position_held.basic_title", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the prime minister of spain 2011 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.008885341696441174, -0.019330160692334175, -0.03012254647910595], "norm_scores": [0.3368595540523529, 0.33335942029953003, 0.32978102564811707]}}
{"id": "WebQTest-1773", "question": "who was the son of king leonidas", "prediction": [["people.person.gender"], ["people.person.children"], ["people.person.parents"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was the son of king leonidas [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.00944360252469778, -0.028724828734993935, -0.042644817382097244], "norm_scores": [0.33918455243110657, 0.33270731568336487, 0.32810813188552856]}}
{"id": "WebQTest-1774", "question": "who was judi dench married to", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children", "people.person.children"]], "ground_paths": [["people.person.spouse_s", "people.person.spouse_s"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.profession", "people.person.profession"], ["people.person.nationality", "people.person.nationality"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was judi dench married to [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> people.person.children <SEP> people.person.children </PATH>"], "scores": [-0.0205291286110878, -0.027911819517612457, -0.046225015074014664], "norm_scores": [0.337009459733963, 0.3345305621623993, 0.3284600079059601]}}
{"id": "WebQTest-1775", "question": "how many languages do they speak in spain", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: how many languages do they speak in spain [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.015885666012763977, -0.018654031679034233, -0.03238329663872719], "norm_scores": [0.3354721665382385, 0.33454474806785583, 0.32998308539390564]}}
{"id": "WebQTest-1776", "question": "where are the sahara desert located", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.contains"]], "ground_paths": [["book.written_work.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the sahara desert located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0024655319284647703, -0.05279356613755226, -0.07146439701318741], "norm_scores": [0.3467111587524414, 0.3296937048435211, 0.3235951364040375]}}
{"id": "WebQTest-1777", "question": "what is rihanna mum called", "prediction": [["people.person.parents"], ["people.person.children"], ["music.artist.origin"]], "ground_paths": [["people.person.parents"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is rihanna mum called [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.012500989250838757, -0.015252312645316124, -0.12647022306919098], "norm_scores": [0.346076101064682, 0.3451252579689026, 0.3087986707687378]}}
{"id": "WebQTest-1779", "question": "when did president theodore roosevelt take office", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_holder"], ["government.us_president.vice_president"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did president theodore roosevelt take office [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.027707824483513832, -0.034620530903339386, -0.034997813403606415], "norm_scores": [0.3349132537841797, 0.33260607719421387, 0.33248063921928406]}}
{"id": "WebQTest-1780", "question": "who does michael oher play for in 2010", "prediction": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["sports.sports_league_draft_pick.player", "sports.professional_sports_team.draft_picks"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does michael oher play for in 2010 [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.receiving <SEP> american_football.player_receiving_statistics.team </PATH>", "<PATH> american_football.football_player.games <SEP> american_football.player_game_statistics.team </PATH>", "<PATH> american_football.player_receiving_statistics.player <SEP> american_football.player_receiving_statistics.team </PATH>"], "scores": [-0.016611699014902115, -0.01847463846206665, -0.01918562315404415], "norm_scores": [0.33382648229599, 0.3332051634788513, 0.3329683542251587]}}
{"id": "WebQTest-1781", "question": "who or what influenced albert einstein", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who or what influenced albert einstein [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced </PATH>"], "scores": [-0.01152485515922308, -0.012197231873869896, -0.10296999663114548], "norm_scores": [0.34341374039649963, 0.34318292140960693, 0.31340330839157104]}}
{"id": "WebQTest-1782", "question": "where does egypt export to", "prediction": [["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["location.location.partially_contains", "location.location.partially_contains"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["base.schemastaging.disputed_location.claimed_by", "base.schemastaging.disputed_location_claimant.disputed_territories"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.events", "location.location.events"], ["location.location.partially_contains", "location.location.containedby"], ["location.location.contains", "location.location.contains"], ["common.topic.notable_types", "common.topic.notable_types"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.contains_major_portion_of", "base.locations.countries.continent"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["location.location.partially_contains", "location.location.partially_containedby"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does egypt export to [/INST]", "raw_output": {"paths": ["<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_area_type <SEP> base.aareas.schema.administrative_area.administrative_area_type </PATH>"], "scores": [-0.026336558163166046, -0.02880224399268627, -0.03195883333683014], "norm_scores": [0.33423230051994324, 0.33340921998023987, 0.3323584496974945]}}
{"id": "WebQTest-1784", "question": "where is the seat of the african union", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["location.location.containedby"], ["organization.organization_founder.organizations_founded", "location.location.containedby"]], "ground_paths": [["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is the seat of the african union [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> organization.organization_founder.organizations_founded <SEP> location.location.containedby </PATH>"], "scores": [-0.00961949024349451, -0.03444775938987732, -0.04057387262582779], "norm_scores": [0.339558869600296, 0.3312320113182068, 0.3292090892791748]}}
{"id": "WebQTest-1785", "question": "what was kim richards in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was kim richards in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.018370812758803368, -0.0369989313185215, -0.037767913192510605], "norm_scores": [0.3375716507434845, 0.3313415050506592, 0.33108681440353394]}}
{"id": "WebQTest-1788", "question": "what artistic style did henri rousseau work in", "prediction": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_artist.art_forms"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what artistic style did henri rousseau work in [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.associated_periods_or_movements </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_artist.art_forms </PATH>"], "scores": [-0.0019909886177629232, -0.04025021567940712, -0.05142667889595032], "norm_scores": [0.3431438207626343, 0.3302633762359619, 0.3265927731990814]}}
{"id": "WebQTest-1789", "question": "what is john edwards indicted for", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.election_campaign.candidate", "government.election_campaign.offense"]], "ground_paths": [["base.crime.acquitted_person.acquittal", "base.crime.acquittal.crime_type"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is john edwards indicted for [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.election_campaign.candidate <SEP> government.election_campaign.offense </PATH>"], "scores": [-0.021940909326076508, -0.025847431272268295, -0.04304097965359688], "norm_scores": [0.33610936999320984, 0.3347989022731781, 0.32909172773361206]}}
{"id": "WebQTest-1790", "question": "where was huey newton killed", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.deceased_person.place_of_death", "base.biblioness.bibs_location.city"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was huey newton killed [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.deceased_person.place_of_death <SEP> base.biblioness.bibs_location.city </PATH>"], "scores": [-0.0003538225719239563, -0.07678407430648804, -0.105842724442482], "norm_scores": [0.35381922125816345, 0.32778432965278625, 0.3183964490890503]}}
{"id": "WebQTest-1791", "question": "when was the printing press invented by gutenberg", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["book.book_edition.author_editor"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the printing press invented by gutenberg [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.014372734352946281, -0.025774255394935608, -0.031131668016314507], "norm_scores": [0.33646878600120544, 0.33265429735183716, 0.3308769166469574]}}
{"id": "WebQTest-1792", "question": "where is midlothian scotland", "prediction": [["location.location.containedby"], ["location.location.contains"], ["base.aareas.schema.administrative_area.administrative_children"]], "ground_paths": [["base.aareas.schema.administrative_area.administrative_parent"], ["location.administrative_division.second_level_division_of"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is midlothian scotland [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children </PATH>"], "scores": [-0.002659434685483575, -0.049333762377500534, -0.06331472098827362], "norm_scores": [0.3453580439090729, 0.32960909605026245, 0.32503288984298706]}}
{"id": "WebQTest-1793", "question": "where does marta play soccer", "prediction": [["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does marta play soccer [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.02676895074546337, -0.03254340961575508, -0.03739317134022713], "norm_scores": [0.33515721559524536, 0.3332274556159973, 0.33161529898643494]}}
{"id": "WebQTest-1794", "question": "where is mitt romney 's family from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["people.person.parents"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is mitt romney 's family from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.013566926121711731, -0.024079622700810432, -0.043802373111248016], "norm_scores": [0.33786532282829285, 0.33433204889297485, 0.3278026878833771]}}
{"id": "WebQTest-1795", "question": "what are john terry 's children called", "prediction": [["people.person.children"], ["people.person.parents"], ["people.person.parent"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are john terry 's children called [/INST]", "raw_output": {"paths": ["<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>", "<PATH> people.person.parent </PATH>"], "scores": [-0.00920872949063778, -0.018476564437150955, -0.1403554379940033], "norm_scores": [0.34869155287742615, 0.34547486901283264, 0.3058335781097412]}}
{"id": "WebQTest-1796", "question": "what city was michael jackson born in", "prediction": [["location.location.people_born_here"], ["people.person.place_of_birth"], ["music.artist.origin"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what city was michael jackson born in [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.011394601315259933, -0.020200414583086967, -0.04047183692455292], "norm_scores": [0.3375442922115326, 0.3345849812030792, 0.32787075638771057]}}
{"id": "WebQTest-1797", "question": "who fought in the battle of vicksburg", "prediction": [["time.event.locations"], ["base.culturalevent.event.entity_involved"], ["military.military_conflict.military_personnel_involved"]], "ground_paths": [["base.culturalevent.event.entity_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who fought in the battle of vicksburg [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>", "<PATH> military.military_conflict.military_personnel_involved </PATH>"], "scores": [-0.01683778688311577, -0.030481752008199692, -0.04595228284597397], "norm_scores": [0.3380943834781647, 0.3335127830505371, 0.3283928334712982]}}
{"id": "WebQTest-1799", "question": "what kind of writing is nikolai gogol famous for", "prediction": [["book.written_work.author"], ["book.author.works_written"], ["book.author.school_or_movement"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of writing is nikolai gogol famous for [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.author.works_written </PATH>", "<PATH> book.author.school_or_movement </PATH>"], "scores": [-0.016521573066711426, -0.023202458396553993, -0.03028515912592411], "norm_scores": [0.3356074094772339, 0.33337271213531494, 0.33101987838745117]}}
{"id": "WebQTest-1800", "question": "what year was first world series", "prediction": [["time.event.instance_of_recurring_event"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was first world series [/INST]", "raw_output": {"paths": ["<PATH> time.event.instance_of_recurring_event </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.championship </PATH>"], "scores": [-0.012491525150835514, -0.017115246504545212, -0.05458556115627289], "norm_scores": [0.33850494027137756, 0.3369434177875519, 0.32455164194107056]}}
{"id": "WebQTest-1801", "question": "who plays simon birch", "prediction": [["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["award.award_nomination.nominated_for", "award.award_nomination.award_nominee"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["award.award_honor.honored_for", "award.award_winner.awards_won"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays simon birch [/INST]", "raw_output": {"paths": ["<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.022113151848316193, -0.02852466143667698, -0.04121510684490204], "norm_scores": [0.33616968989372253, 0.3340212106704712, 0.32980912923812866]}}
{"id": "WebQTest-1802", "question": "where are the timezones in the usa", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["base.locations.countries.continent"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the timezones in the usa [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> base.locations.countries.continent </PATH>"], "scores": [-2.4394232241320424e-05, -0.1230454370379448, -0.15159837901592255], "norm_scores": [0.3644846975803375, 0.32229378819465637, 0.3132214844226837]}}
{"id": "WebQTest-1803", "question": "what does gm make", "prediction": [["automotive.company.make_s"], ["automotive.make.parent_company"], ["common.topic.subjects"]], "ground_paths": [["business.business_operation.industry"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does gm make [/INST]", "raw_output": {"paths": ["<PATH> automotive.company.make_s </PATH>", "<PATH> automotive.make.parent_company </PATH>", "<PATH> common.topic.subjects </PATH>"], "scores": [-0.011711282655596733, -0.019028447568416595, -0.07310548424720764], "norm_scores": [0.34092897176742554, 0.33844342827796936, 0.32062751054763794]}}
{"id": "WebQTest-1804", "question": "what do people speak in uk", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people speak in uk [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.013687845319509506, -0.022040316835045815, -0.03601502999663353], "norm_scores": [0.33674541115760803, 0.3339444398880005, 0.3293101489543915]}}
{"id": "WebQTest-1805", "question": "what do people in jamaica speak", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in jamaica speak [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.011768712662160397, -0.01854318380355835, -0.045009955763816833], "norm_scores": [0.3377746641635895, 0.33549413084983826, 0.3267311453819275]}}
{"id": "WebQTest-1806", "question": "what position did lebron james play", "prediction": [["basketball.basketball_player.position_s"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.position"], ["sports.sports_position.sport", "sports.sports_team.sport"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team_roster.position"], ["basketball.basketball_player.position_s"], ["basketball.basketball_position.players"], ["sports.pro_athlete.teams", "sports.sports_team_roster.position"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did lebron james play [/INST]", "raw_output": {"paths": ["<PATH> basketball.basketball_player.position_s </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.position </PATH>", "<PATH> sports.sports_position.sport <SEP> sports.sports_team.sport </PATH>"], "scores": [-0.007970232516527176, -0.034401725977659225, -0.04710293933749199], "norm_scores": [0.34065309166908264, 0.3317670524120331, 0.32757988572120667]}}
{"id": "WebQTest-1807", "question": "what was wayne gretzky 's first team", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was wayne gretzky 's first team [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.008315084502100945, -0.02032974548637867, -0.03445792198181152], "norm_scores": [0.3375808894634247, 0.33354923129081726, 0.32886993885040283]}}
{"id": "WebQTest-1808", "question": "who is the president of peru now", "prediction": [["people.person.nationality"], ["government.government_position_held.jurisdiction_of_office", "government.politician.government_positions_held"], ["government.governmental_jurisdiction.governing_officials", "government.government_position_held.office_holder"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the president of peru now [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> government.government_position_held.jurisdiction_of_office <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.governmental_jurisdiction.governing_officials <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.004119338467717171, -0.03625279292464256, -0.04372405633330345], "norm_scores": [0.3413497507572174, 0.33055537939071655, 0.3280948996543884]}}
{"id": "WebQTest-1809", "question": "what kind of guitar did jerry garcia play", "prediction": [["music.group_member.instruments_played"], ["music.guitarist.guitars_played"], ["music.guitar.guitarists"]], "ground_paths": [["music.guitarist.guitars_played"], ["music.guitar.guitarists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of guitar did jerry garcia play [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.guitarist.guitars_played </PATH>", "<PATH> music.guitar.guitarists </PATH>"], "scores": [-0.012762410566210747, -0.02656441740691662, -0.0329870842397213], "norm_scores": [0.3371235728263855, 0.332502543926239, 0.3303738534450531]}}
{"id": "WebQTest-1810", "question": "what type of art leonardo da vinci do", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["visual_art.visual_art_form.artists"]], "ground_paths": [["visual_art.visual_artist.art_forms"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of art leonardo da vinci do [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> visual_art.visual_art_form.artists </PATH>"], "scores": [-0.00021548631775658578, -0.08995717018842697, -0.1000109389424324], "norm_scores": [0.3547118306159973, 0.3242659866809845, 0.3210222125053406]}}
{"id": "WebQTest-1811", "question": "who are the senators of new jersey now", "prediction": [["location.administrative_division.first_level_division_of", "people.person.nationality"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "ground_paths": [["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["location.location.containedby", "people.person.nationality"], ["government.political_district.representatives", "government.politician.government_positions_held"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the senators of new jersey now [/INST]", "raw_output": {"paths": ["<PATH> location.administrative_division.first_level_division_of <SEP> people.person.nationality </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.political_district.representatives <SEP> government.politician.government_positions_held </PATH>"], "scores": [-0.017619643360376358, -0.018105706200003624, -0.021805647760629654], "norm_scores": [0.33385229110717773, 0.3336900472640991, 0.33245769143104553]}}
{"id": "WebQTest-1812", "question": "where was rihanna raised", "prediction": [["location.location.people_born_here"], ["music.artist.origin"], ["people.person.nationality"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was rihanna raised [/INST]", "raw_output": {"paths": ["<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.019680935889482498, -0.021117374300956726, -0.025896521285176277], "norm_scores": [0.3341834843158722, 0.33370381593704224, 0.33211272954940796]}}
{"id": "WebQTest-1813", "question": "what team does messi play for 2011", "prediction": [["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["soccer.football_player.statistics", "soccer.football_player_stats.team"]], "ground_paths": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_goal.scorer", "soccer.football_goal.point_awarded_to"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["people.person.nationality", "sports.sports_team_location.teams"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_award.award_winner", "sports.sports_award.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"], ["base.x2010fifaworldcupsouthafrica.current_world_cup_squad.players", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does messi play for 2011 [/INST]", "raw_output": {"paths": ["<PATH> soccer.football_goal.scorer <SEP> soccer.football_goal.point_awarded_to </PATH>", "<PATH> soccer.football_player_match_participation.player <SEP> soccer.football_player_match_participation.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_player_stats.team </PATH>"], "scores": [-0.024470271542668343, -0.02637310139834881, -0.03892683610320091], "norm_scores": [0.3351491093635559, 0.33451199531555176, 0.33033883571624756]}}
{"id": "WebQTest-1815", "question": "where does lil wayne go to school", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.education", "education.education.institution"], ["education.education.student", "education.educational_institution.students_graduates"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does lil wayne go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.016352685168385506, -0.01636591926217079, -0.028837351128458977], "norm_scores": [0.33471909165382385, 0.3347146809101105, 0.3305662274360657]}}
{"id": "WebQTest-1817", "question": "where did jrr tolkien go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "ground_paths": [["education.education.student", "education.educational_institution.students_graduates"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["business.employment_tenure.person", "business.employment_tenure.company"], ["people.person.employment_history", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did jrr tolkien go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.011565555818378925, -0.018942249938845634, -0.03395190089941025], "norm_scores": [0.33664220571517944, 0.33416804671287537, 0.3291897475719452]}}
{"id": "WebQTest-1818", "question": "what state is saint louis university in", "prediction": [["location.location.containedby"], ["education.educational_institution.subsidiary_or_constituent_schools", "location.location.containedby"], ["education.educational_institution.parent_institution", "location.location.containedby"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state is saint louis university in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> education.educational_institution.subsidiary_or_constituent_schools <SEP> location.location.containedby </PATH>", "<PATH> education.educational_institution.parent_institution <SEP> location.location.containedby </PATH>"], "scores": [-0.006146067287772894, -0.03364177048206329, -0.03985878452658653], "norm_scores": [0.34016749262809753, 0.33094173669815063, 0.32889068126678467]}}
{"id": "WebQTest-1819", "question": "what religion does canada follow", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["royalty.monarch.kingdom", "people.person.religion"], ["book.newspaper.circulation_areas", "book.periodical_subject.periodicals"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion does canada follow [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>"], "scores": [-0.012839995324611664, -0.021453268826007843, -0.02703448198735714], "norm_scores": [0.33587145805358887, 0.3329909145832062, 0.33113759756088257]}}
{"id": "WebQTest-1821", "question": "which countries have spanish as their main language", "prediction": [["location.country.official_language"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries have spanish as their main language [/INST]", "raw_output": {"paths": ["<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.008883022703230381, -0.02690301090478897, -0.03306492790579796], "norm_scores": [0.33803772926330566, 0.33200085163116455, 0.329961359500885]}}
{"id": "WebQTest-1822", "question": "who is in paul ryan 's family", "prediction": [["people.person.parents"], ["people.person.children"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.children"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.parents"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.parents", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is in paul ryan 's family [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.020680874586105347, -0.025480758398771286, -0.033969663083553314], "norm_scores": [0.33534419536590576, 0.33373844623565674, 0.3309173583984375]}}
{"id": "WebQTest-1823", "question": "what government does iraq use", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what government does iraq use [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.006033963989466429, -0.022274665534496307, -0.13191181421279907], "norm_scores": [0.34896546602249146, 0.3433437943458557, 0.30769070982933044]}}
{"id": "WebQTest-1824", "question": "what type of currency do they use in spain", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of currency do they use in spain [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.0036177372094243765, -0.03153054416179657, -0.13055320084095], "norm_scores": [0.35047584772109985, 0.3408283591270447, 0.30869582295417786]}}
{"id": "WebQTest-1825", "question": "what did lincoln steffens wrote", "prediction": [["book.author.works_written"], ["book.written_work.author"], ["book.book_edition.author_editor"]], "ground_paths": [["book.author.works_written"], ["book.written_work.author"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did lincoln steffens wrote [/INST]", "raw_output": {"paths": ["<PATH> book.author.works_written </PATH>", "<PATH> book.written_work.author </PATH>", "<PATH> book.book_edition.author_editor </PATH>"], "scores": [-0.013130946084856987, -0.015534704551100731, -0.0737864300608635], "norm_scores": [0.3402811288833618, 0.3394641578197479, 0.32025474309921265]}}
{"id": "WebQTest-1826", "question": "what schools did anders celsius attend", "prediction": [["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what schools did anders celsius attend [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01801229454576969, -0.019951436668634415, -0.02252718061208725], "norm_scores": [0.33405062556266785, 0.33340349793434143, 0.33254584670066833]}}
{"id": "WebQTest-1827", "question": "what can do in new york", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.neighborhood.neighborhood_of"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what can do in new york [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.014135248027741909, -0.01649320498108864, -0.033759310841560364], "norm_scores": [0.33577194809913635, 0.33498114347457886, 0.32924696803092957]}}
{"id": "WebQTest-1828", "question": "what currency does france accept", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does france accept [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.007667110301554203, -0.02291770651936531, -0.09588629007339478], "norm_scores": [0.34477701783180237, 0.33955883979797363, 0.3156641125679016]}}
{"id": "WebQTest-1829", "question": "what type of religion did massachusetts have", "prediction": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["religion.religion.states"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of religion did massachusetts have [/INST]", "raw_output": {"paths": ["<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> religion.religion.states </PATH>"], "scores": [-0.0032739765010774136, -0.03888731449842453, -0.0820126011967659], "norm_scores": [0.3461052179336548, 0.33399614691734314, 0.3198986351490021]}}
{"id": "WebQTest-1830", "question": "what language do they speak in colombia south america", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do they speak in colombia south america [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.008367256261408329, -0.018280556425452232, -0.05895295739173889], "norm_scores": [0.3400425910949707, 0.3366883099079132, 0.3232690989971161]}}
{"id": "WebQTest-1833", "question": "what kind of government does brazil has", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does brazil has [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.0038332315161824226, -0.028593776747584343, -0.1727951020002365], "norm_scores": [0.3545992970466614, 0.34592702984809875, 0.29947370290756226]}}
{"id": "WebQTest-1835", "question": "what country did gregor mendel live in", "prediction": [["people.deceased_person.place_of_death"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what country did gregor mendel live in [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.015388071537017822, -0.016842836514115334, -0.046593405306339264], "norm_scores": [0.33694735169410706, 0.3364575207233429, 0.32659512758255005]}}
{"id": "WebQTest-1836", "question": "where laura ingalls wilder lived", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["people.deceased_person.place_of_death"]], "ground_paths": [["people.deceased_person.place_of_death"], ["people.person.parents", "people.deceased_person.place_of_death"], ["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"], ["book.author.works_written", "fictional_universe.work_of_fiction.setting"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.administrative_division.first_level_division_of"], ["book.book_edition.author_editor", "book.written_work.subjects"], ["influence.influence_node.influenced", "location.location.people_born_here"], ["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where laura ingalls wilder lived [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.deceased_person.place_of_death </PATH>"], "scores": [-0.02584589272737503, -0.028866631910204887, -0.029065418988466263], "norm_scores": [0.3340270519256592, 0.33301958441734314, 0.33295339345932007]}}
{"id": "WebQTest-1838", "question": "what sports are popular in germany", "prediction": [["sports.sports_team.location"], ["sports.sports_team_location.teams"], ["time.event.locations"]], "ground_paths": [["sports.sports_team_location.teams"], ["cricket.cricket_team.country"], ["sports.sports_team.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what sports are popular in germany [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.location </PATH>", "<PATH> sports.sports_team_location.teams </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.010935421101748943, -0.014811971224844456, -0.13682235777378082], "norm_scores": [0.34748217463493347, 0.34613773226737976, 0.30638009309768677]}}
{"id": "WebQTest-1839", "question": "what 's my timezone in oklahoma", "prediction": [["location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 's my timezone in oklahoma [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> location.location.time_zones <PATH>", "</PATH> location.location.time_zones </PATH>"], "scores": [-9.40089284995338e-06, -0.18986788392066956, -0.19104820489883423], "norm_scores": [0.3769066631793976, 0.3117305338382721, 0.3113628029823303]}}
{"id": "WebQTest-1840", "question": "what do they speak in norway", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do they speak in norway [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.01720811054110527, -0.018936991691589355, -0.031962476670742035], "norm_scores": [0.33516258001327515, 0.3345836102962494, 0.3302537500858307]}}
{"id": "WebQTest-1842", "question": "what currency does singapore use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country", "location.country.currency_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency does singapore use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country <SEP> location.country.currency_used </PATH>"], "scores": [-0.003967090975493193, -0.03211700916290283, -0.13666439056396484], "norm_scores": [0.35112693905830383, 0.3413805663585663, 0.3074924647808075]}}
{"id": "WebQTest-1843", "question": "what did the blackfoot indians speak", "prediction": [["people.ethnicity.languages_spoken"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.languages_spoken", "language.human_language.region", "language.human_language.region"]], "ground_paths": [["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did the blackfoot indians speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.languages_spoken <SEP> language.human_language.region <SEP> language.human_language.region </PATH>"], "scores": [-0.001139242434874177, -0.06753897666931152, -0.07906147837638855], "norm_scores": [0.3495534062385559, 0.32709696888923645, 0.32334962487220764]}}
{"id": "WebQTest-1844", "question": "who plays robin hood in prince of thieves", "prediction": [["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["film.film.starring", "film.performance.actor"]], "ground_paths": [["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays robin hood in prince of thieves [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.film.starring <SEP> film.performance.actor </PATH>"], "scores": [-0.012696356512606144, -0.03540312871336937, -0.04916299507021904], "norm_scores": [0.33993494510650635, 0.33230310678482056, 0.3277619779109955]}}
{"id": "WebQTest-1845", "question": "what standard time is texas", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["location.location.time_Zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what standard time is texas [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_Zones </PATH>"], "scores": [-1.7264403140870854e-05, -0.1362423300743103, -0.1862919181585312], "norm_scores": [0.37000149488449097, 0.3228803873062134, 0.30711811780929565]}}
{"id": "WebQTest-1847", "question": "where does kurdish people live", "prediction": [["people.ethnicity.geographic_distribution"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["people.ethnicity.geographic_distribution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does kurdish people live [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.geographic_distribution </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0011342412326484919, -0.06204616650938988, -0.11817272007465363], "norm_scores": [0.3532998561859131, 0.3324219882488251, 0.3142782151699066]}}
{"id": "WebQTest-1848", "question": "what instrument does louis armstrong", "prediction": [["music.group_member.instruments_played"], ["music.artist.track_contributions", "music.track_contribution.role"], ["music.group_member.instruments_played", "music.group_member.instruments_played"]], "ground_paths": [["music.group_member.instruments_played"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what instrument does louis armstrong [/INST]", "raw_output": {"paths": ["<PATH> music.group_member.instruments_played </PATH>", "<PATH> music.artist.track_contributions <SEP> music.track_contribution.role </PATH>", "<PATH> music.group_member.instruments_played <SEP> music.group_member.instruments_played </PATH>"], "scores": [-0.00024003180442377925, -0.08088772743940353, -0.12403006106615067], "norm_scores": [0.3563685119152069, 0.32875657081604004, 0.3148748576641083]}}
{"id": "WebQTest-1849", "question": "who does alyson stoner play in camp rock", "prediction": [["film.actor.film", "film.film_character.portrayed_in_films"], ["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.performance.actor", "film.film_character.portrayed_in_films"]], "ground_paths": [["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"], ["film.film.starring", "film.performance.character"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does alyson stoner play in camp rock [/INST]", "raw_output": {"paths": ["<PATH> film.actor.film <SEP> film.film_character.portrayed_in_films </PATH>", "<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.performance.actor <SEP> film.film_character.portrayed_in_films </PATH>"], "scores": [-0.02529192715883255, -0.02540692128241062, -0.02550865337252617], "norm_scores": [0.333370178937912, 0.3333318531513214, 0.3332979679107666]}}
{"id": "WebQTest-1850", "question": "what religion does madonna practice", "prediction": [["people.person.religion"], ["religion.religion.notable_figures"], ["religion.religion.practices"]], "ground_paths": [["people.person.religion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religion does madonna practice [/INST]", "raw_output": {"paths": ["<PATH> people.person.religion </PATH>", "<PATH> religion.religion.notable_figures </PATH>", "<PATH> religion.religion.practices </PATH>"], "scores": [-0.002469763858243823, -0.04155232384800911, -0.10567671060562134], "norm_scores": [0.34920936822891235, 0.3358246386051178, 0.31496599316596985]}}
{"id": "WebQTest-1851", "question": "what countries are located near egypt", "prediction": [["location.location.events", "time.event.locations"], ["time.event.locations", "time.event.locations"], ["location.location.partially_contains", "location.location.partially_contains"]], "ground_paths": [["location.location.partially_contains", "location.location.partially_contains"], ["government.form_of_government.countries", "location.country.form_of_government"], ["travel.tour_operator.travel_destinations", "travel.tour_operator.travel_destinations"], ["location.location.events", "base.culturalevent.event.entity_involved"], ["location.location.partially_contains", "location.location.containedby"], ["time.event.locations", "time.event.locations"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["language.human_language.countries_spoken_in", "language.human_language.countries_spoken_in"], ["location.location.adjoin_s", "location.location.adjoin_s"], ["location.location.contains", "location.location.containedby"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["location.location.partially_containedby", "location.location.partially_contains"], ["location.location.events", "location.location.events"], ["location.location.contains", "location.location.contains"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.location.contains_major_portion_of", "base.locations.countries.continent"], ["location.location.partially_containedby", "location.location.contains"], ["location.adjoining_relationship.adjoins", "location.adjoining_relationship.adjoins"], ["language.human_language.countries_spoken_in", "language.human_language.main_country"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["base.culturalevent.event.entity_involved", "base.culturalevent.event.entity_involved"], ["location.location.contains_major_portion_of", "location.location.containedby"], ["base.schemastaging.disputed_location.claimed_by", "base.schemastaging.disputed_location_claimant.disputed_territories"], ["film.film.featured_film_locations", "film.film.featured_film_locations"], ["common.topic.notable_types", "common.topic.notable_types"], ["location.location.partially_contains", "location.location.partially_containedby"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["base.culturalevent.event.entity_involved", "location.location.events"], ["location.location.events", "time.event.locations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.location.time_zones", "location.location.time_zones"], ["location.location.partially_containedby", "location.location.partially_containedby"], ["location.country.languages_spoken", "language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what countries are located near egypt [/INST]", "raw_output": {"paths": ["<PATH> location.location.events <SEP> time.event.locations </PATH>", "<PATH> time.event.locations <SEP> time.event.locations </PATH>", "<PATH> location.location.partially_contains <SEP> location.location.partially_contains </PATH>"], "scores": [-0.04211157560348511, -0.051643531769514084, -0.05176354572176933], "norm_scores": [0.33546826243400574, 0.3322857916355133, 0.33224591612815857]}}
{"id": "WebQTest-1853", "question": "what made richard hammond famous", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what made richard hammond famous [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.001882322714664042, -0.047078777104616165, -0.11807766556739807], "norm_scores": [0.3513566553592682, 0.33583012223243713, 0.31281328201293945]}}
{"id": "WebQTest-1854", "question": "who originally voiced lois griffin", "prediction": [["film.performance.character", "film.actor.film"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_episodes", "tv.tv_actor.guest_roles"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["people.person.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who originally voiced lois griffin [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> tv.tv_character.appeared_in_tv_episodes <SEP> tv.tv_actor.guest_roles </PATH>", "<PATH> tv.regular_tv_appearance.character <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.0262954980134964, -0.028124729171395302, -0.03254292532801628], "norm_scores": [0.33423078060150146, 0.33361998200416565, 0.3321492075920105]}}
{"id": "WebQTest-1855", "question": "what airport do you fly into in paris", "prediction": [["aviation.airport.serves"], ["location.location.nearby_airports"], ["location.location.containedby"]], "ground_paths": [["aviation.airport.serves"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what airport do you fly into in paris [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.nearby_airports </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.006755617912858725, -0.02518102154135704, -0.05855279788374901], "norm_scores": [0.3411496579647064, 0.33492138981819153, 0.3239288926124573]}}
{"id": "WebQTest-1857", "question": "what rihanna new album called 2012", "prediction": [["music.album.artist"], ["music.artist.album"], ["music.recording.artist"]], "ground_paths": [["music.artist.album"], ["music.album.artist"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what rihanna new album called 2012 [/INST]", "raw_output": {"paths": ["<PATH> music.album.artist </PATH>", "<PATH> music.artist.album </PATH>", "<PATH> music.recording.artist </PATH>"], "scores": [-0.010067819617688656, -0.0215215515345335, -0.05501645803451538], "norm_scores": [0.3395979702472687, 0.33573049306869507, 0.3246714770793915]}}
{"id": "WebQTest-1858", "question": "where does dave ramsey live", "prediction": [["people.place_lived.person", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"], ["location.location.people_born_here"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.place_lived.person", "people.place_lived.location"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does dave ramsey live [/INST]", "raw_output": {"paths": ["<PATH> people.place_lived.person <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.025050543248653412, -0.02532123774290085, -0.033026959747076035], "norm_scores": [0.3342486321926117, 0.33415818214416504, 0.3315931558609009]}}
{"id": "WebQTest-1860", "question": "what discovery did galileo make", "prediction": [["astronomy.astronomer.astronomical_objects_discovered"], ["law.invention.inventor"], ["symbols.namesake.named_after"]], "ground_paths": [["astronomy.astronomical_discovery.discoverer"], ["astronomy.astronomer.astronomical_objects_discovered"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what discovery did galileo make [/INST]", "raw_output": {"paths": ["<PATH> astronomy.astronomer.astronomical_objects_discovered </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.012548038735985756, -0.036671917885541916, -0.04002843424677849], "norm_scores": [0.3390912711620331, 0.3310089707374573, 0.32989975810050964]}}
{"id": "WebQTest-1861", "question": "what is the government system of saudi arabia", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["location.location.contains", "location.country.form_of_government"]], "ground_paths": [["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the government system of saudi arabia [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> location.location.contains <SEP> location.country.form_of_government </PATH>"], "scores": [-0.010999924503266811, -0.0124648567289114, -0.09818524867296219], "norm_scores": [0.3430480659008026, 0.34254589676856995, 0.31440600752830505]}}
{"id": "WebQTest-1864", "question": "who plays the role of tony dinozzo on ncis", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.tv_character.appeared_in_tv_program", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays the role of tony dinozzo on ncis [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.01813821867108345, -0.030295561999082565, -0.031837575137615204], "norm_scores": [0.3362123966217041, 0.3321496844291687, 0.3316378891468048]}}
{"id": "WebQTest-1865", "question": "what did jesus do for living", "prediction": [["people.person.profession"], ["common.topic.notable_types"], ["people.profession.people_with_this_profession"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jesus do for living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> common.topic.notable_types </PATH>", "<PATH> people.profession.people_with_this_profession </PATH>"], "scores": [-0.0010229840409010649, -0.07580932229757309, -0.07849184423685074], "norm_scores": [0.35045939683914185, 0.32520589232444763, 0.3243347108364105]}}
{"id": "WebQTest-1866", "question": "what is the state name of new york city", "prediction": [["location.location.containedby"], ["base.aareas.schema.administrative_area.administrative_children", "location.location.containedby"], ["location.location.containedby", "location.location.containedby"]], "ground_paths": [["base.biblioness.bibs_location.state"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the state name of new york city [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.aareas.schema.administrative_area.administrative_children <SEP> location.location.containedby </PATH>", "<PATH> location.location.containedby <SEP> location.location.containedby </PATH>"], "scores": [-0.011346393264830112, -0.04768438637256622, -0.05115073174238205], "norm_scores": [0.341846227645874, 0.3296472132205963, 0.3285065293312073]}}
{"id": "WebQTest-1868", "question": "who plays knox overstreet", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"]], "ground_paths": [["film.performance.character", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays knox overstreet [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> tv.tv_character.appeared_in_tv_program <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.02697446569800377, -0.03408137336373329, -0.03648294135928154], "norm_scores": [0.33518186211586, 0.3328082263469696, 0.3320099115371704]}}
{"id": "WebQTest-1870", "question": "what movies did david carradine play in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.film.edited_by"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["tv.regular_tv_appearance.actor", "tv.tv_program.regular_cast"], ["award.award_winner.awards_won", "award.award_honor.honored_for"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["film.personal_film_appearance.person", "film.film.personal_appearances"], ["film.personal_film_appearance.person", "film.personal_film_appearance.film"], ["film.director.film"], ["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"], ["people.person.languages", "tv.tv_program.languages"], ["film.producer.films_executive_produced"], ["tv.tv_actor.starring_roles", "tv.tv_program.regular_cast"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies did david carradine play in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.film.starring </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.015173017978668213, -0.0359557680785656, -0.03743645176291466], "norm_scores": [0.33813321590423584, 0.3311784267425537, 0.3306884169578552]}}
{"id": "WebQTest-1871", "question": "what is the stanley cup named after", "prediction": [["sports.sports_championship_event.championship"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.champion", "sports.sports_team_owner.teams_owned"]], "ground_paths": [["symbols.name_source.namesakes"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the stanley cup named after [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.championship </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.champion <SEP> sports.sports_team_owner.teams_owned </PATH>"], "scores": [-0.015048396773636341, -0.017059186473488808, -0.0479324571788311], "norm_scores": [0.33719512820243835, 0.3365177810192108, 0.32628709077835083]}}
{"id": "WebQTest-1873", "question": "where is traditional chinese used", "prediction": [["location.country.languages_spoken"], ["location.country.official_language"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is traditional chinese used [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.01859351061284542, -0.023861227557063103, -0.02888159453868866], "norm_scores": [0.33506327867507935, 0.33330291509628296, 0.3316337764263153]}}
{"id": "WebQTest-1874", "question": "who does portugal trade with", "prediction": [["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"]], "ground_paths": [["government.form_of_government.countries", "location.country.form_of_government"], ["location.statistical_region.places_exported_to", "location.imports_and_exports.exported_to"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_participating_country.olympics_participated_in"], ["organization.organization.founders", "organization.organization_founder.organizations_founded"], ["olympics.olympic_games.participating_countries", "olympics.olympic_participating_country.olympics_participated_in"], ["base.culturalevent.event.entity_involved", "time.event.locations"], ["language.human_language.countries_spoken_in", "location.country.official_language"], ["organization.organization_scope.organizations_with_this_scope", "organization.organization.geographic_scope"], ["travel.travel_destination.tour_operators", "travel.travel_destination.tour_operators"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["location.statistical_region.places_exported_to", "location.statistical_region.places_imported_from"], ["location.country.form_of_government", "government.form_of_government.countries"], ["olympics.olympic_participating_country.olympics_participated_in", "olympics.olympic_games.participating_countries"], ["olympics.olympic_games.participating_countries", "olympics.olympic_games.participating_countries"], ["sports.sport_country.multi_event_tournaments_participated_in", "sports.sport_country.multi_event_tournaments_participated_in"], ["common.topic.notable_types", "common.topic.notable_types"], ["organization.organization.geographic_scope", "organization.organization.geographic_scope"], ["location.imports_and_exports.imported_from", "location.imports_and_exports.exported_to"], ["government.form_of_government.countries", "government.form_of_government.countries"], ["base.aareas.schema.administrative_area.administrative_parent", "base.aareas.schema.administrative_area.administrative_parent"], ["language.human_language.countries_spoken_in", "location.country.languages_spoken"], ["military.military_combatant.belongs_to_combatant_group", "military.military_combatant.includes_allies"], ["organization.organization.geographic_scope", "organization.organization_scope.organizations_with_this_scope"], ["organization.organization.founders", "organization.organization.founders"], ["travel.travel_destination.tour_operators", "travel.tour_operator.travel_destinations"], ["base.aareas.schema.administrative_area.administrative_area_type", "base.aareas.schema.administrative_area.administrative_area_type"], ["location.country.form_of_government", "location.country.form_of_government"], ["people.person.nationality", "location.location.people_born_here"], ["location.location.time_zones", "location.location.time_zones"], ["book.written_work.subjects", "book.written_work.subjects"], ["location.imports_and_exports.imported_from", "location.statistical_region.places_imported_from"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does portugal trade with [/INST]", "raw_output": {"paths": ["<PATH> base.aareas.schema.administrative_area.administrative_parent <SEP> base.aareas.schema.administrative_area.administrative_parent </PATH>", "<PATH> olympics.olympic_participating_country.olympics_participated_in <SEP> olympics.olympic_participating_country.olympics_participated_in </PATH>", "<PATH> location.imports_and_exports.imported_from <SEP> location.statistical_region.places_imported_from </PATH>"], "scores": [-0.029057228937745094, -0.031206918880343437, -0.03871627151966095], "norm_scores": [0.33464515209198, 0.3339265286922455, 0.3314283490180969]}}
{"id": "WebQTest-1875", "question": "what type of political system is brazil", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of political system is brazil [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries <PATH>"], "scores": [-0.004779531620442867, -0.025124531239271164, -0.17109309136867523], "norm_scores": [0.3537767827510834, 0.34665191173553467, 0.29957133531570435]}}
{"id": "WebQTest-1876", "question": "what language did the ancient babylonians speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language did the ancient babylonians speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.007776164449751377, -0.029981235042214394, -0.03603648394346237], "norm_scores": [0.33896297216415405, 0.331519216299057, 0.3295178711414337]}}
{"id": "WebQTest-1877", "question": "what kind of painter was wassily kandinsky", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists"], ["book.author.school_or_movement"]], "ground_paths": [["visual_art.visual_artist.associated_periods_or_movements"], ["visual_art.art_period_movement.associated_artists"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of painter was wassily kandinsky [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists </PATH>", "<PATH> book.author.school_or_movement </PATH>"], "scores": [-0.0033343711402267218, -0.03674113005399704, -0.06066974997520447], "norm_scores": [0.34347474575042725, 0.3321899175643921, 0.3243354260921478]}}
{"id": "WebQTest-1878", "question": "what form of government does the united states use", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does the united states use [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.form_of_government </PATH>"], "scores": [-0.007059078197926283, -0.018199557438492775, -0.148723766207695], "norm_scores": [0.35003790259361267, 0.3461599349975586, 0.3038021922111511]}}
{"id": "WebQTest-1879", "question": "what money does cuba use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["base.coinsdaily.coin_type.country"]], "ground_paths": [["location.country.currency_used"], ["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what money does cuba use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> base.coinsdaily.coin_type.country </PATH>"], "scores": [-0.012689735740423203, -0.014599029906094074, -0.1460210531949997], "norm_scores": [0.3480358123779297, 0.3473719358444214, 0.3045922815799713]}}
{"id": "WebQTest-1880", "question": "who is my representative in illinois house", "prediction": [["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.government_position_held.district_represented", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"]], "ground_paths": [["government.governmental_body.members", "government.politician.government_positions_held"], ["government.governmental_body.members", "government.government_position_held.office_holder"], ["government.government_position_held.governmental_body", "government.politician.government_positions_held"], ["government.government_position_held.governmental_body", "government.government_position_held.office_holder"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is my representative in illinois house [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.governmental_body <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.district_represented <SEP> government.politician.government_positions_held </PATH>", "<PATH> government.government_position_held.governmental_body <SEP> government.government_position_held.office_holder </PATH>"], "scores": [-0.021974338218569756, -0.03077063150703907, -0.03123406507074833], "norm_scores": [0.3353425860404968, 0.3324057161808014, 0.33225172758102417]}}
{"id": "WebQTest-1881", "question": "what timezone is sweden gmt", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what timezone is sweden gmt [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.003117624670267105, -0.03187873214483261, -0.13310560584068298], "norm_scores": [0.35090741515159607, 0.34095868468284607, 0.30813390016555786]}}
{"id": "WebQTest-1882", "question": "what was franklin d roosevelt 's job before president", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"], ["government.government_office_or_title.category", "government.government_office_or_title.jurisdiction", "base.aareas.schema.administrative_area.administrative_parent", "government.governmental_jurisdiction.government_positions"], ["government.government_position_held.basic_title", "government.government_position_held.jurisdiction_of_office", "government.governmental_jurisdiction.government_positions"]], "ground_paths": [["government.government_position_held.appointed_by", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_office_or_title.office_holders"], ["government.politician.government_positions_held", "government.government_position_held.office_position_or_title"], ["government.government_position_held.office_holder", "government.government_position_held.office_position_or_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was franklin d roosevelt 's job before president [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.office_position_or_title </PATH>", "<PATH> government.government_office_or_title.category <SEP> government.government_office_or_title.jurisdiction <SEP> base.aareas.schema.administrative_area.administrative_parent <SEP> government.governmental_jurisdiction.government_positions </PATH>", "<PATH> government.government_position_held.basic_title <SEP> government.government_position_held.jurisdiction_of_office <SEP> government.governmental_jurisdiction.government_positions </PATH>"], "scores": [-0.01871640607714653, -0.025209449231624603, -0.027997467666864395], "norm_scores": [0.3350881040096283, 0.3329194188117981, 0.331992506980896]}}
{"id": "WebQTest-1883", "question": "where did robert downey jr go to jail", "prediction": [["people.person.nationality", "location.location.containedby"], ["base.prison.imprisonment.prisoner", "base.prison.prison.prisoners"], ["people.person.languages", "location.country.official_language"]], "ground_paths": [["base.prison.imprisonment.prisoner", "base.prison.imprisonment.prison"], ["celebrities.rehab.celebrity", "celebrities.rehab.rehab_facility"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robert downey jr go to jail [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> base.prison.imprisonment.prisoner <SEP> base.prison.prison.prisoners </PATH>", "<PATH> people.person.languages <SEP> location.country.official_language </PATH>"], "scores": [-0.007023413199931383, -0.02618425153195858, -0.050304923206567764], "norm_scores": [0.34029075503349304, 0.3338325619697571, 0.3258766531944275]}}
{"id": "WebQTest-1884", "question": "what degrees did barack obama get", "prediction": [["education.education.student", "education.education.degree"], ["people.person.education", "education.education.degree"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees did barack obama get [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01109500601887703, -0.01982779987156391, -0.026984846219420433], "norm_scores": [0.3360733389854431, 0.333151251077652, 0.3307753801345825]}}
{"id": "WebQTest-1885", "question": "what does god shiva represent", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["common.topic.notable_types"]], "ground_paths": [["religion.deity.deity_of"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what does god shiva represent [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.011071786284446716, -0.024738438427448273, -0.053806450217962265], "norm_scores": [0.3396056294441223, 0.33499592542648315, 0.32539841532707214]}}
{"id": "WebQTest-1886", "question": "what inventions were made by ben franklin", "prediction": [["law.inventor.inventions"], ["law.invention.inventor"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.inventor.inventions"], ["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what inventions were made by ben franklin [/INST]", "raw_output": {"paths": ["<PATH> law.inventor.inventions </PATH>", "<PATH> law.invention.inventor </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.007610599976032972, -0.021153714507818222, -0.07275722175836563], "norm_scores": [0.34205830097198486, 0.33745700120925903, 0.3204847276210785]}}
{"id": "WebQTest-1887", "question": "what is the name of the currency used in switzerland", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["common.topic.notable_types"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the name of the currency used in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.005134786944836378, -0.024688567966222763, -0.16248305141925812], "norm_scores": [0.35272839665412903, 0.34589821100234985, 0.3013734221458435]}}
{"id": "WebQTest-1888", "question": "what year did the milwaukee brewers go to the world series", "prediction": [["sports.sports_championship_event.champion"], ["sports.sports_team.championships"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the milwaukee brewers go to the world series [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.005822635721415281, -0.02095661126077175, -0.04675178974866867], "norm_scores": [0.3395727276802063, 0.33447232842445374, 0.3259549140930176]}}
{"id": "WebQTest-1889", "question": "who was vice president with bill clinton", "prediction": [["government.us_vice_president.to_president"], ["government.us_president.vice_president"], ["people.person.profession", "people.person.profession"]], "ground_paths": [["government.us_vice_president.to_president"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was vice president with bill clinton [/INST]", "raw_output": {"paths": ["<PATH> government.us_vice_president.to_president </PATH>", "<PATH> government.us_president.vice_president </PATH>", "<PATH> people.person.profession <SEP> people.person.profession </PATH>"], "scores": [-0.018423086032271385, -0.021209359169006348, -0.037209007889032364], "norm_scores": [0.33572739362716675, 0.3347932696342468, 0.32947930693626404]}}
{"id": "WebQTest-1890", "question": "what songs did franz liszt compose", "prediction": [["music.composition.composer"], ["music.lyricist.lyrics_written"], ["music.composition.lyricist"]], "ground_paths": [["book.author.works_written"], ["music.composition.composer"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what songs did franz liszt compose [/INST]", "raw_output": {"paths": ["<PATH> music.composition.composer </PATH>", "<PATH> music.lyricist.lyrics_written </PATH>", "<PATH> music.composition.lyricist </PATH>"], "scores": [-0.003083609277382493, -0.06122496351599693, -0.06200578808784485], "norm_scores": [0.34646472334861755, 0.32689520716667175, 0.3266400694847107]}}
{"id": "WebQTest-1891", "question": "where did the russian japanese war happen", "prediction": [["time.event.locations"], ["location.location.events"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the russian japanese war happen [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.011202811263501644, -0.018589047715067863, -0.06124400347471237], "norm_scores": [0.3396933972835541, 0.3371935784816742, 0.32311302423477173]}}
{"id": "WebQTest-1892", "question": "what year did the la kings win the cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.champion"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did the la kings win the cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.00820511020720005, -0.016041425988078117, -0.11859260499477386], "norm_scores": [0.34629857540130615, 0.3435954749584198, 0.31010591983795166]}}
{"id": "WebQTest-1893", "question": "what kind of money does egypt have", "prediction": [["finance.currency.countries_used"], ["location.country.currency_used"], ["finance.currency.countries_formerly_used"]], "ground_paths": [["finance.currency.countries_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of money does egypt have [/INST]", "raw_output": {"paths": ["<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_formerly_used </PATH>"], "scores": [-0.002378626726567745, -0.04339437186717987, -0.08578405529260635], "norm_scores": [0.34724727272987366, 0.33329281210899353, 0.3194598853588104]}}
{"id": "WebQTest-1894", "question": "what baseball team was jackie robinson on", "prediction": [["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["baseball.batting_statistics.player", "baseball.batting_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["baseball.baseball_player.batting_stats", "baseball.batting_statistics.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what baseball team was jackie robinson on [/INST]", "raw_output": {"paths": ["<PATH> baseball.batting_statistics.player <SEP> baseball.batting_statistics.team </PATH>", "<PATH> baseball.baseball_player.batting_stats <SEP> baseball.batting_statistics.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>"], "scores": [-0.01353282481431961, -0.01512821577489376, -0.03307625278830528], "norm_scores": [0.33567723631858826, 0.3351421356201172, 0.32918062806129456]}}
{"id": "WebQTest-1895", "question": "what body of water does st lawrence river flow into", "prediction": [["geography.river.mouth"], ["geography.body_of_water.mouth"], ["location.location.containedby", "geography.lake.basin_countries"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what body of water does st lawrence river flow into [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> geography.body_of_water.mouth </PATH>", "<PATH> location.location.containedby <SEP> geography.lake.basin_countries </PATH>"], "scores": [-0.002702520927414298, -0.05207892134785652, -0.07006200402975082], "norm_scores": [0.34641849994659424, 0.3297290503978729, 0.3238524794578552]}}
{"id": "WebQTest-1896", "question": "what form of government does the united states have", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["base.locations.countries.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does the united states have [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> base.locations.countries.form_of_government </PATH>"], "scores": [-0.007058082148432732, -0.018199410289525986, -0.1470794528722763], "norm_scores": [0.3498631715774536, 0.34598687291145325, 0.30414995551109314]}}
{"id": "WebQTest-1897", "question": "what are the names of ariel 's six sisters", "prediction": [["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"]], "ground_paths": [["fictional_universe.fictional_character.siblings", "fictional_universe.fictional_character.siblings"], ["common.topic.notable_types", "common.topic.notable_types"], ["fictional_universe.fictional_character.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"], ["fictional_universe.fictional_character.gender", "fictional_universe.fictional_character.gender"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.fictional_character.siblings"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.parents"], ["fictional_universe.fictional_character.parents", "fictional_universe.fictional_character.children"], ["fictional_universe.sibling_relationship_of_fictional_characters.siblings", "fictional_universe.sibling_relationship_of_fictional_characters.siblings"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of ariel 's six sisters [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.children </PATH>", "<PATH> fictional_universe.fictional_character.parents <SEP> fictional_universe.fictional_character.parents </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> fictional_universe.fictional_character.gender </PATH>"], "scores": [-0.01639690436422825, -0.02243891730904579, -0.02671423926949501], "norm_scores": [0.3351529836654663, 0.33313408493995667, 0.33171290159225464]}}
{"id": "WebQTest-1898", "question": "what voices does seth macfarlane play on family guy", "prediction": [["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.character"], ["tv.tv_program.regular_cast", "tv.tv_character.appeared_in_tv_program"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_universe.characters"], ["fictional_universe.work_of_fiction.part_of_these_fictional_universes", "fictional_universe.fictional_character.appears_in_these_fictional_universes"], ["tv.tv_program_creator.programs_created", "fictional_universe.fictional_character.character_created_by"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.character"], ["tv.regular_tv_appearance.series", "tv.tv_character.appeared_in_tv_program"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what voices does seth macfarlane play on family guy [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_character.appears_in_these_fictional_universes </PATH>", "<PATH> fictional_universe.work_of_fiction.part_of_these_fictional_universes <SEP> fictional_universe.fictional_universe.characters </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_character.appeared_in_tv_program </PATH>"], "scores": [-0.019266832619905472, -0.02151665836572647, -0.02382591925561428], "norm_scores": [0.33409014344215393, 0.3333393633365631, 0.33257049322128296]}}
{"id": "WebQTest-1899", "question": "who was william mckinley married to", "prediction": [["people.person.parents", "people.person.parents"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["base.famouspets.pet_ownership.owner", "base.famouspets.pet_ownership.owner"]], "ground_paths": [["base.famouspets.pet_ownership.owner", "base.famouspets.pet_owner.pets_owned"], ["people.person.profession", "common.topic.notable_types"], ["base.famouspets.pet_owner.pets_owned", "base.famouspets.pet_owner.pets_owned"], ["people.person.nationality", "people.person.nationality"], ["people.person.children", "people.person.parents"], ["people.person.spouse_s", "people.marriage.spouse"], ["people.person.children", "people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was william mckinley married to [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents <SEP> people.person.parents </PATH>", "<PATH> base.famouspets.pet_ownership.owner <SEP> base.famouspets.pet_owner.pets_owned </PATH>", "<PATH> base.famouspets.pet_ownership.owner <SEP> base.famouspets.pet_ownership.owner </PATH>"], "scores": [-0.037388015538454056, -0.03781239315867424, -0.038806188851594925], "norm_scores": [0.33353808522224426, 0.3333965539932251, 0.333065390586853]}}
{"id": "WebQTest-1900", "question": "who played darth vader in the first movie", "prediction": [["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.performance.actor"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played darth vader in the first movie [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>", "<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>"], "scores": [-0.01912013255059719, -0.020410608500242233, -0.024076009169220924], "norm_scores": [0.33402734994888306, 0.3335965573787689, 0.33237606287002563]}}
{"id": "WebQTest-1901", "question": "where did kevin james go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did kevin james go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.013585752807557583, -0.01762930490076542, -0.02587101422250271], "norm_scores": [0.3351482152938843, 0.3337957561016083, 0.33105599880218506]}}
{"id": "WebQTest-1902", "question": "what year was hitler was born", "prediction": [["people.person.place_of_birth"], ["military.military_person.participated_in_conflicts"], ["base.activism.activist.area_of_activism", "time.event.locations"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was hitler was born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> military.military_person.participated_in_conflicts </PATH>", "<PATH> base.activism.activist.area_of_activism <SEP> time.event.locations </PATH>"], "scores": [-0.011152076534926891, -0.03746633231639862, -0.04458335414528847], "norm_scores": [0.34000304341316223, 0.33117279410362244, 0.3288242220878601]}}
{"id": "WebQTest-1904", "question": "what 's there to see in atlanta", "prediction": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "ground_paths": [["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what 's there to see in atlanta [/INST]", "raw_output": {"paths": ["<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.015727121382951736, -0.017996177077293396, -0.022835124284029007], "norm_scores": [0.33437541127204895, 0.33361753821372986, 0.3320070803165436]}}
{"id": "WebQTest-1905", "question": "where was auschwitz concentration camp located", "prediction": [["location.location.containedby"], ["location.location.contains"], ["time.event.locations"]], "ground_paths": [["location.location.contains"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was auschwitz concentration camp located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> time.event.locations </PATH>"], "scores": [-0.0019972913432866335, -0.06457126885652542, -0.08926238864660263], "norm_scores": [0.35016730427742004, 0.3289274275302887, 0.32090526819229126]}}
{"id": "WebQTest-1907", "question": "what kind of currency does dominican republic use", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.form_of_government"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of currency does dominican republic use [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.form_of_government </PATH>"], "scores": [-8.52250013849698e-05, -0.10461331158876419, -0.12947271764278412], "norm_scores": [0.3597920835018158, 0.32408255338668823, 0.31612536311149597]}}
{"id": "WebQTest-1909", "question": "what is spoken in czech republic", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is spoken in czech republic [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.011875534430146217, -0.01909446530044079, -0.04350007325410843], "norm_scores": [0.3376465141773224, 0.33521783351898193, 0.3271356523036957]}}
{"id": "WebQTest-1910", "question": "where was shakespeare born at", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was shakespeare born at [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.012175415642559528, -0.014631242491304874, -0.08571583032608032], "norm_scores": [0.3416880965232849, 0.34084999561309814, 0.31746193766593933]}}
{"id": "WebQTest-1911", "question": "where did scott fitzgerald go to college", "prediction": [["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"], ["education.education.student", "education.education.institution"]], "ground_paths": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did scott fitzgerald go to college [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>"], "scores": [-0.013069716282188892, -0.015571005642414093, -0.026694320142269135], "norm_scores": [0.33512404561042786, 0.3342868685722351, 0.33058908581733704]}}
{"id": "WebQTest-1912", "question": "what did joan crawford died of", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["deceased_person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did joan crawford died of [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> deceased_person.cause_of_death </PATH>"], "scores": [-0.002452787244692445, -0.03611714020371437, -0.12244700640439987], "norm_scores": [0.35040733218193054, 0.33880746364593506, 0.3107852041721344]}}
{"id": "WebQTest-1913", "question": "where did rob kardashian go to school", "prediction": [["people.person.education", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "ground_paths": [["people.person.place_of_birth", "location.location.containedby"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did rob kardashian go to school [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>"], "scores": [-0.014521870762109756, -0.018231991678476334, -0.0270028468221426], "norm_scores": [0.3351326286792755, 0.33389154076576233, 0.33097583055496216]}}
{"id": "WebQTest-1914", "question": "what year was allen iverson mvp", "prediction": [["award.award_honor.award_winner", "award.award_honor.season"], ["sports.sports_award_winner.awards", "sports.sports_award.season"], ["award.award_winner.awards", "award.award_honor.honored_for"]], "ground_paths": [["sports.sports_award_winner.awards", "sports.sports_league_season.awards"], ["basketball.basketball_player.player_statistics", "basketball.basketball_player_stats.season"], ["sports.sports_award.award_winner", "sports.sports_award.season"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year was allen iverson mvp [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.season </PATH>", "<PATH> sports.sports_award_winner.awards <SEP> sports.sports_award.season </PATH>", "<PATH> award.award_winner.awards <SEP> award.award_honor.honored_for </PATH>"], "scores": [-0.02850775606930256, -0.03019089065492153, -0.04112129285931587], "norm_scores": [0.33492040634155273, 0.3343571722507477, 0.3307223916053772]}}
{"id": "WebQTest-1915", "question": "where did harry s truman go to school", "prediction": [["book.written_work.subjects", "location.location.containedby"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"]], "ground_paths": [["education.education.student", "education.education.institution"], ["people.person.education", "education.educational_institution.students_graduates"], ["people.person.education", "education.education.institution"], ["book.written_work.subjects", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did harry s truman go to school [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.subjects <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.educational_institution.students_graduates </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.010849679820239544, -0.02685559168457985, -0.03358608856797218], "norm_scores": [0.3376505970954895, 0.3322891891002655, 0.3300602436065674]}}
{"id": "WebQTest-1916", "question": "what county is utica ny in", "prediction": [["location.location.containedby"], ["location.us_county.hud_county_place"], ["location.hud_county_place.county"]], "ground_paths": [["location.us_county.county_seat"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what county is utica ny in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.us_county.hud_county_place </PATH>", "<PATH> location.hud_county_place.county </PATH>"], "scores": [-0.023032912984490395, -0.023187419399619102, -0.02344408817589283], "norm_scores": [0.33339619636535645, 0.3333446681499481, 0.33325913548469543]}}
{"id": "WebQTest-1917", "question": "what did charles babbage discover", "prediction": [["law.invention.inventor"], ["symbols.namesake.named_after"], ["base.argumentmaps.innovator.original_ideas"]], "ground_paths": [["law.invention.inventor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did charles babbage discover [/INST]", "raw_output": {"paths": ["<PATH> law.invention.inventor </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> base.argumentmaps.innovator.original_ideas </PATH>"], "scores": [-0.003011495340615511, -0.05402486398816109, -0.06086757406592369], "norm_scores": [0.34553632140159607, 0.3283514082431793, 0.3261122703552246]}}
{"id": "WebQTest-1919", "question": "who plays king julian madagascar", "prediction": [["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"], ["film.film_character.portrayed_in_films", "film.performance.actor"]], "ground_paths": [["film.performance.film", "film.performance.actor"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays king julian madagascar [/INST]", "raw_output": {"paths": ["<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> film.film_character.portrayed_in_films <SEP> film.performance.actor </PATH>"], "scores": [-0.029968861490488052, -0.03592275083065033, -0.039328016340732574], "norm_scores": [0.33503660559654236, 0.3330478072166443, 0.33191555738449097]}}
{"id": "WebQTest-1920", "question": "who is rick scott", "prediction": [["people.person.profession"], ["government.politician.government_positions_held", "government.government_position_held.office_holder"], ["government.politician.government_positions_held"]], "ground_paths": [["people.profession.people_with_this_profession"], ["people.person.profession"], ["common.topic.notable_types"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is rick scott [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.office_holder </PATH>", "<PATH> government.politician.government_positions_held </PATH>"], "scores": [-0.0018353555351495743, -0.05780141055583954, -0.07699079066514969], "norm_scores": [0.3480475842952728, 0.3291037976741791, 0.32284867763519287]}}
{"id": "WebQTest-1921", "question": "where did the crimean war take place", "prediction": [["time.event.locations"], ["location.location.events"], ["book.written_work.subjects"]], "ground_paths": [["time.event.locations"], ["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did the crimean war take place [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> location.location.events </PATH>", "<PATH> book.written_work.subjects </PATH>"], "scores": [-0.009589118883013725, -0.01978156901896, -0.09363110363483429], "norm_scores": [0.34373095631599426, 0.3402452766895294, 0.31602373719215393]}}
{"id": "WebQTest-1922", "question": "what offices did theodore roosevelt hold", "prediction": [["business.employment_tenure.person", "business.employment_tenure.title"], ["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["common.topic.notable_types", "type.type.expected_by"]], "ground_paths": [["government.government_position_held.office_holder", "government.government_position_held.basic_title"], ["government.government_position_held.office_holder", "government.government_office_category.officeholders"], ["business.employment_tenure.person", "business.employment_tenure.title"], ["government.politician.government_positions_held", "government.government_position_held.basic_title"], ["common.topic.notable_types", "type.type.expected_by"], ["government.government_position_held.appointed_by", "government.government_position_held.basic_title"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what offices did theodore roosevelt hold [/INST]", "raw_output": {"paths": ["<PATH> business.employment_tenure.person <SEP> business.employment_tenure.title </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.basic_title </PATH>", "<PATH> common.topic.notable_types <SEP> type.type.expected_by </PATH>"], "scores": [-0.016198528930544853, -0.018307160586118698, -0.027593983337283134], "norm_scores": [0.3348330557346344, 0.3341277837753296, 0.331039160490036]}}
{"id": "WebQTest-1923", "question": "what movies has miley cyrus been in", "prediction": [["people.person.nationality", "film.film.country"], ["people.person.languages", "film.film.language"], ["music.artist.genre", "music.album.genre"]], "ground_paths": [["film.actor.film", "film.performance.film"], ["film.person_or_entity_appearing_in_film.films", "film.film.personal_appearances"], ["people.person.children", "film.film.executive_produced_by"], ["people.person.languages", "tv.tv_program.languages"], ["music.artist.concert_tours", "music.album.supporting_tours"], ["music.artist.album", "media_common.adaptation.adapted_from"], ["music.artist.origin", "film.film.featured_film_locations"], ["film.performance.actor", "film.performance.film"], ["people.person.children", "film.producer.film"], ["music.artist.genre", "media_common.netflix_title.netflix_genres"], ["people.person.children", "film.film.produced_by"], ["people.person.parents", "film.film.produced_by"], ["film.film_featured_song.performed_by", "film.film.featured_song"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["music.lyricist.lyrics_written", "film.film.featured_song"], ["film.film.music"], ["music.artist.genre", "music.album.genre"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["common.topic.notable_types", "common.topic.notable_types"], ["people.person.languages", "film.film.language"], ["music.recording_contribution.contributor", "music.recording_contribution.album"], ["film.actor.film", "film.film.starring"], ["music.concert_tour.artist", "music.album.supporting_tours"], ["film.performance.actor", "film.film.starring"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies has miley cyrus been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> people.person.languages <SEP> film.film.language </PATH>", "<PATH> music.artist.genre <SEP> music.album.genre </PATH>"], "scores": [-0.027643008157610893, -0.03014339506626129, -0.036498647183179855], "norm_scores": [0.3345951735973358, 0.333759605884552, 0.3316451907157898]}}
{"id": "WebQTest-1925", "question": "who did james franco play in milk", "prediction": [["people.person.gender", "fictional_universe.fictional_character.gender"], ["film.actor.film", "film.performance.character"], ["film.performance.actor", "film.performance.character"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did james franco play in milk [/INST]", "raw_output": {"paths": ["<PATH> people.person.gender <SEP> fictional_universe.fictional_character.gender </PATH>", "<PATH> film.actor.film <SEP> film.performance.character </PATH>", "<PATH> film.performance.actor <SEP> film.performance.character </PATH>"], "scores": [-0.01903843693435192, -0.026888448745012283, -0.033328816294670105], "norm_scores": [0.3357967436313629, 0.3331710398197174, 0.3310322165489197]}}
{"id": "WebQTest-1926", "question": "when did the libyan conflict start", "prediction": [["time.event.locations"], ["time.event.includes_event"], ["time.event.included_in_event"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did the libyan conflict start [/INST]", "raw_output": {"paths": ["<PATH> time.event.locations </PATH>", "<PATH> time.event.includes_event </PATH>", "<PATH> time.event.included_in_event </PATH>"], "scores": [-0.010000878944993019, -0.026572365313768387, -0.03877531737089157], "norm_scores": [0.33838650584220886, 0.3328251242637634, 0.3287883698940277]}}
{"id": "WebQTest-1928", "question": "what wnba team is diana taurasi on", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what wnba team is diana taurasi on [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.018282586708664894, -0.025734703987836838, -0.029644234105944633], "norm_scores": [0.33542659878730774, 0.33293625712394714, 0.3316371738910675]}}
{"id": "WebQTest-1929", "question": "what do see in paris", "prediction": [["travel.travel_destination.tourist_attractions"], ["travel.tourist_attraction.near_travel_destination"], ["location.location.containedby"]], "ground_paths": [["base.arthist.helynevek.intezmeny"], ["travel.tourist_attraction.near_travel_destination"], ["travel.travel_destination.tourist_attractions"], ["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do see in paris [/INST]", "raw_output": {"paths": ["<PATH> travel.travel_destination.tourist_attractions </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>", "<PATH> location.location.containedby </PATH>"], "scores": [-0.013607712462544441, -0.015496166422963142, -0.040639813989400864], "norm_scores": [0.33653679490089417, 0.3359018564224243, 0.3275613486766815]}}
{"id": "WebQTest-1932", "question": "who made the laws in canada", "prediction": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_jurisdiction.government"], ["government.governmental_body.jurisdiction"]], "ground_paths": [["government.governmental_jurisdiction.government_bodies"], ["government.governmental_body.jurisdiction"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who made the laws in canada [/INST]", "raw_output": {"paths": ["<PATH> government.governmental_jurisdiction.government_bodies </PATH>", "<PATH> government.governmental_jurisdiction.government </PATH>", "<PATH> government.governmental_body.jurisdiction </PATH>"], "scores": [-0.01696205697953701, -0.027995700016617775, -0.0476444810628891], "norm_scores": [0.3379736542701721, 0.3342650532722473, 0.32776129245758057]}}
{"id": "WebQTest-1933", "question": "what year did adam morrison get drafted", "prediction": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"], ["basketball.basketball_player_stats.player", "basketball.basketball_player_stats.season"], ["sports.drafted_athlete.drafted", "sports.sports_league_draft.picks"]], "ground_paths": [["sports.sports_league_draft_pick.player", "sports.sports_league_draft_pick.draft"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what year did adam morrison get drafted [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_league_draft_pick.player <SEP> sports.sports_league_draft_pick.draft </PATH>", "<PATH> basketball.basketball_player_stats.player <SEP> basketball.basketball_player_stats.season </PATH>", "<PATH> sports.drafted_athlete.drafted <SEP> sports.sports_league_draft.picks </PATH>"], "scores": [-0.01961233839392662, -0.02227761782705784, -0.022787289693951607], "norm_scores": [0.33398255705833435, 0.3330936133861542, 0.33292388916015625]}}
{"id": "WebQTest-1934", "question": "where michael jackson from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["music.artist.origin"]], "ground_paths": [["music.artist.origin"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where michael jackson from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> music.artist.origin </PATH>"], "scores": [-0.00601132633164525, -0.035008665174245834, -0.04738740622997284], "norm_scores": [0.34119361639022827, 0.3314419686794281, 0.327364444732666]}}
{"id": "WebQTest-1935", "question": "what college did maya moore go to", "prediction": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what college did maya moore go to [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.012653493322432041, -0.020771585404872894, -0.025171810761094093], "norm_scores": [0.3356296420097351, 0.3329159915447235, 0.331454336643219]}}
{"id": "WebQTest-1936", "question": "when was the last time the dodgers won a championship", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.mascot.team"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the dodgers won a championship [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.mascot.team </PATH>"], "scores": [-0.008974652737379074, -0.014626200310885906, -0.09960072487592697], "norm_scores": [0.34391164779663086, 0.34197351336479187, 0.31411486864089966]}}
{"id": "WebQTest-1937", "question": "who is captain james cook facts", "prediction": [["people.person.profession"], ["symbols.namesake.named_after"], ["symbols.name_source.namesakes"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is captain james cook facts [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> symbols.namesake.named_after </PATH>", "<PATH> symbols.name_source.namesakes </PATH>"], "scores": [-0.0018879352137446404, -0.06295812875032425, -0.07505520433187485], "norm_scores": [0.34840747714042664, 0.3277668356895447, 0.3238257169723511]}}
{"id": "WebQTest-1938", "question": "where did robin williams get married", "prediction": [["people.person.children", "people.person.place_of_birth"], ["people.marriage.spouse", "people.marriage.location_of_ceremony"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["people.person.spouse_s", "people.marriage.location_of_ceremony"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did robin williams get married [/INST]", "raw_output": {"paths": ["<PATH> people.person.children <SEP> people.person.place_of_birth </PATH>", "<PATH> people.marriage.spouse <SEP> people.marriage.location_of_ceremony </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.0233119148761034, -0.02483263984322548, -0.0288716908544302], "norm_scores": [0.3341200649738312, 0.3336123526096344, 0.3322675824165344]}}
{"id": "WebQTest-1939", "question": "what form of government does canada", "prediction": [["location.country.form_of_government"], ["government.form_of_government.countries"], ["form_of_government.countries"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what form of government does canada [/INST]", "raw_output": {"paths": ["<PATH> location.country.form_of_government </PATH>", "<PATH> government.form_of_government.countries </PATH>", "<PATH> form_of_government.countries </PATH>"], "scores": [-0.007824872620403767, -0.01819479465484619, -0.169418066740036], "norm_scores": [0.35205432772636414, 0.34842240810394287, 0.29952332377433777]}}
{"id": "WebQTest-1940", "question": "who do islamic people follow", "prediction": [["religion.religion.deities"], ["religion.deity.deity_of"], ["people.person.religion"]], "ground_paths": [["religion.deity.deity_of"], ["religion.religion.deities"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who do islamic people follow [/INST]", "raw_output": {"paths": ["<PATH> religion.religion.deities </PATH>", "<PATH> religion.deity.deity_of </PATH>", "<PATH> people.person.religion </PATH>"], "scores": [-0.005432638339698315, -0.032490313053131104, -0.0723777562379837], "norm_scores": [0.3438137471675873, 0.3346356749534607, 0.3215506076812744]}}
{"id": "WebQTest-1941", "question": "what kind of government does peru", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of government does peru [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.009969182312488556, -0.015059483237564564, -0.14259041845798492], "norm_scores": [0.34834474325180054, 0.346576064825058, 0.3050791919231415]}}
{"id": "WebQTest-1942", "question": "who was in the israeli palestinian conflict", "prediction": [["time.event.people_involved"], ["people.person.nationality"], ["people.person.ethnicity"]], "ground_paths": [["time.event.people_involved"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who was in the israeli palestinian conflict [/INST]", "raw_output": {"paths": ["<PATH> time.event.people_involved </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> people.person.ethnicity </PATH>"], "scores": [-0.0006041489541530609, -0.071600541472435, -0.11037240922451019], "norm_scores": [0.3536684513092041, 0.32942986488342285, 0.31690168380737305]}}
{"id": "WebQTest-1943", "question": "who founded the afl", "prediction": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"], ["organization.organization.spun_off_from"]], "ground_paths": [["organization.organization.founders"], ["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who founded the afl [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.founders </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> organization.organization.spun_off_from </PATH>"], "scores": [-0.001379448571242392, -0.05246337130665779, -0.1078222319483757], "norm_scores": [0.35097256302833557, 0.33349376916885376, 0.3155336380004883]}}
{"id": "WebQTest-1944", "question": "where did diego velazquez die", "prediction": [["people.deceased_person.place_of_death"], ["people.person.place_of_birth"], ["people.place_of_death.people_deceased_here"]], "ground_paths": [["people.deceased_person.place_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did diego velazquez die [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.place_of_birth </PATH>", "<PATH> people.place_of_death.people_deceased_here </PATH>"], "scores": [-0.00024277677584905177, -0.11085249483585358, -0.11285806447267532], "norm_scores": [0.3585793972015381, 0.3210318684577942, 0.32038867473602295]}}
{"id": "WebQTest-1946", "question": "what is the country code for mexico", "prediction": [["location.country.internet_tld"], ["location.location.containedby"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.internet_tld"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is the country code for mexico [/INST]", "raw_output": {"paths": ["<PATH> location.country.internet_tld </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.017565570771694183, -0.02849193476140499, -0.03380756080150604], "norm_scores": [0.33635804057121277, 0.3327028751373291, 0.33093905448913574]}}
{"id": "WebQTest-1947", "question": "what currency do they use in switzerland", "prediction": [["location.country.currency_used"], ["finance.currency.countries_used"], ["location.country.currency_formerly_used"]], "ground_paths": [["location.country.currency_used"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what currency do they use in switzerland [/INST]", "raw_output": {"paths": ["<PATH> location.country.currency_used </PATH>", "<PATH> finance.currency.countries_used </PATH>", "<PATH> location.country.currency_formerly_used </PATH>"], "scores": [-0.004358159378170967, -0.029657192528247833, -0.16548338532447815], "norm_scores": [0.35383152961730957, 0.3449922204017639, 0.3011762499809265]}}
{"id": "WebQTest-1950", "question": "where is president kennedy buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is president kennedy buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.004854036960750818, -0.02908773347735405, -0.07486458122730255], "norm_scores": [0.3438267409801483, 0.33559471368789673, 0.32057857513427734]}}
{"id": "WebQTest-1952", "question": "where was the prophet mohammed from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["symbols.namesake.named_after"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the prophet mohammed from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> symbols.namesake.named_after </PATH>"], "scores": [-0.0047666169703006744, -0.027508055791258812, -0.0946962907910347], "norm_scores": [0.3458399772644043, 0.3380638062953949, 0.3160961866378784]}}
{"id": "WebQTest-1954", "question": "what language do ukrainian people speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do ukrainian people speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.00931496825069189, -0.0176636204123497, -0.0722423642873764], "norm_scores": [0.3412156403064728, 0.33837881684303284, 0.3204054832458496]}}
{"id": "WebQTest-1955", "question": "who are the judges appointed in the supreme court", "prediction": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"], ["book.written_work.subjects", "book.written_work.author"]], "ground_paths": [["law.court.jurisdiction", "people.person.nationality"], ["book.written_work.subjects", "book.author.works_written"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who are the judges appointed in the supreme court [/INST]", "raw_output": {"paths": ["<PATH> law.court.jurisdiction <SEP> people.person.nationality </PATH>", "<PATH> book.written_work.subjects <SEP> book.author.works_written </PATH>", "<PATH> book.written_work.subjects <SEP> book.written_work.author </PATH>"], "scores": [-0.013299199752509594, -0.02255157008767128, -0.02863232046365738], "norm_scores": [0.33606958389282227, 0.33297449350357056, 0.3309558928012848]}}
{"id": "WebQTest-1956", "question": "which country in europe has the largest land area", "prediction": [["location.location.containedby"], ["base.locations.continents.countries_within"], ["location.location.partially_containedby"]], "ground_paths": [["location.location.partially_containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which country in europe has the largest land area [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> base.locations.continents.countries_within </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.014094973914325237, -0.027479013428092003, -0.02917567454278469], "norm_scores": [0.3365034759044647, 0.33202970027923584, 0.33146682381629944]}}
{"id": "WebQTest-1957", "question": "when did kaley cuoco join charmed", "prediction": [["tv.tv_actor.starring_roles", "tv.tv_character.appeared_in_tv_program"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.seasons"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when did kaley cuoco join charmed [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_actor.starring_roles <SEP> tv.tv_character.appeared_in_tv_program </PATH>", "<PATH> people.person.nationality <SEP> tv.tv_program.country_of_origin </PATH>", "<PATH> tv.tv_actor.starring_roles <SEP> tv.regular_tv_appearance.seasons </PATH>"], "scores": [-0.026121875271201134, -0.02658381499350071, -0.02880745194852352], "norm_scores": [0.3336830139160156, 0.33352890610694885, 0.3327880799770355]}}
{"id": "WebQTest-1958", "question": "where is chris rock from", "prediction": [["people.person.place_of_birth"], ["location.location.people_born_here"], ["people.person.nationality"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is chris rock from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> location.location.people_born_here </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.007965333759784698, -0.022015193477272987, -0.08159621059894562], "norm_scores": [0.34304577112197876, 0.3382597267627716, 0.31869450211524963]}}
{"id": "WebQTest-1960", "question": "when was the last time the toronto maple leafs win the stanley cup", "prediction": [["sports.sports_team.championships"], ["sports.sports_championship_event.champion"], ["sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_team.championships"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was the last time the toronto maple leafs win the stanley cup [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.champion </PATH>", "<PATH> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.009749016724526882, -0.0114966481924057, -0.07279696315526962], "norm_scores": [0.34046581387519836, 0.339871346950531, 0.31966283917427063]}}
{"id": "WebQTest-1961", "question": "what kind of language do egyptian speak", "prediction": [["people.ethnicity.people", "people.person.nationality", "language.human_language.countries_spoken_in"], ["people.ethnicity.people", "people.person.nationality", "language.human_language.main_country"], ["people.person.ethnicity", "people.person.nationality", "language.human_language.countries_spoken_in"]], "ground_paths": [["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what kind of language do egyptian speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>", "<PATH> people.ethnicity.people <SEP> people.person.nationality <SEP> language.human_language.main_country </PATH>", "<PATH> people.person.ethnicity <SEP> people.person.nationality <SEP> language.human_language.countries_spoken_in </PATH>"], "scores": [-0.03364460542798042, -0.035540033131837845, -0.03908262401819229], "norm_scores": [0.3341483175754547, 0.33351558446884155, 0.3323361575603485]}}
{"id": "WebQTest-1962", "question": "what language do brazilians use", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["location.country.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do brazilians use [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.0014733480056747794, -0.06060394272208214, -0.06972064822912216], "norm_scores": [0.34763103723526, 0.3276713490486145, 0.3246976137161255]}}
{"id": "WebQTest-1963", "question": "what position did vince lombardi play", "prediction": [["american_football.football_player.position_s"], ["sports.sports_team_coach.sports_coached", "sports.sports_team_coach.sports_coached"], ["sports.sports_team_coach.sports_coached", "sports.sports_team_member.position_s"]], "ground_paths": [["american_football.football_player.position_s"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what position did vince lombardi play [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_coach.sports_coached </PATH>", "<PATH> sports.sports_team_coach.sports_coached <SEP> sports.sports_team_member.position_s </PATH>"], "scores": [-0.010009665973484516, -0.029101917520165443, -0.047355908900499344], "norm_scores": [0.33962416648864746, 0.33320146799087524, 0.3271743953227997]}}
{"id": "WebQTest-1964", "question": "who played stanley kowalski in a streetcar named desire movie", "prediction": [["film.film.country", "people.person.nationality"], ["film.performance.character", "film.performance.actor"], ["award.award_nomination.nominated_for", "award.award_nominee.award_nominations"]], "ground_paths": [["common.image.appears_in_topic_gallery", "common.topic.image"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["tv.regular_tv_appearance.character", "tv.tv_actor.starring_roles"], ["theater.play.country_of_origin", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["award.award_nominated_work.award_nominations", "award.award_nominee.award_nominations"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played stanley kowalski in a streetcar named desire movie [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>", "<PATH> award.award_nomination.nominated_for <SEP> award.award_nominee.award_nominations </PATH>"], "scores": [-0.02779345028102398, -0.033916864544153214, -0.036428820341825485], "norm_scores": [0.3349750339984894, 0.3329300880432129, 0.33209484815597534]}}
{"id": "WebQTest-1965", "question": "who did the voice for lola bunny", "prediction": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.actor.film"], ["film.performance.character", "film.performance.actor"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["tv.tv_character.appeared_in_tv_program", "tv.tv_actor.starring_roles"], ["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.performance.character", "film.performance.actor"], ["film.performance.character", "film.actor.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did the voice for lola bunny [/INST]", "raw_output": {"paths": ["<PATH> fictional_universe.fictional_character.gender <SEP> people.person.gender </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.performance.actor </PATH>"], "scores": [-0.015042834915220737, -0.02778598852455616, -0.0315055325627327], "norm_scores": [0.3365858793258667, 0.33232393860816956, 0.33109012246131897]}}
{"id": "WebQTest-1966", "question": "what book did charles darwin wrote in 1859", "prediction": [["book.written_work.author"], ["book.written_work.subjects"], ["common.topic.notable_types"]], "ground_paths": [["base.argumentmaps.innovator.original_ideas"], [], ["book.written_work.author"], ["exhibitions.exhibition_subject.exhibitions_created_about_this_subject"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what book did charles darwin wrote in 1859 [/INST]", "raw_output": {"paths": ["<PATH> book.written_work.author </PATH>", "<PATH> book.written_work.subjects </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0015475908294320107, -0.06509162485599518, -0.06642826646566391], "norm_scores": [0.34775203466415405, 0.3263419270515442, 0.32590600848197937]}}
{"id": "WebQTest-1967", "question": "what language do serbs speak", "prediction": [["people.ethnicity.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["people.ethnicity.languages_spoken"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do serbs speak [/INST]", "raw_output": {"paths": ["<PATH> people.ethnicity.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.008426792919635773, -0.03134378418326378, -0.048176053911447525], "norm_scores": [0.3403242826461792, 0.3326137661933899, 0.3270619511604309]}}
{"id": "WebQTest-1968", "question": "what was the cause of death for laci peterson", "prediction": [["people.deceased_person.cause_of_death"], ["people.cause_of_death.people"], ["people.person.cause_of_death"]], "ground_paths": [["people.deceased_person.cause_of_death"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the cause of death for laci peterson [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.cause_of_death </PATH>", "<PATH> people.cause_of_death.people </PATH>", "<PATH> people.person.cause_of_death </PATH>"], "scores": [-0.0024385119322687387, -0.03443699702620506, -0.10181277990341187], "norm_scores": [0.3479577898979187, 0.33699995279312134, 0.31504228711128235]}}
{"id": "WebQTest-1969", "question": "what team is kris humphries play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"]], "ground_paths": [["sports.sports_team_roster.player", "sports.sports_team.roster"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team is kris humphries play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team.roster </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>"], "scores": [-0.020829787477850914, -0.02111971005797386, -0.02203078754246235], "norm_scores": [0.3334989845752716, 0.3334023058414459, 0.33309870958328247]}}
{"id": "WebQTest-1971", "question": "what movies gerard butler has been in", "prediction": [["film.performance.actor", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["film.film.produced_by"], ["film.performance.actor", "film.performance.film"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["music.recording.artist"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.film.produced_by", "film.film.sequel"], ["film.actor.film", "film.performance.film"], ["people.person.nationality", "film.film.country"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["film.producer.films_executive_produced"], ["film.actor.film", "film.film.starring"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.producer.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies gerard butler has been in [/INST]", "raw_output": {"paths": ["<PATH> film.performance.actor <SEP> film.performance.film </PATH>", "<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.029978234320878983, -0.03127836808562279, -0.031995851546525955], "norm_scores": [0.33370205760002136, 0.33326849341392517, 0.33302947878837585]}}
{"id": "WebQTest-1972", "question": "where did john franklin live", "prediction": [["people.deceased_person.place_of_death"], ["people.person.places_lived", "people.place_lived.location"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did john franklin live [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_death </PATH>", "<PATH> people.person.places_lived <SEP> people.place_lived.location </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.023711178451776505, -0.03350897505879402, -0.037612248212099075], "norm_scores": [0.33597123622894287, 0.33269554376602173, 0.331333190202713]}}
{"id": "WebQTest-1975", "question": "what was the last movie sean connery was in", "prediction": [["people.person.nationality", "film.film.country"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.film.starring"]], "ground_paths": [["film.film.produced_by"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what was the last movie sean connery was in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>"], "scores": [-0.016466790810227394, -0.030855689197778702, -0.03442201018333435], "norm_scores": [0.33693644404411316, 0.3321229815483093, 0.3309406340122223]}}
{"id": "WebQTest-1976", "question": "what team did drogba play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"]], "ground_paths": [["soccer.football_player.statistics", "soccer.football_player_stats.team"], ["people.person.place_of_birth", "sports.sports_team_location.teams"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team"], ["soccer.football_player_stats.player", "soccer.football_team.player_statistics"], ["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["soccer.football_player.statistics", "soccer.football_team.player_statistics"], ["soccer.football_player.matches_played", "soccer.football_player_match_participation.team"], ["soccer.football_player.goals_scored", "soccer.football_goal.point_awarded_to"], ["soccer.football_player_match_participation.player", "soccer.football_player_match_participation.team"], ["sports.sports_team_roster.player", "sports.sports_team.roster"], ["sports.pro_athlete.teams", "sports.sports_team.roster"], ["base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team", "base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did drogba play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> soccer.football_player.statistics <SEP> soccer.football_team.player_statistics </PATH>"], "scores": [-0.02685694210231304, -0.030610039830207825, -0.03729298710823059], "norm_scores": [0.33491051197052, 0.3336559236049652, 0.3314335346221924]}}
{"id": "WebQTest-1977", "question": "where did crabtree go to college", "prediction": [["education.education.student", "education.education.institution"], ["people.person.nationality", "location.location.containedby"], ["people.person.education", "education.education.institution"]], "ground_paths": [["people.person.nationality", "location.location.containedby"], ["education.education.student", "education.education.institution"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where did crabtree go to college [/INST]", "raw_output": {"paths": ["<PATH> education.education.student <SEP> education.education.institution </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.education <SEP> education.education.institution </PATH>"], "scores": [-0.01570446975529194, -0.018656695261597633, -0.028907407075166702], "norm_scores": [0.3351278305053711, 0.3341399133205414, 0.33073222637176514]}}
{"id": "WebQTest-1978", "question": "where does freddie ljungberg come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does freddie ljungberg come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.004626063629984856, -0.038979098200798035, -0.03936808183789253], "norm_scores": [0.341054230928421, 0.32953694462776184, 0.32940879464149475]}}
{"id": "WebQTest-1979", "question": "who is number 22 on miami dolphins", "prediction": [["american_football.football_player.position_s", "people.person.nationality", "people.person.nationality"], ["sports.sports_team_roster.team", "sports.sports_team_roster.player"], ["american_football.football_player.position_s"]], "ground_paths": [], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is number 22 on miami dolphins [/INST]", "raw_output": {"paths": ["<PATH> american_football.football_player.position_s <SEP> people.person.nationality <SEP> people.person.nationality </PATH>", "<PATH> sports.sports_team_roster.team <SEP> sports.sports_team_roster.player </PATH>", "<PATH> american_football.football_player.position_s </PATH>"], "scores": [-0.020602703094482422, -0.025961941108107567, -0.027607999742031097], "norm_scores": [0.3347085118293762, 0.33291950821876526, 0.3323719799518585]}}
{"id": "WebQTest-1981", "question": "what state did john adams live in", "prediction": [["government.government_position_held.office_holder", "government.government_position_held.jurisdiction_of_office"], ["government.politician.government_positions_held", "government.government_position_held.jurisdiction_of_office"], ["government.government_position_held.office_holder", "government.government_position_held.district_represented"]], "ground_paths": [["symbols.name_source.namesakes", "base.usnationalparks.us_national_park.state"], ["people.place_lived.person", "people.place_lived.location"], ["government.politician.government_positions_held", "government.government_position_held.district_represented"], ["symbols.name_source.namesakes", "location.location.containedby"], ["people.deceased_person.place_of_death", "location.location.containedby"], ["people.deceased_person.place_of_burial", "location.location.containedby"], ["people.person.place_of_birth", "location.location.containedby"], ["tv.tv_program.country_of_origin", "base.locations.countries.states_provinces_within"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what state did john adams live in [/INST]", "raw_output": {"paths": ["<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.politician.government_positions_held <SEP> government.government_position_held.jurisdiction_of_office </PATH>", "<PATH> government.government_position_held.office_holder <SEP> government.government_position_held.district_represented </PATH>"], "scores": [-0.024992087855935097, -0.03307121619582176, -0.04237162321805954], "norm_scores": [0.3361656367778778, 0.33346062898635864, 0.33037370443344116]}}
{"id": "WebQTest-1982", "question": "what type of music did vivaldi compose", "prediction": [["music.artist.genre"], ["music.genre.artists"], ["book.author.works_written"]], "ground_paths": [["music.artist.genre"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of music did vivaldi compose [/INST]", "raw_output": {"paths": ["<PATH> music.artist.genre </PATH>", "<PATH> music.genre.artists </PATH>", "<PATH> book.author.works_written </PATH>"], "scores": [-0.002930512884631753, -0.038370415568351746, -0.10859600454568863], "norm_scores": [0.3490515649318695, 0.3368978500366211, 0.3140506148338318]}}
{"id": "WebQTest-1985", "question": "what team does mike fisher play for", "prediction": [["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team does mike fisher play for [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.014964624308049679, -0.017092308029532433, -0.023198476061224937], "norm_scores": [0.3344845771789551, 0.3337736427783966, 0.3317417800426483]}}
{"id": "WebQTest-1986", "question": "what films has daniel craig been in", "prediction": [["people.person.nationality", "film.film.country"], ["film.performance.actor", "film.film.starring"], ["film.actor.film", "film.performance.film"]], "ground_paths": [["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.film.executive_produced_by"], ["film.actor.film", "film.performance.film"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["people.person.languages", "film.film.language"], ["tv.tv_actor.starring_roles", "tv.regular_tv_appearance.series"], ["people.person.nationality", "film.film.country"], ["people.person.languages", "tv.tv_program.languages"], ["film.actor.film", "film.film.starring"], ["film.performance.actor", "film.performance.film"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what films has daniel craig been in [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> film.film.country </PATH>", "<PATH> film.performance.actor <SEP> film.film.starring </PATH>", "<PATH> film.actor.film <SEP> film.performance.film </PATH>"], "scores": [-0.020571846514940262, -0.03492302447557449, -0.036331769078969955], "norm_scores": [0.336687296628952, 0.33188995718955994, 0.33142274618148804]}}
{"id": "WebQTest-1987", "question": "what materials did eduardo paolozzi use in his work", "prediction": [["visual_art.visual_artist.art_forms"], ["visual_art.visual_artist.art_forms", "visual_art.visual_artist.art_forms"], ["visual_art.art_period_movement.associated_artists", "visual_art.art_period_movement.associated_artists"]], "ground_paths": [["visual_art.visual_artist.artworks", "visual_art.artwork.media"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what materials did eduardo paolozzi use in his work [/INST]", "raw_output": {"paths": ["<PATH> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.visual_artist.art_forms <SEP> visual_art.visual_artist.art_forms </PATH>", "<PATH> visual_art.art_period_movement.associated_artists <SEP> visual_art.art_period_movement.associated_artists </PATH>"], "scores": [-0.002630156697705388, -0.03383472561836243, -0.044572681188583374], "norm_scores": [0.3415064811706543, 0.3310144543647766, 0.3274790942668915]}}
{"id": "WebQTest-1988", "question": "what did joe biden study in college", "prediction": [["people.person.education", "education.education.major_field_of_study"], ["education.education.student", "education.education.major_field_of_study"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["common.topic.subjects", "book.written_work.school_or_movement"], ["education.education.student", "education.education.major_field_of_study"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did joe biden study in college [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.major_field_of_study </PATH>", "<PATH> education.education.student <SEP> education.education.major_field_of_study </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.016161130741238594, -0.016231870278716087, -0.02711542882025242], "norm_scores": [0.3345561623573303, 0.3345325291156769, 0.3309113681316376]}}
{"id": "WebQTest-1990", "question": "who plays timon 's voice in the lion king", "prediction": [["film.film.country", "people.person.nationality"], ["film.film.starring", "film.actor.film"], ["film.performance.character", "film.actor.film"]], "ground_paths": [["film.performance.film", "film.actor.film"], ["film.film.language", "people.person.languages"], ["film.film.starring", "film.actor.film"], ["film.film.country", "people.person.nationality"], ["music.recording.releases", "music.featured_artist.recordings"], ["film.film.featured_song", "music.recording.artist"], ["music.recording.releases", "music.artist.track"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays timon 's voice in the lion king [/INST]", "raw_output": {"paths": ["<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> film.film.starring <SEP> film.actor.film </PATH>", "<PATH> film.performance.character <SEP> film.actor.film </PATH>"], "scores": [-0.028386326506733894, -0.03121592290699482, -0.037038225680589676], "norm_scores": [0.33460932970046997, 0.33366385102272034, 0.3317268490791321]}}
{"id": "WebQTest-1991", "question": "what part of the country is ohio in", "prediction": [["location.location.containedby"], ["location.location.contains"], ["location.administrative_division.country"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what part of the country is ohio in [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> location.administrative_division.country </PATH>"], "scores": [-0.012445650063455105, -0.01785944774746895, -0.07597177475690842], "norm_scores": [0.3409419655799866, 0.3391011655330658, 0.31995686888694763]}}
{"id": "WebQTest-1992", "question": "which island is kailua on", "prediction": [["location.location.containedby"], ["location.location.contains"], ["geography.island.island_group"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which island is kailua on [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>", "<PATH> geography.island.island_group </PATH>"], "scores": [-0.009051116183400154, -0.03234932944178581, -0.04393257945775986], "norm_scores": [0.33982500433921814, 0.3319992125034332, 0.32817575335502625]}}
{"id": "WebQTest-1993", "question": "what language do speak in argentina", "prediction": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["language.human_language.main_country"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what language do speak in argentina [/INST]", "raw_output": {"paths": ["<PATH> location.country.languages_spoken </PATH>", "<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> language.human_language.main_country </PATH>"], "scores": [-0.007096630055457354, -0.028556285426020622, -0.049578841775655746], "norm_scores": [0.3404630422592163, 0.33323466777801514, 0.32630231976509094]}}
{"id": "WebQTest-1994", "question": "what did whoopi goldberg won a grammy for", "prediction": [["award.award_honor.award_winner", "award.award_honor.award"], ["award.award_winner.awards_won", "award.award_honor.award"], ["award.award_nomination.award_nominee", "award.award_nomination.award"]], "ground_paths": [["award.award_honor.award_winner", "award.award_category.winners"], ["award.award_nomination.award_nominee", "award.award_category.nominees"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did whoopi goldberg won a grammy for [/INST]", "raw_output": {"paths": ["<PATH> award.award_honor.award_winner <SEP> award.award_honor.award </PATH>", "<PATH> award.award_winner.awards_won <SEP> award.award_honor.award </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.award </PATH>"], "scores": [-0.016222568228840828, -0.02165163867175579, -0.025890663266181946], "norm_scores": [0.33501240611076355, 0.33319851756095886, 0.3317890763282776]}}
{"id": "WebQTest-1996", "question": "who played maggie in himym", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"]], "ground_paths": [["fictional_universe.fictional_character.gender", "people.person.gender"], ["film.film_character.portrayed_in_films", "film.actor.film"], ["fictional_universe.fictional_character.place_of_birth", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played maggie in himym [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.01744786463677883, -0.03630252555012703, -0.03682679310441017], "norm_scores": [0.3375949561595917, 0.3312893509864807, 0.3311156928539276]}}
{"id": "WebQTest-1997", "question": "who does owen schmitt play for", "prediction": [["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["american_football.football_player.receiving", "american_football.player_receiving_statistics.team"], ["american_football.player_receiving_statistics.player", "american_football.player_receiving_statistics.team"], ["american_football.football_player.rushing", "american_football.player_rushing_statistics.team"], ["american_football.player_game_statistics.player", "american_football.player_game_statistics.team"], ["sports.sports_team_roster.player", "sports.sports_team_roster.team"], ["american_football.football_player.games", "american_football.player_game_statistics.team"], ["base.schemastaging.athlete_extra.salary", "base.schemastaging.athlete_salary.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who does owen schmitt play for [/INST]", "raw_output": {"paths": ["<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.sports_team_roster.player <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.020269958302378654, -0.020765533670783043, -0.02845950424671173], "norm_scores": [0.33429738879203796, 0.33413177728652954, 0.3315708339214325]}}
{"id": "WebQTest-1998", "question": "what party was thomas jefferson affiliated with", "prediction": [["people.person.nationality", "organization.organization.geographic_scope"], ["organization.organization_founder.organizations_founded"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["organization.organization_founder.organizations_founded"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party was thomas jefferson affiliated with [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.01870226114988327, -0.019114991649985313, -0.026176104322075844], "norm_scores": [0.33420881628990173, 0.33407092094421387, 0.3317202925682068]}}
{"id": "WebQTest-1999", "question": "what religions are popular in france", "prediction": [["people.person.nationality", "people.person.religion"], ["location.statistical_region.religions", "location.religion_percentage.religion"], ["royalty.monarch.kingdom", "people.person.religion"]], "ground_paths": [["location.statistical_region.religions", "location.religion_percentage.religion"], ["people.person.nationality", "people.person.religion"], ["periodicals.newspaper_circulation_area.newspapers", "book.periodical.subjects"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what religions are popular in france [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality <SEP> people.person.religion </PATH>", "<PATH> location.statistical_region.religions <SEP> location.religion_percentage.religion </PATH>", "<PATH> royalty.monarch.kingdom <SEP> people.person.religion </PATH>"], "scores": [-0.009247960522770882, -0.013410630635917187, -0.07323933392763138], "norm_scores": [0.34084799885749817, 0.33943209052085876, 0.3197198808193207]}}
{"id": "WebQTest-2000", "question": "who played berger in sex and the city", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["film.film.country", "people.person.nationality"], ["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"]], "ground_paths": [["tv.tv_program.regular_cast", "tv.regular_tv_appearance.actor"], ["film.film.country", "people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who played berger in sex and the city [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> film.film.country <SEP> people.person.nationality </PATH>", "<PATH> tv.tv_program.regular_cast <SEP> tv.regular_tv_appearance.actor </PATH>"], "scores": [-0.0232285987585783, -0.023689070716500282, -0.045810241252183914], "norm_scores": [0.33588483929634094, 0.3357301950454712, 0.32838499546051025]}}
{"id": "WebQTest-2001", "question": "who won the 2000 fa cup final", "prediction": [["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_championship_event.championship", "sports.sports_team.championships"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"]], "ground_paths": [["sports.sports_championship_event.championship", "sports.sports_championship_event.champion"], ["time.event.instance_of_recurring_event", "sports.sports_championship_event.champion"], ["sports.sports_championship_event.championship", "sports.sports_championship_event.runner_up"], ["time.event.instance_of_recurring_event", "sports.sports_team.championships"], ["sports.sports_award_type.winners", "sports.sports_award.award_winner"], ["sports.sports_award.award", "sports.sports_award.award_winner"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who won the 2000 fa cup final [/INST]", "raw_output": {"paths": ["<PATH> sports.sports_award_type.winners <SEP> sports.sports_award.award_winner </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_team.championships </PATH>", "<PATH> sports.sports_championship_event.championship <SEP> sports.sports_championship_event.runner_up </PATH>"], "scores": [-0.020084459334611893, -0.021903544664382935, -0.02416212670505047], "norm_scores": [0.3339886963367462, 0.333381712436676, 0.33262959122657776]}}
{"id": "WebQTest-2002", "question": "what are the names of michael jackson 's kids", "prediction": [["people.person.parents"], ["people.person.children"], ["music.composition.composer", "people.person.parents"]], "ground_paths": [["people.person.parents"], ["people.person.children"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the names of michael jackson 's kids [/INST]", "raw_output": {"paths": ["<PATH> people.person.parents </PATH>", "<PATH> people.person.children </PATH>", "<PATH> music.composition.composer <SEP> people.person.parents </PATH>"], "scores": [-0.008028906770050526, -0.019539346918463707, -0.0947817713022232], "norm_scores": [0.3441796600818634, 0.3402407169342041, 0.3155795931816101]}}
{"id": "WebQTest-2003", "question": "what movies have robert pattinson in them", "prediction": [["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"]], "ground_paths": [["award.award_winner.awards_won", "award.award_honor.honored_for"], ["music.artist.track", "music.recording.releases"], ["award.award_nomination.award_nominee", "award.award_nomination.nominated_for"], ["film.actor.film", "film.performance.film"], ["film.performance.actor", "film.performance.film"], ["award.award_nominee.award_nominations", "award.award_nominated_work.award_nominations"], ["music.recording.artist", "music.recording.releases"], ["people.person.nationality", "tv.tv_program.country_of_origin"], ["award.award_honor.award_winner", "award.award_winning_work.awards_won"], ["people.person.nationality", "film.film.country"], ["film.producer.films_executive_produced"], ["award.award_nominee.award_nominations", "award.award_nomination.nominated_for"], ["award.award_honor.award_winner", "award.award_honor.honored_for"], ["people.person.nationality", "media_common.netflix_title.netflix_genres"], ["film.actor.film", "film.film.starring"], ["award.award_winner.awards_won", "award.award_winning_work.awards_won"], ["film.performance.actor", "film.film.starring"], ["award.award_nomination.award_nominee", "award.award_nominated_work.award_nominations"], ["tv.regular_tv_appearance.actor", "tv.regular_tv_appearance.series"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what movies have robert pattinson in them [/INST]", "raw_output": {"paths": ["<PATH> award.award_nominee.award_nominations <SEP> award.award_nomination.nominated_for </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nominated_work.award_nominations </PATH>", "<PATH> award.award_nomination.award_nominee <SEP> award.award_nomination.nominated_for </PATH>"], "scores": [-0.028356337919831276, -0.02998926304280758, -0.031526803970336914], "norm_scores": [0.3338671922683716, 0.3333224654197693, 0.33281034231185913]}}
{"id": "WebQTest-2004", "question": "what the time zone in japan", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"], ["language.human_language.countries_spoken_in", "location.location.time_zones"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what the time zone in japan [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.location.time_zones </PATH>"], "scores": [-0.0004895245074294508, -0.06293075531721115, -0.10635937750339508], "norm_scores": [0.35223549604415894, 0.33091408014297485, 0.3168505132198334]}}
{"id": "WebQTest-2005", "question": "who plays bob kelso in scrubs", "prediction": [["tv.tv_program.country_of_origin", "people.person.nationality"], ["tv.regular_tv_appearance.series", "tv.regular_tv_appearance.actor"], ["tv.regular_tv_appearance.series", "tv.tv_actor.starring_roles"]], "ground_paths": [["tv.regular_tv_appearance.character", "tv.regular_tv_appearance.actor"], ["fictional_universe.fictional_character.gender", "people.person.gender"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who plays bob kelso in scrubs [/INST]", "raw_output": {"paths": ["<PATH> tv.tv_program.country_of_origin <SEP> people.person.nationality </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.regular_tv_appearance.actor </PATH>", "<PATH> tv.regular_tv_appearance.series <SEP> tv.tv_actor.starring_roles </PATH>"], "scores": [-0.01597355119884014, -0.02693559229373932, -0.033806443214416504], "norm_scores": [0.336539089679718, 0.33287009596824646, 0.3305908143520355]}}
{"id": "WebQTest-2006", "question": "what race is vanessa carlton", "prediction": [["people.person.ethnicity"], ["people.ethnicity.people"], ["people.person.nationality"]], "ground_paths": [["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what race is vanessa carlton [/INST]", "raw_output": {"paths": ["<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.006476196460425854, -0.04135189205408096, -0.05352813005447388], "norm_scores": [0.3424935042858124, 0.33075472712516785, 0.32675179839134216]}}
{"id": "WebQTest-2007", "question": "what do people in czech republic speak", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.languages_spoken"], ["location.country.official_language"]], "ground_paths": [["language.human_language.main_country"], ["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what do people in czech republic speak [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.languages_spoken </PATH>", "<PATH> location.country.official_language </PATH>"], "scores": [-0.012807033024728298, -0.014009064994752407, -0.06616013497114182], "norm_scores": [0.3393460810184479, 0.338938444852829, 0.32171541452407837]}}
{"id": "WebQTest-2008", "question": "which countries speak arabic language", "prediction": [["language.human_language.countries_spoken_in"], ["location.country.official_language"], ["location.country.languages_spoken"]], "ground_paths": [["location.country.languages_spoken"], ["language.human_language.countries_spoken_in"], ["location.country.official_language"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which countries speak arabic language [/INST]", "raw_output": {"paths": ["<PATH> language.human_language.countries_spoken_in </PATH>", "<PATH> location.country.official_language </PATH>", "<PATH> location.country.languages_spoken </PATH>"], "scores": [-0.016745315864682198, -0.01713295839726925, -0.033630069345235825], "norm_scores": [0.335247665643692, 0.33511772751808167, 0.3296346068382263]}}
{"id": "WebQTest-2009", "question": "where does tim cook work", "prediction": [["people.person.employment_history", "business.employer.employees"], ["people.person.nationality", "location.location.containedby"], ["people.person.employment_history", "business.employment_tenure.company"]], "ground_paths": [["organization.leadership.person", "organization.organization.leadership"], ["people.person.employment_history", "business.employment_tenure.company"], ["business.employment_tenure.person", "business.employment_tenure.company"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does tim cook work [/INST]", "raw_output": {"paths": ["<PATH> people.person.employment_history <SEP> business.employer.employees </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>", "<PATH> people.person.employment_history <SEP> business.employment_tenure.company </PATH>"], "scores": [-0.01986253261566162, -0.0339537039399147, -0.040741149336099625], "norm_scores": [0.33722883462905884, 0.33251023292541504, 0.3302609622478485]}}
{"id": "WebQTest-2010", "question": "what degrees does romney have", "prediction": [["people.person.education", "education.education.degree"], ["education.education.student", "education.education.degree"], ["people.person.nationality", "location.location.containedby"]], "ground_paths": [["people.person.education", "education.education.degree"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what degrees does romney have [/INST]", "raw_output": {"paths": ["<PATH> people.person.education <SEP> education.education.degree </PATH>", "<PATH> education.education.student <SEP> education.education.degree </PATH>", "<PATH> people.person.nationality <SEP> location.location.containedby </PATH>"], "scores": [-0.01838281936943531, -0.018487881869077682, -0.01937996782362461], "norm_scores": [0.33345580101013184, 0.3334207534790039, 0.33312344551086426]}}
{"id": "WebQTest-2011", "question": "where is bob marley buried", "prediction": [["people.deceased_person.place_of_burial"], ["people.place_of_interment.interred_here"], ["people.person.place_of_interment"]], "ground_paths": [["people.deceased_person.place_of_burial"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is bob marley buried [/INST]", "raw_output": {"paths": ["<PATH> people.deceased_person.place_of_burial </PATH>", "<PATH> people.place_of_interment.interred_here </PATH>", "<PATH> people.person.place_of_interment </PATH>"], "scores": [-0.003592496272176504, -0.032154977321624756, -0.08321806788444519], "norm_scores": [0.34538689255714417, 0.3356613516807556, 0.31895169615745544]}}
{"id": "WebQTest-2012", "question": "what is london uk time zone", "prediction": [["location.location.time_zones"], ["time.time_zone.locations_in_this_time_zone"]], "ground_paths": [["location.location.time_zones"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what is london uk time zone [/INST]", "raw_output": {"paths": ["<PATH> location.location.time_zones </PATH>", "<PATH> time.time_zone.locations_in_this_time_zone </PATH>", "<PATH> location.location.time_zones <PATH>"], "scores": [-4.540367444860749e-05, -0.1025107130408287, -0.18931426107883453], "norm_scores": [0.3662770688533783, 0.330605149269104, 0.3031177222728729]}}
{"id": "WebQTest-2013", "question": "where to stay in chicago tourist", "prediction": [["location.location.containedby"], ["travel.travel_destination.accommodation"], ["travel.tourist_attraction.near_travel_destination"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where to stay in chicago tourist [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> travel.travel_destination.accommodation </PATH>", "<PATH> travel.tourist_attraction.near_travel_destination </PATH>"], "scores": [-0.0219150148332119, -0.021940654143691063, -0.025024930015206337], "norm_scores": [0.33368155360221863, 0.33367297053337097, 0.332645446062088]}}
{"id": "WebQTest-2014", "question": "what type of breast cancer did sheryl crow have", "prediction": [["medicine.disease.notable_people_with_this_condition"], ["medicine.notable_person_with_medical_condition.condition"], ["medicine.disease.parent_disease"]], "ground_paths": [["medicine.notable_person_with_medical_condition.condition"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of breast cancer did sheryl crow have [/INST]", "raw_output": {"paths": ["<PATH> medicine.disease.notable_people_with_this_condition </PATH>", "<PATH> medicine.notable_person_with_medical_condition.condition </PATH>", "<PATH> medicine.disease.parent_disease </PATH>"], "scores": [-0.0027142777107656, -0.034761786460876465, -0.05211540684103966], "norm_scores": [0.3424353003501892, 0.33163508772850037, 0.3259296417236328]}}
{"id": "WebQTest-2015", "question": "what type of government does argentina have today", "prediction": [["government.form_of_government.countries"], ["location.country.form_of_government"], ["language.human_language.countries_spoken_in", "location.country.form_of_government"]], "ground_paths": [["location.country.form_of_government"], ["government.form_of_government.countries"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what type of government does argentina have today [/INST]", "raw_output": {"paths": ["<PATH> government.form_of_government.countries </PATH>", "<PATH> location.country.form_of_government </PATH>", "<PATH> language.human_language.countries_spoken_in <SEP> location.country.form_of_government </PATH>"], "scores": [-0.008739154785871506, -0.015779653564095497, -0.13630317151546478], "norm_scores": [0.3480414152145386, 0.34559959173202515, 0.3063589334487915]}}
{"id": "WebQTest-2016", "question": "where does fabio capello come from", "prediction": [["people.person.place_of_birth"], ["people.person.nationality"], ["location.location.people_born_here"]], "ground_paths": [["people.person.place_of_birth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where does fabio capello come from [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.nationality </PATH>", "<PATH> location.location.people_born_here </PATH>"], "scores": [-0.005547489505261183, -0.03470838814973831, -0.048121627420186996], "norm_scores": [0.341345876455307, 0.331535667181015, 0.3271183967590332]}}
{"id": "WebQTest-2017", "question": "what party was abe lincoln part of", "prediction": [["organization.organization_founder.organizations_founded"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.political_party_tenure.politician", "government.political_party_tenure.party"]], "ground_paths": [["influence.influence_node.influenced_by", "organization.organization.founders"], ["government.political_party_tenure.politician", "government.political_party.politicians_in_this_party"], ["organization.organization_founder.organizations_founded"], ["people.person.nationality", "organization.organization.geographic_scope"], ["government.politician.party", "government.political_party_tenure.party"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what party was abe lincoln part of [/INST]", "raw_output": {"paths": ["<PATH> organization.organization_founder.organizations_founded </PATH>", "<PATH> people.person.nationality <SEP> organization.organization.geographic_scope </PATH>", "<PATH> government.political_party_tenure.politician <SEP> government.political_party_tenure.party </PATH>"], "scores": [-0.013632679358124733, -0.020518040284514427, -0.0256313718855381], "norm_scores": [0.3354341387748718, 0.3331325054168701, 0.33143341541290283]}}
{"id": "WebQTest-2018", "question": "what did jack london do for a living", "prediction": [["people.person.profession"], ["fictional_universe.fictional_character.occupation"], ["common.topic.notable_types"]], "ground_paths": [["people.person.profession"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what did jack london do for a living [/INST]", "raw_output": {"paths": ["<PATH> people.person.profession </PATH>", "<PATH> fictional_universe.fictional_character.occupation </PATH>", "<PATH> common.topic.notable_types </PATH>"], "scores": [-0.0012330446625128388, -0.05685553327202797, -0.06712313741445541], "norm_scores": [0.34696561098098755, 0.32819345593452454, 0.3248409032821655]}}
{"id": "WebQTest-2019", "question": "who did morgan freeman marry", "prediction": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["freebase.valuenotation.is_reviewed", "freebase.valuenotation.is_reviewed"], ["people.person.nationality", "people.person.nationality"]], "ground_paths": [["freebase.valuenotation.is_reviewed", "freebase.valuenotation.has_value"], ["people.person.parents", "people.person.children"], ["people.marriage.spouse", "people.marriage.spouse"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who did morgan freeman marry [/INST]", "raw_output": {"paths": ["<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.has_value </PATH>", "<PATH> freebase.valuenotation.is_reviewed <SEP> freebase.valuenotation.is_reviewed </PATH>", "<PATH> people.person.nationality <SEP> people.person.nationality </PATH>"], "scores": [-0.01927557960152626, -0.03655983507633209, -0.039798155426979065], "norm_scores": [0.3375469744205475, 0.3317628502845764, 0.3306902348995209]}}
{"id": "WebQTest-2020", "question": "what are the mountains in northern italy called", "prediction": [["location.location.containedby"], ["location.location.partially_containedby"], ["location.location.partially_contains"]], "ground_paths": [[]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what are the mountains in northern italy called [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>", "<PATH> location.location.partially_contains </PATH>"], "scores": [-0.012732559815049171, -0.029037360101938248, -0.03474194556474686], "norm_scores": [0.33760306239128113, 0.3321431279182434, 0.3302537798881531]}}
{"id": "WebQTest-2021", "question": "what influenced john steinbeck to start writing", "prediction": [["influence.influence_node.influenced"], ["influence.influence_node.influenced_by"], ["influence.influence_node.influenced", "influence.influence_node.influenced_by"]], "ground_paths": [["influence.influence_node.influenced_by"], ["influence.influence_node.influenced"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what influenced john steinbeck to start writing [/INST]", "raw_output": {"paths": ["<PATH> influence.influence_node.influenced </PATH>", "<PATH> influence.influence_node.influenced_by </PATH>", "<PATH> influence.influence_node.influenced <SEP> influence.influence_node.influenced_by </PATH>"], "scores": [-0.009216424077749252, -0.013964642770588398, -0.10414358973503113], "norm_scores": [0.34426936507225037, 0.3426385819911957, 0.31309205293655396]}}
{"id": "WebQTest-2022", "question": "where is dwayne johnson ethnicity", "prediction": [["people.person.ethnicity"], ["people.ethnicity.people"], ["people.person.nationality"]], "ground_paths": [["people.ethnicity.people"], ["people.person.ethnicity"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is dwayne johnson ethnicity [/INST]", "raw_output": {"paths": ["<PATH> people.person.ethnicity </PATH>", "<PATH> people.ethnicity.people </PATH>", "<PATH> people.person.nationality </PATH>"], "scores": [-0.01595260575413704, -0.01643444411456585, -0.04597354680299759], "norm_scores": [0.3367067277431488, 0.33654454350471497, 0.32674869894981384]}}
{"id": "WebQTest-2023", "question": "where are the nato headquarters located", "prediction": [["organization.organization.headquarters", "location.mailing_address.citytown"], ["organization.organization.headquarters", "location.mailing_address.state_province_region"], ["organization.organization_founder.organizations_founded"]], "ground_paths": [["organization.organization.founders", "base.biblioness.bibs_location.country"], ["organization.organization.headquarters", "location.mailing_address.citytown"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where are the nato headquarters located [/INST]", "raw_output": {"paths": ["<PATH> organization.organization.headquarters <SEP> location.mailing_address.citytown </PATH>", "<PATH> organization.organization.headquarters <SEP> location.mailing_address.state_province_region </PATH>", "<PATH> organization.organization_founder.organizations_founded </PATH>"], "scores": [-0.007348831743001938, -0.04668963700532913, -0.05375618487596512], "norm_scores": [0.34292659163475037, 0.32969751954078674, 0.3273759186267853]}}
{"id": "WebQTest-2024", "question": "in which state was the battle of antietam fought", "prediction": [["location.location.events"], ["time.event.locations"], ["base.culturalevent.event.entity_involved"]], "ground_paths": [["location.location.events"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: in which state was the battle of antietam fought [/INST]", "raw_output": {"paths": ["<PATH> location.location.events </PATH>", "<PATH> time.event.locations </PATH>", "<PATH> base.culturalevent.event.entity_involved </PATH>"], "scores": [-0.0013950604479759932, -0.050006747245788574, -0.11546804755926132], "norm_scores": [0.35152551531791687, 0.33484596014022827, 0.31362855434417725]}}
{"id": "WebQTest-2025", "question": "which ocean does the zambezi river flow into", "prediction": [["geography.river.mouth"], ["location.location.containedby"], ["location.location.partially_containedby"]], "ground_paths": [["geography.river.mouth"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: which ocean does the zambezi river flow into [/INST]", "raw_output": {"paths": ["<PATH> geography.river.mouth </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.partially_containedby </PATH>"], "scores": [-0.0023888968862593174, -0.05126519128680229, -0.06789382547140121], "norm_scores": [0.3461533188819885, 0.3296414315700531, 0.3242052495479584]}}
{"id": "WebQTest-2027", "question": "what team did david beckham play for before la galaxy", "prediction": [["base.schemastaging.athlete_salary.athlete", "base.schemastaging.athlete_salary.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"], ["sports.pro_athlete.teams", "sports.sports_team.roster"]], "ground_paths": [["soccer.football_player.loans", "soccer.football_player_loan.lending_team"], ["soccer.football_player_stats.player", "soccer.football_player_stats.team"], ["sports.pro_athlete.teams", "sports.sports_team_roster.team"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: what team did david beckham play for before la galaxy [/INST]", "raw_output": {"paths": ["<PATH> base.schemastaging.athlete_salary.athlete <SEP> base.schemastaging.athlete_salary.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team_roster.team </PATH>", "<PATH> sports.pro_athlete.teams <SEP> sports.sports_team.roster </PATH>"], "scores": [-0.0232319924980402, -0.02464894950389862, -0.032278090715408325], "norm_scores": [0.33449527621269226, 0.334021657705307, 0.33148306608200073]}}
{"id": "WebQTest-2028", "question": "who is the current leader of france 2010", "prediction": [["people.person.nationality"], ["base.onephylogeny.type_of_thing.things_of_this_type", "people.person.profession"], ["common.topic.image", "common.topic.image"]], "ground_paths": [["people.person.nationality"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: who is the current leader of france 2010 [/INST]", "raw_output": {"paths": ["<PATH> people.person.nationality </PATH>", "<PATH> base.onephylogeny.type_of_thing.things_of_this_type <SEP> people.person.profession </PATH>", "<PATH> common.topic.image <SEP> common.topic.image </PATH>"], "scores": [-0.0004252718645147979, -0.07306414842605591, -0.1069563552737236], "norm_scores": [0.35349634289741516, 0.32872918248176575, 0.3177745044231415]}}
{"id": "WebQTest-2029", "question": "where was the palace of knossos located", "prediction": [["location.location.containedby"], ["architecture.building.building_complex"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where was the palace of knossos located [/INST]", "raw_output": {"paths": ["<PATH> location.location.containedby </PATH>", "<PATH> architecture.building.building_complex </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.0009964988566935062, -0.07370403409004211, -0.09907107055187225], "norm_scores": [0.3525528907775879, 0.3278293013572693, 0.3196178376674652]}}
{"id": "WebQTest-2030", "question": "where is roswell area 51", "prediction": [["aviation.airport.serves"], ["location.location.containedby"], ["location.location.contains"]], "ground_paths": [["location.location.containedby"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: where is roswell area 51 [/INST]", "raw_output": {"paths": ["<PATH> aviation.airport.serves </PATH>", "<PATH> location.location.containedby </PATH>", "<PATH> location.location.contains </PATH>"], "scores": [-0.004757387097924948, -0.03037264570593834, -0.078125961124897], "norm_scores": [0.34435635805130005, 0.3356476128101349, 0.3199959993362427]}}
{"id": "WebQTest-2031", "question": "when was father chris riley born", "prediction": [["people.person.place_of_birth"], ["people.person.children"], ["people.person.parents"]], "ground_paths": [["location.location.people_born_here"]], "input": "[INST] <<SYS>>\n<</SYS>>\nPlease generate a valid relation path that can be helpful for answering the following question: when was father chris riley born [/INST]", "raw_output": {"paths": ["<PATH> people.person.place_of_birth </PATH>", "<PATH> people.person.children </PATH>", "<PATH> people.person.parents </PATH>"], "scores": [-0.001309053273871541, -0.06949533522129059, -0.08989964425563812], "norm_scores": [0.3509625792503357, 0.32782942056655884, 0.32120802998542786]}}
